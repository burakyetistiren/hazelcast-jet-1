<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta http-equiv="X-UA-Compatible" content="IE=edge"/><title>Blog · Hazelcast Jet</title><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta name="generator" content="Docusaurus"/><meta name="description" content="Open-Source Distributed Stream Processing"/><meta name="docsearch:language" content="en"/><meta property="og:title" content="Blog · Hazelcast Jet"/><meta property="og:type" content="website"/><meta property="og:url" content="https://jet-start.sh/"/><meta property="og:description" content="Open-Source Distributed Stream Processing"/><meta property="og:image" content="https://jet-start.sh/img/Hazelcast-Jet-Logo-Blue_Dark.jpg"/><meta name="twitter:card" content="summary"/><meta name="twitter:image" content="https://jet-start.sh/img/Hazelcast-Jet-Logo-Blue_Dark.jpg"/><link rel="shortcut icon" href="/img/favicon.png"/><link rel="stylesheet" href="https://cdn.jsdelivr.net/docsearch.js/1/docsearch.min.css"/><link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css"/><link rel="alternate" type="application/atom+xml" href="https://jet-start.sh/blog/atom.xml" title="Hazelcast Jet Blog ATOM Feed"/><link rel="alternate" type="application/rss+xml" href="https://jet-start.sh/blog/feed.xml" title="Hazelcast Jet Blog RSS Feed"/><script>
              (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
              (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
              m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
              })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

              ga('create', 'UA-158279495-1', 'auto');
              ga('send', 'pageview');
            </script><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Ubuntu:300,400,500,600"/><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:300,400,500,600,700,800"/><script type="text/javascript" src="https://buttons.github.io/buttons.js"></script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.0/clipboard.min.js"></script><script type="text/javascript" src="/js/code-block-buttons.js"></script><script type="text/javascript" src="https://plausible.io/js/plausible.js" async="" defer="" data-domain="jet-start.sh"></script><script src="/js/scrollSpy.js"></script><link rel="stylesheet" href="/css/prism.css"/><link rel="stylesheet" href="/css/main.css"/><script src="/js/codetabs.js"></script></head><body class="blog"><div class="fixedHeaderContainer"><div class="headerWrapper wrapper"><header><a href="/"><img class="logo" src="/img/logo-dark.svg" alt="Hazelcast Jet"/></a><a href="/versions"><h3>4.5.4</h3></a><div class="navigationWrapper navigationSlider"><nav class="slidingNav"><ul class="nav-site nav-site-internal"><li class=""><a href="/docs/get-started/intro" target="_self">Docs</a></li><li class=""><a href="/download" target="_self">Download</a></li><li class=""><a href="/demos" target="_self">Demos</a></li><li class=""><a href="https://github.com/hazelcast/hazelcast-jet" target="_self">GitHub</a></li><li class=""><a href="https://slack.hazelcast.com/" target="_self">Community</a></li><li class="siteNavGroupActive siteNavItemActive"><a href="/blog/" target="_self">Blog</a></li><li class="navSearchWrapper reactNavSearchWrapper"><input type="text" id="search_input_react" placeholder="Search" title="Search"/></li></ul></nav></div></header></div></div><div class="navPusher"><div class="docMainWrapper wrapper"><div class="docsNavContainer" id="docsNav"><nav class="toc"><div class="toggleNav"><section class="navWrapper wrapper"><div class="navBreadcrumb wrapper"><div class="navToggle" id="navToggler"><div class="hamburger-menu"><div class="line1"></div><div class="line2"></div><div class="line3"></div></div></div><h2><i>›</i><span>All posts</span></h2><div class="tocToggler" id="tocToggler"><i class="icon-toc"></i></div></div><div class="navGroups"><div class="navGroup"><h3 class="navGroupCategoryTitle">All posts</h3><ul class=""><li class="navListItem"><a class="navItem" href="/blog/2023/06/14/jet-engine-in-hazelcast">Jet engine lives on in Hazelcast 5.x</a></li><li class="navListItem"><a class="navItem" href="/blog/2021/04/21/jet-45-is-released">Jet 4.5 Released</a></li><li class="navListItem"><a class="navItem" href="/blog/2021/03/17/billion-events-per-second">Billion Events Per Second with Millisecond Latency: Streaming Analytics at Giga-Scale</a></li><li class="navListItem"><a class="navItem" href="/blog/2021/02/03/jet-44-is-released">Jet 4.4 Released</a></li><li class="navListItem"><a class="navItem" href="/blog/2020/10/23/jet-43-is-released">Jet 4.3 Released</a></li><li class="navListItem"><a class="navItem" href="/blog/2020/10/06/enabling-full-text-search">Enabling Full-text Search with Change Data Capture in a Legacy Application</a></li><li class="navListItem"><a class="navItem" href="/blog/2020/09/18/cdc-meets-stream-processing">Change Data Capture meets Stream Processing</a></li><li class="navListItem"><a class="navItem" href="/blog/2020/08/05/gc-tuning-for-jet">Sub-10 ms Latency in Java: Concurrent GC with Green Threads</a></li><li class="navListItem"><a class="navItem" href="/blog/2020/07/16/designing-evergreen-cache-cdc">Designing an Evergreen Cache with Change Data Capture</a></li><li class="navListItem"><a class="navItem" href="/blog/2020/07/14/jet-42-is-released">Jet 4.2 is Released</a></li><li class="navListItem"><a class="navItem" href="/blog/2020/06/23/jdk-gc-benchmarks-rematch">Performance of Modern Java on Data-Heavy Workloads: The Low-Latency Rematch</a></li><li class="navListItem"><a class="navItem" href="/blog/2020/06/09/jdk-gc-benchmarks-part2">Performance of Modern Java on Data-Heavy Workloads: Batch Processing</a></li><li class="navListItem"><a class="navItem" href="/blog/2020/06/09/jdk-gc-benchmarks-part1">Performance of Modern Java on Data-Heavy Workloads: Real-Time Streaming</a></li><li class="navListItem"><a class="navItem" href="/blog/2020/05/25/grcp">Processing 10M queries / second on a single node using Jet and gRPC</a></li><li class="navListItem"><a class="navItem" href="/blog/2020/05/18/spark-jet">How Hazelcast Jet Compares to Apache Spark</a></li><li class="navListItem"><a class="navItem" href="/blog/2020/04/29/jet-41-is-released">Jet 4.1 is Released</a></li><li class="navListItem"><a class="navItem" href="/blog/2020/04/01/upgrading-to-jet-40">Upgrading to Jet 4.0</a></li><li class="navListItem"><a class="navItem" href="/blog/2020/03/30/ml-inference">Machine Learning Inference at Scale</a></li><li class="navListItem"><a class="navItem" href="/blog/2020/03/02/jet-40-is-released">Jet 4.0 is Released</a></li><li class="navListItem"><a class="navItem" href="/blog/2020/02/20/transactional-processors">Transactional connectors in Hazelcast Jet</a></li><li class="navListItem"><a class="navItem" href="/blog/2020/01/28/new-website">Announcing New Documentation Website</a></li><li class="navListItem"><a class="navItem" href="/blog/2019/11/12/stream-deduplication">Stream Deduplication with Hazelcast Jet</a></li></ul></div></div></section></div><script>
            var coll = document.getElementsByClassName('collapsible');
            var checkActiveCategory = true;
            for (var i = 0; i < coll.length; i++) {
              var links = coll[i].nextElementSibling.getElementsByTagName('*');
              if (checkActiveCategory){
                for (var j = 0; j < links.length; j++) {
                  if (links[j].classList.contains('navListItemActive')){
                    coll[i].nextElementSibling.classList.toggle('hide');
                    coll[i].childNodes[1].classList.toggle('rotate');
                    checkActiveCategory = false;
                    break;
                  }
                }
              }

              coll[i].addEventListener('click', function() {
                var arrow = this.childNodes[1];
                arrow.classList.toggle('rotate');
                var content = this.nextElementSibling;
                content.classList.toggle('hide');
              });
            }

            document.addEventListener('DOMContentLoaded', function() {
              createToggler('#navToggler', '#docsNav', 'docsSliderActive');
              createToggler('#tocToggler', 'body', 'tocActive');

              var headings = document.querySelector('.toc-headings');
              headings && headings.addEventListener('click', function(event) {
                var el = event.target;
                while(el !== headings){
                  if (el.tagName === 'A') {
                    document.body.classList.remove('tocActive');
                    break;
                  } else{
                    el = el.parentNode;
                  }
                }
              }, false);

              function createToggler(togglerSelector, targetSelector, className) {
                var toggler = document.querySelector(togglerSelector);
                var target = document.querySelector(targetSelector);

                if (!toggler) {
                  return;
                }

                toggler.onclick = function(event) {
                  event.preventDefault();

                  target.classList.toggle(className);
                };
              }
            });
        </script></nav></div><div class="container mainContainer postContainer blogContainer"><div class="wrapper"><div class="posts"><div class="post"><header class="postHeader"><h1 class="postHeaderTitle"><a href="/blog/2020/06/23/jdk-gc-benchmarks-rematch">Performance of Modern Java on Data-Heavy Workloads: The Low-Latency Rematch</a></h1><p class="post-meta">June 23, 2020</p><div class="authorBlock"><p class="post-authorName"><a href="https://twitter.com/mtopolnik" target="_blank" rel="noreferrer noopener">Marko Topolnik</a></p><div class="authorPhoto"><a href="https://twitter.com/mtopolnik" target="_blank" rel="noreferrer noopener"><img src="https://i.imgur.com/xuavzce.jpg" alt="Marko Topolnik"/></a></div></div></header><article class="post-content"><div><span><p>This post is a part of a series:</p>
<ul>
<li><a href="/blog/2020/06/09/jdk-gc-benchmarks-part1">Part 1 (Intro and high-throughput streaming
benchmark)</a></li>
<li><a href="/blog/2020/06/09/jdk-gc-benchmarks-part2">Part 2 (batch workload benchmark)</a></li>
<li>Part 3 (you are here)</li>
<li><a href="/blog/2020/08/05/gc-tuning-for-jet">Part 4 (concurrent GC with green threads)</a></li>
<li><a href="/blog/2021/03/17/billion-events-per-second">Part 5 (billion events per second)</a></li>
</ul>
<p>This is a followup on Part 1 of the blog post series we started earlier
this month, analyzing the performance of modern JVMs on workloads that
are relevant to the use case of real-time stream processing.</p>
<p>As a quick recap, in Part 1 we tested the basic functionality of
<a href="https://github.com/hazelcast/hazelcast-jet">Hazelcast Jet</a> (sliding
window aggregation) on two types of workload: lightweight with a focus
on low latency, and heavyweight with a focus on the data pipeline
keeping up with high throughput and large aggregation state. For the
low-latency benchmarks we chose the JDK 14 as the most recent stable
version and three of its garbage collectors: Shenandoah, ZGC, and G1 GC.</p>
<p>Our finding that Shenandoah apparently fared worse than the other GCs
attracted some reactions, most notably from the Shenandoah team who
reproduced our finding, created an
<a href="https://bugs.openjdk.java.net/browse/JDK-8247358">issue</a>, came up with
a fix, and committed it to the jdk/jdk16 repository, all in the span of
a few days. The change pertains to the heuristics that decide how much
work the GC should do in the background in order to exactly match the
applications allocation rate. This component is called the <em>pacer</em>. It
was constantly detecting it's falling behind the application, triggering
a brief &quot;panic mode&quot; in order to catch up. The fix fine-tunes the
pacer's heuristics to make the background GC work more proactive.</p>
<p>Given this quick development, we wanted to test out the effects of the
fix, but also take the opportunity to zoom in on the low-latency
streaming case and make a more detailed analysis.</p>
<p>Here are our main conclusions:</p>
<ol>
<li>ZGC is still the winner and the only GC whose 99.99th percentile
latency stayed below 10 ms across almost all of our tested range</li>
<li>Shenandoah's pacer improvement showed a very strong effect, reducing
the latency by a factor of three, but still staying well above 10 ms
except in the very lowest part of our tested range</li>
<li>G1 kept its 99.99th percentile latency below 13 ms across a wide
range of throughputs</li>
</ol>
<h2><a class="anchor" aria-hidden="true" id="the-jdk-we-tested"></a><a href="#the-jdk-we-tested" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>The JDK We Tested</h2>
<p>Since this is all so fresh, we couldn't use an existing JDK release, not
even EA, to see the effects of the fix. JDK version 14.0.2 is slated to
be released on July 14. To nevertheless make progress, we took the
source code from the jdk14u tree, at the changeset number
<a href="http://hg.openjdk.java.net/jdk-updates/jdk14u/rev/e9d41bbaea38">57869:e9d41bbaea38</a>,
and applied the changeset number
<a href="https://hg.openjdk.java.net/jdk/jdk/rev/29b4bb22b5e2">59746:29b4bb22b5e2</a>
from the main jdk tree on top of it. The jdk14u tree is where JDK 14.0.2
will be released from and the changeset 59746:29b4bb22b5e2 applies the
patch resolving the mentioned Shenandoah issue.</p>
<h2><a class="anchor" aria-hidden="true" id="the-jvm-options"></a><a href="#the-jvm-options" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>The JVM Options</h2>
<p>There are two HotSpot JVM options whose default values change
automatically when you use the ZGC so we had to decide which choice to
make when testing the other garbage collectors.</p>
<ul>
<li><p><code>-XX:-UseBiasedLocking</code>: biased locking has for a while been under
criticism that it causes higher latency spikes due to bias revocation
that must be done within a GC safepoint. In the upcoming JDK version
15, biased locking will be <a href="https://openjdk.java.net/jeps/374">disabled by default and
deprecated</a>. Any low-latency Java
application should have this disabled and we disabled it in all our
measurements.</p></li>
<li><p><code>-XX:+UseNUMA</code>: Shenandoah and ZGC can query the NUMA layout of the
host machine and optimize their memory layout accordingly. The only
reason why Shenandoah doesn't do it by default is a general precaution
against suddenly changing the behavior for upgrading users, but the
precaution is no longer necessary. It will be <a href="https://openjdk.java.net/jeps/163">enabled by
default</a> in upcoming JDK versions,
and we saw no harm in enabling it in all cases as well. <strong>Late
update</strong>: G1 can also optimize for the NUMA layout, but we didn't use
<code>UseNUMA</code> for it. However, we also checked the c5.4xlarge instance
with <code>numactl</code> and it indicated that the entire machine was a single
NUMA node anyway.</p></li>
</ul>
<p>There is also a JVM feature that is simply incompatible with ZGC's
colored pointers: compressed object pointers. In other words, ZGC
applies <code>-XX:-UseCompressedOops</code> without the option to enable it.
A compressed pointer is just 32 bits long but handles heaps of up to
32 GB and it's usually beneficial to both memory usage and performance.
We left this option enabled for Shenandoah.</p>
<p>For the G1 collector, we also set <code>-XX:MaxGCPauseMillis=5</code>, same as in
the previous testing round, because the default of 200 milliseconds is
optimized for throughput and the G1 can give you much better latency
than that.</p>
<p>We performed all our tests on an EC2 c5.4xlarge instance. It has 16
vCPUs and 32 GB of RAM.</p>
<h2><a class="anchor" aria-hidden="true" id="the-data-pipeline"></a><a href="#the-data-pipeline" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>The Data Pipeline</h2>
<p>To get a more nuanced insight into the performance, we made some
improvements to the testing code. Whereas in the first iteration we just
reported the maximum latency, this time around we wanted to capture the
entire latency profile. To this end we had to increase the number of
reports per second the pipeline outputs. Initially we set it to 10 times
per second, a number which results in too few data points for the
latency chart. The pipeline in this round emits 100 reports per second.
The event rate and the length of the time window are the same: 1 million
events per second and 10 seconds, respectively. This results in 1,000
hashtables each holding 10,000 keys as the aggregation state. We tested
across a wide range of keyset sizes, starting from 5,000 up to 105,000.</p>
<p>Note that the size of the keyset, somewhat counterintuitively, does not
affect the size of the aggregation state. As long as the 10,000 input
events received during one time slice of 10 milliseconds all use
distinct keys, the state is fixed as described above. Only in the lowest
setting, 5,000, the state is half as large since every hashtable
contains just 5,000 keys.</p>
<p>What the keyset size does affect is allocation rate. The pipeline emits
the full keyset every 10 milliseconds. For example, with 50,000 keys
that's 5,000,000 result items per second. If we add to that the rate of
the input stream (a fixed million events per second), we get a value
that is a good proxy for the overall allocation rate. This is why we
chose combined input+output rate as the x-axis value in the charts that
we'll be showing below.</p>
<p>Here is the basic code of the pipeline, available on
<a href="https://github.com/mtopolnik/jet-gc-benchmark/blob/round-2/src/main/java/org/example/StreamingRound2.java">GitHub</a>:</p>
<pre><code class="hljs css language-java"><span class="token class-name">StreamStage</span><span class="token generics"><span class="token punctuation">&lt;</span><span class="token class-name">Long</span><span class="token punctuation">></span></span> source <span class="token operator">=</span> p<span class="token punctuation">.</span><span class="token function">readFrom</span><span class="token punctuation">(</span><span class="token function">longSource</span><span class="token punctuation">(</span><span class="token constant">EVENTS_PER_SECOND</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
                            <span class="token punctuation">.</span><span class="token function">withNativeTimestamps</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>
                            <span class="token punctuation">.</span><span class="token function">rebalance</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token class-name">StreamStage</span><span class="token generics"><span class="token punctuation">&lt;</span><span class="token class-name">Tuple2</span><span class="token punctuation">&lt;</span><span class="token class-name">Long</span><span class="token punctuation">,</span> <span class="token class-name">Long</span><span class="token punctuation">></span><span class="token punctuation">></span></span> latencies <span class="token operator">=</span> source
        <span class="token punctuation">.</span><span class="token function">groupingKey</span><span class="token punctuation">(</span>n <span class="token operator">-></span> n <span class="token operator">%</span> <span class="token constant">NUM_KEYS</span><span class="token punctuation">)</span>
        <span class="token punctuation">.</span><span class="token function">window</span><span class="token punctuation">(</span><span class="token function">sliding</span><span class="token punctuation">(</span><span class="token constant">WIN_SIZE_MILLIS</span><span class="token punctuation">,</span> <span class="token constant">SLIDING_STEP_MILLIS</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token punctuation">.</span><span class="token function">aggregate</span><span class="token punctuation">(</span><span class="token function">counting</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token punctuation">.</span><span class="token function">filter</span><span class="token punctuation">(</span>kwr <span class="token operator">-></span> kwr<span class="token punctuation">.</span><span class="token function">getKey</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">%</span> <span class="token constant">DIAGNOSTIC_KEYSET_DOWNSAMPLING_FACTOR</span> <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">)</span>
        <span class="token punctuation">.</span><span class="token function">mapStateful</span><span class="token punctuation">(</span><span class="token class-name">DetermineLatency</span><span class="token operator">::</span><span class="token keyword">new</span><span class="token punctuation">,</span> <span class="token class-name">DetermineLatency</span><span class="token operator">::</span><span class="token function">map</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

latencies<span class="token punctuation">.</span><span class="token function">filter</span><span class="token punctuation">(</span>t2 <span class="token operator">-></span> t2<span class="token punctuation">.</span><span class="token function">f0</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">&lt;</span> <span class="token constant">TOTAL_TIME_MILLIS</span><span class="token punctuation">)</span>
         <span class="token punctuation">.</span><span class="token function">map</span><span class="token punctuation">(</span>t2 <span class="token operator">-></span> <span class="token class-name">String</span><span class="token punctuation">.</span><span class="token function">format</span><span class="token punctuation">(</span><span class="token string">"%d,%d"</span><span class="token punctuation">,</span> t2<span class="token punctuation">.</span><span class="token function">f0</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> t2<span class="token punctuation">.</span><span class="token function">f1</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
         <span class="token punctuation">.</span><span class="token function">writeTo</span><span class="token punctuation">(</span><span class="token class-name">Sinks</span><span class="token punctuation">.</span><span class="token function">files</span><span class="token punctuation">(</span><span class="token string">"/home/ec2-user/laten"</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
latencies
      <span class="token punctuation">.</span><span class="token function">mapStateful</span><span class="token punctuation">(</span><span class="token class-name">RecordLatencyHistogram</span><span class="token operator">::</span><span class="token keyword">new</span><span class="token punctuation">,</span> <span class="token class-name">RecordLatencyHistogram</span><span class="token operator">::</span><span class="token function">map</span><span class="token punctuation">)</span>
      <span class="token punctuation">.</span><span class="token function">writeTo</span><span class="token punctuation">(</span><span class="token class-name">Sinks</span><span class="token punctuation">.</span><span class="token function">files</span><span class="token punctuation">(</span><span class="token string">"/home/ec2-user/bench"</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
</code></pre>
<p>The main part, sliding window aggregation, remains the same, but the
following stages that process the results are new. We write the data to
two files: <code>laten</code>, containing all the raw latency data points, and
<code>bench</code>, containing an <a href="https://hdrhistogram.github.io/HdrHistogram/plotFiles.html">HDR
Histogram</a>
of the latencies.</p>
<p>Another key difference is that, in the original post, we measured the
latency of <em>completing</em> to emit a result set, but here we measure the
latency of <em>starting</em> to emit it. Since we are changing the size of the
output, if we kept measuring the completion latency, we'd be introducing
a different amount of application-induced latency at each data point.</p>
<p>There's another, relatively minor technical point worth mentioning:
since we tested on a cloud server instance, we used Jet's client-server
mode, which means we separately start a Jet node and then deploy the
pipeline to it using Jet's command <code>jet submit</code>. The code available on
GitHub is the client code and the Jet server code was a build from the
Jet master branch before Jet 4.2 was released. We expect all the results
to be reproducible with the <a href="https://github.com/hazelcast/hazelcast-jet/releases/download/v4.2/hazelcast-jet-4.2.tar.gz">Jet 4.2
release</a>.</p>
<h2><a class="anchor" aria-hidden="true" id="what-exactly-we-measured"></a><a href="#what-exactly-we-measured" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>What Exactly We Measured</h2>
<p>We measured the latency as the timestamp at which the pipeline emits a
given result minus the timestamp to which the result pertains, giving us
end-to-end latency (the only kind the user actually cares about).</p>
<p>Keep especially in mind that latency does not equal a GC pause.
Normally, neither Shenandoah nor ZGC enter anything more than a
millisecond of GC pause, but their background work shares the limited
system capacity with the application. With G1 the equivalence is much
stronger and its 10-20 millisecond latencies are primarily the result of
GC pauses that long.</p>
<h2><a class="anchor" aria-hidden="true" id="the-measurements"></a><a href="#the-measurements" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>The Measurements</h2>
<p>To come up with the charts below, for each data point we let the
pipeline warm up for 20 seconds and then gathered the latencies for 4
minutes, collecting 24,000 samples.</p>
<p>Here is the latency histogram taken at 2 million items per second,
close to the bottom of our range:</p>
<p><img src="/blog/assets/2020-06-23-histo-2m.png" alt="Latency on JDK 14.0.2 pre-release, 2M items per second"></p>
<p>Unpatched Shenandoah seems like the winner, except for the single
worst-case latency. With the patch applied, latency increases sooner but
more gently and doesn't have a strong peak. ZGC comes somewhere between,
but overall all three cases show pretty similar behavior. G1 is clearly
worse and its latency exceeds the 10 ms mark before even reaching the
99th percentile. Since our pipeline emits a new result set ever 10 ms,
we shall consider 10 ms as the cutoff point: everything above 10 ms
should be considered a failure for our use case.</p>
<p>Next, let's take a look at the latencies after increasing the throughput
a bit, to 3 million items per second:</p>
<p><img src="/blog/assets/2020-06-23-histo-3m.png" alt="Latency on JDK 14.0.2 pre-release, 3M items per second"></p>
<p>Wow, what an unexpected difference! Now we can clearly see the pacer
improvement doing its thing, lowering the latency about threefold.
However, even with the improvement, Shenandoah unfortunately crosses the
10 ms mark pretty early, below the 99th percentile, and is worse than G1
at almost every percentile. ZGC and G1 score basically the same as
before.</p>
<p>Note also the very regular shape of the pale blue curve (unpatched
Shenandoah): this is a symptom of the way a single bad event trickles
down into the lower latency percentiles. For example, if one result is
late by 50 ms, that means it has already caused the next four results to
have at least the latencies of 40, 30, 20, and 10 ms, even if they would
be emitted instantaneously.</p>
<p>Next, let's zoom out to an overview of the entire range of throughputs
we benchmarked, taking the 99.99%ile as the reference point and showing
its dependence on throughput. To paint an intuitive picture, 99.99%
latency tells you that, in any span of 100 seconds you look at, you're
likely to find a latency spike at least that large. Here's the chart:</p>
<p><img src="/blog/assets/2020-06-23-latencies-jdk14.png" alt="Latencies on JDK 14.0.2 pre-release"></p>
<p>Here are some things to note:</p>
<ol>
<li>ZGC stays below 10 ms over a large part of the range, up to 8 M items
per second. This makes it not just the winner, but the only choice
for the range from 2 million to 8 million items per second.</li>
<li>The G1 collector is unphased by the differences in throughput. While
its latency is never under 10 milliseconds, it keeps its level over
the entire tested range and more. Its latency even improves a bit
with higher loads.</li>
<li>At 9.5 M items per second, ZGC shows a remarkable recovery.
Sandwiched between the latencies of 92 and 209 milliseconds, at this
exact throughput it achieves 10 ms latency! We of course thought it
was a measurement error and repeated it for three times, but the
result was consistent. Maybe there's a lesson in there for the ZGC
engineers.</li>
</ol>
<h2><a class="anchor" aria-hidden="true" id="a-sneak-peek-into-upcoming-versions"></a><a href="#a-sneak-peek-into-upcoming-versions" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>A Sneak Peek into Upcoming Versions</h2>
<p>As a preview into what's coming up in OpenJDK, we also took a look at
the <a href="https://download.java.net/java/early_access/jdk15/28/GPL/openjdk-15-ea+28_linux-x64_bin.tar.gz">Early Access release 27 of JDK
15</a>.
Shenandoah's pacer improvement is not applied in it, so to properly test
Shenandoah's prospects we used a build available at
<a href="https://builds.shipilev.net/openjdk-jdk/">builds.shipilev.net/openjdk-jdk</a>,
specifically one that reports its version as <code>build 16-testing+0-builds.shipilev.net-openjdk-jdk-b1282-20200611</code>. Out of
interest we also doubled our throughput range to capture more of the
behavior after the latency exceeds 10 milliseconds. Here's what we got:</p>
<p><img src="/blog/assets/2020-06-23-latencies-latest.png" alt="Latencies on upcoming JDK versions"></p>
<p>We can see a nice incremental improvement for the ZGC: less than 5 ms
latencies at throughputs below 5 M/s. Shenandoah's curve is even a bit
worse at 2.5-3 M per second, but generally pretty similar. At higher
loads we can see ZGC's failure mode is quite a bit more severe than
Shenandoah's, although just how bad the latency gets doesn't affect the
bottom line of a scenario where everything above 10 ms is already a
failure.</p>
<p>The wider chart also gives better insight into the stability of G1,
keeping itself below 20 ms all the way up to 20 M items per second.</p>
<p><em>If you enjoyed reading this post, check out Jet at
<a href="https://github.com/hazelcast/hazelcast-jet">GitHub</a> and give us a
star!</em></p>
</span></div></article></div><div class="post"><header class="postHeader"><h1 class="postHeaderTitle"><a href="/blog/2020/06/09/jdk-gc-benchmarks-part2">Performance of Modern Java on Data-Heavy Workloads: Batch Processing</a></h1><p class="post-meta">June 9, 2020</p><div class="authorBlock"><p class="post-authorName"><a href="https://twitter.com/mtopolnik" target="_blank" rel="noreferrer noopener">Marko Topolnik</a></p><div class="authorPhoto"><a href="https://twitter.com/mtopolnik" target="_blank" rel="noreferrer noopener"><img src="https://i.imgur.com/xuavzce.jpg" alt="Marko Topolnik"/></a></div></div></header><article class="post-content"><div><span><p>This post is a part of a series:</p>
<ul>
<li><a href="/blog/2020/06/09/jdk-gc-benchmarks-part1">Part 1 (Intro and high-throughput streaming
benchmark)</a></li>
<li>Part 2 (you are here)</li>
<li><a href="/blog/2020/06/23/jdk-gc-benchmarks-rematch">Part 3 (low-latency benchmark)</a></li>
<li><a href="/blog/2020/08/05/gc-tuning-for-jet">Part 4 (concurrent GC with green threads)</a></li>
<li><a href="/blog/2021/03/17/billion-events-per-second">Part 5 (billion events per second)</a></li>
</ul>
<h2><a class="anchor" aria-hidden="true" id="batch-pipeline-benchmark"></a><a href="#batch-pipeline-benchmark" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Batch Pipeline Benchmark</h2>
<p>A batch pipeline processes a finite amount of stored data. There are no
running results, we need the output of the aggregate function applied to
the entire dataset. This changes our performance requirements: the key
factor in streaming, latency, doesn't exist here since we are not
processing data in real time. The only metric that matters is the total
run time of the pipeline.</p>
<p>For this reason we considered the Parallel GC as a relevant candidate.
In the first testing round, on a single node, it actually delivered the
best throughput (but only after GC tuning). However, it achieves that
throughput at the expense of GC pause duration. In a cluster, whenever
any node enters a GC pause, it stalls the whole data pipeline. Since
individual nodes enter GC pauses at different times, the amount of time
spent in GC goes up with every node you add to the cluster. We explored
this effect by comparing single-node tests with tests in a three-node
cluster.</p>
<p>On the flip side, we did not consider the experimental low-latency
collectors in this round since their very short GC pauses have no effect
on the test result, and they achieve them at the expense of throughput.</p>
<h3><a class="anchor" aria-hidden="true" id="single-node-benchmark-the-pipeline"></a><a href="#single-node-benchmark-the-pipeline" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Single-Node Benchmark: The Pipeline</h3>
<p>For the single-node batch benchmark we used this simple pipeline, full
code on
<a href="https://github.com/mtopolnik/jet-gc-benchmark/blob/master/src/main/java/org/example/BatchBenchmark.java">GitHub</a>:</p>
<pre><code class="hljs css language-java">p<span class="token punctuation">.</span><span class="token function">readFrom</span><span class="token punctuation">(</span>longSource<span class="token punctuation">)</span>
 <span class="token punctuation">.</span><span class="token function">rebalance</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment">// Introduced in Jet 4.2</span>
 <span class="token punctuation">.</span><span class="token function">groupingKey</span><span class="token punctuation">(</span>n <span class="token operator">-></span> n <span class="token operator">%</span> <span class="token constant">NUM_KEYS</span><span class="token punctuation">)</span>
 <span class="token punctuation">.</span><span class="token function">aggregate</span><span class="token punctuation">(</span><span class="token function">summingLong</span><span class="token punctuation">(</span>n <span class="token operator">-></span> n<span class="token punctuation">)</span><span class="token punctuation">)</span>
 <span class="token punctuation">.</span><span class="token function">filter</span><span class="token punctuation">(</span>e <span class="token operator">-></span> <span class="token punctuation">(</span>e<span class="token punctuation">.</span><span class="token function">getKey</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">&amp;</span> <span class="token number">0</span>xFF_FFFFL<span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">)</span>
 <span class="token punctuation">.</span><span class="token function">writeTo</span><span class="token punctuation">(</span><span class="token class-name">Sinks</span><span class="token punctuation">.</span><span class="token function">logger</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre>
<p>The source is again a self-contained mock source that just emits a
sequence of <code>long</code> numbers and the key function is defined so that the
grouping key cycles through the key space: 0, 1, 2, ..., <code>NUM_KEYS</code>, 0,
1, 2, ... This means that, over the first cycle, the pipeline observes
all the keys and builds up a fixed data structure to hold the
aggregation results. Over the following cycles it just updates the
existing data. This aligns perfectly with the Generational Garbage
Hypothesis: the objects either last through the entire computation or
are short-lived temporary objects that become garbage very soon after
creation.</p>
<p>We let our source emit 400 million items and had 100 million distinct
keys, so we cycled four times over the same keys.</p>
<p>The <code>.rebalance()</code> operator changes Jet's default <a href="/docs/concepts/dag#group-and-aggregate-transform-needs-data-partitioning">two-stage
aggregation</a>
to single-stage. It exhibited more predictable behavior in our
benchmark.</p>
<p>We also tested a variant where the aggregate operation uses a boxed
<code>Long</code> instance as state, producing garbage every time the running score
is updated. In this case many objects die after having spent substantial
time in the old generation. For this variant we had to reduce the number
of keys to 70 million, with 100 million the GC pressure was too high.</p>
<p>For the batch pipeline we didn't focus on the low-latency collectors
since they have nothing to offer in this case. Also, because we saw
earlier that JDK 14 performs much the same as JDK 11, we just ran one
test to confirm it, but otherwise focused on JDK 8 vs. JDK 11 and
compared the JDK 8 default Parallel collector with G1.</p>
<h3><a class="anchor" aria-hidden="true" id="single-node-benchmark-the-results"></a><a href="#single-node-benchmark-the-results" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Single-Node Benchmark: The Results</h3>
<p>For single-node testing, we ran the benchmark on a laptop with 16 GB RAM
and a 6-core Intel Core i7. We used a heap size of 10 GB.</p>
<p>Initially we got very bad performance out of the Parallel collector and
had to resort to GC tuning. For this purpose we highly recommend using
VisualVM and its Visual GC plugin. When you set the frame rate to the
highest setting (10 FPS), you can enjoy a very fine-grained visual
insight into how the interplay between your application's allocation and
the GC works out. By watching these live animations for a while, we
realized that the main issue was a too large slice of RAM given to the
new generation. By default the ratio between Old and New generations is
just 2:1, and it is not dynamically adaptable at runtime. Based on this
we decided to try with <code>-XX:NewRatio=8</code> and it completely changed the
picture. Now Parallel was turning in the best times overall. We also
used <code>-XX:MaxTenuringThreshold=2</code> to reduce the copying of data between
the Survivor spaces, since in the pipeline the temporary objects die
pretty soon.</p>
<p>Now, on to the results. The only relevant metric in this batch pipeline
benchmark is the time for the job to complete. To visualize the results
we took the reciprocal of that, so the charts show throughput in items
per second. Here are our results on a single node:</p>
<p><img src="/blog/assets/2020-06-01-batch-mutable.png" alt="Single-node Batch pipeline with garbage-free aggregation"></p>
<p><img src="/blog/assets/2020-06-01-batch-boxed.png" alt="Single-node Batch pipeline with garbage-producing aggregation"></p>
<p>Comparing the two charts we can see that garbage-free aggregation gave a
throughput boost of around 30-35%, and that's despite the larger keyset
we used for it. G1 on JDK 8 was the worst performer and the fine-tuned
Parallel on JDK 11 was the best. G1 on JDK 11 wasn't far behind. Note
that we didn't have to touch anything in the configuration of G1, which
is an important fact. GC tuning is highly case-specific, the results may
dramatically change with e.g., more data, and it must be applied to the
entire cluster, making it specifically tuned for one kind of workload.</p>
<p>Here's the performance of the default Parallel GC compared to the tuned
version we used for testing:</p>
<p><img src="/blog/assets/2020-06-01-batch-parallel.png" alt="Throughput of the Parallel Collector with and without tuning"></p>
<p>With 10 GB of heap it failed completely, stuck in back-to-back Full GC
operations each taking about 7 seconds. With more heap it managed to
make some progress, but was still hampered with very frequent Full GCs.
Note that we got the above results for the most favorable case, with
garbage-free aggregation.</p>
<h3><a class="anchor" aria-hidden="true" id="three-node-cluster-benchmark-the-pipeline"></a><a href="#three-node-cluster-benchmark-the-pipeline" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Three-Node Cluster Benchmark: The Pipeline</h3>
<p>To properly benchmark in the cluster, we had to use a bit more complex
<a href="https://github.com/mtopolnik/jet-gc-benchmark/blob/master/src/main/java/org/example/ClusterBatchBenchmark.java">pipeline</a>:</p>
<pre><code class="hljs css language-java">p<span class="token punctuation">.</span><span class="token function">readFrom</span><span class="token punctuation">(</span><span class="token function">longSource</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
 <span class="token punctuation">.</span><span class="token function">rebalance</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
 <span class="token punctuation">.</span><span class="token function">flatMap</span><span class="token punctuation">(</span>n <span class="token operator">-></span> <span class="token punctuation">{</span>
     <span class="token class-name">Long</span><span class="token punctuation">[</span><span class="token punctuation">]</span> items <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">Long</span><span class="token punctuation">[</span><span class="token constant">SOURCE_STEP</span><span class="token punctuation">]</span><span class="token punctuation">;</span>
     <span class="token class-name">Arrays</span><span class="token punctuation">.</span><span class="token function">setAll</span><span class="token punctuation">(</span>items<span class="token punctuation">,</span> i <span class="token operator">-></span> n <span class="token operator">+</span> i<span class="token punctuation">)</span><span class="token punctuation">;</span>
     <span class="token keyword">return</span> <span class="token function">traverseArray</span><span class="token punctuation">(</span>items<span class="token punctuation">)</span><span class="token punctuation">;</span>
 <span class="token punctuation">}</span><span class="token punctuation">)</span>
 <span class="token punctuation">.</span><span class="token function">rebalance</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
 <span class="token punctuation">.</span><span class="token function">groupingKey</span><span class="token punctuation">(</span>n <span class="token operator">-></span> n <span class="token operator">%</span> <span class="token constant">NUM_KEYS</span><span class="token punctuation">)</span>
 <span class="token punctuation">.</span><span class="token function">aggregate</span><span class="token punctuation">(</span><span class="token class-name">AggregateOperations</span><span class="token punctuation">.</span><span class="token function">summingLong</span><span class="token punctuation">(</span>n <span class="token operator">-></span> n<span class="token punctuation">)</span><span class="token punctuation">)</span>
 <span class="token punctuation">.</span><span class="token function">filter</span><span class="token punctuation">(</span>e <span class="token operator">-></span> e<span class="token punctuation">.</span><span class="token function">getKey</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">%</span> <span class="token number">1_000_000</span> <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">)</span>
 <span class="token punctuation">.</span><span class="token function">writeTo</span><span class="token punctuation">(</span><span class="token class-name">Sinks</span><span class="token punctuation">.</span><span class="token function">logger</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token punctuation">;</span>
</code></pre>
<p>Since the source is non-parallel, we applied some optimizations so it
doesn't become a bottleneck. We let the source emit the numbers 0, 10,
20, ... and then applied a parallelized <code>flatMap</code> stage that
interpolates the missing numbers. We also used <code>rebalance()</code> between the
source and <code>flatMap</code>, spreading the data across the cluster. We applied
rebalancing again before entering the main stage, keyed aggregation.
After the aggregation stage we first reduce the output to every
millionth key-value pair and then send it to the logger. We used one
billion data items and a keyset of half a billion.</p>
<p>Same as on single-node, we tested both this pipeline, with a garbage-free
aggregation, and a modified one with garbage-producing aggregation.</p>
<h3><a class="anchor" aria-hidden="true" id="three-node-cluster-benchmark-the-results"></a><a href="#three-node-cluster-benchmark-the-results" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Three-Node Cluster Benchmark: The Results</h3>
<p>We performed this benchmark on an AWS cluster of three c5d.4xlarge
instances. They have 16 virtualized CPU cores and 32 GB of RAM. The
network is 10 Gbit/s. Here are the results:</p>
<p><img src="/blog/assets/2020-06-01-batch-cluster-mutable.png" alt="3-Node Batch pipeline with garbage-free aggregation"></p>
<p><img src="/blog/assets/2020-06-01-batch-cluster-boxed.png" alt="3-Node Batch pipeline with garbage-producing aggregation"></p>
<p>In passing, let's note the overall increase in throughput compared to
single-node benchmarks, about three times. That's the advantage of
distributed processing. As for collectors, G1 on JDK 11 is the clear
winner in both tests. Another striking result is the almost nonexistent
bar for G1 on JDK 8, however there's a deeper story here that affects
other measurements as well, for example the apparent advantage of
Parallel GC on JDK 8 vs. JDK 11. It has to do with the effect we noted
at the outset: a GC pause on any one member halts the processing on the
entire cluster. G1 on JDK 8 enters very long GC pauses, more than a
minute. This is enough to trigger the cluster's failure detector and
consider the node dead. The job fails, the cluster reshapes itself, and
then the job restarts on just two nodes. This, naturally, fails even
sooner because there's more data on each member. In the meantime the
kicked-out node has rejoined, so the job restarts on two nodes again,
but different ones. We end up in an endless loop of job restarts.</p>
<p>The Parallel collector's GC pauses stopped short of bringing down the
cluster, but it fared significantly worse than in single-node tests.
Here it was 30% behind the G1 on JDK 11. With a bigger cluster this
would get even worse.</p>
<p>Compared to all other tests, it is surprising to see Parallel on JDK 8
win over JDK 11, however this is due to a very lucky coincidence that,
in those particular test runs, the Full GC pauses got synchronized on
all nodes, parallelizing the effort of the GC. Clearly, this is not a
reliable effect.</p>
<p>Even though in the particular benchmark setup which we report here, we
didn't observe the catastrophic consequence of long GC pauses on cluster
stability while using the Parallel collector, it is more of a chance
outcome. In other tests, where we used a larger heap and more data, or
the same heap but with less headroom left, the Parallel collector did
cause the same damage. However, even when it doesn't cause outright
failure, the charts show the advantage it had on a single node has
disappeared. You can expect the results to get worse with each further
node you add to the cluster.</p>
<p>The JDK 11 G1 collector, on the other hand, was producing GC pauses of a
sufficiently short duration that the rest of the pipeline didn't get
stalled. The pipeline has mechanisms that dampen out short hiccups and
as long as the GC pauses are within acceptable limits (up to some 150
ms), the effect of GC stays local.</p>
<p><em>If you enjoyed reading this post, check out Jet at
<a href="https://github.com/hazelcast/hazelcast-jet">GitHub</a> and give us a
star!</em></p>
</span></div></article></div><div class="post"><header class="postHeader"><h1 class="postHeaderTitle"><a href="/blog/2020/06/09/jdk-gc-benchmarks-part1">Performance of Modern Java on Data-Heavy Workloads: Real-Time Streaming</a></h1><p class="post-meta">June 9, 2020</p><div class="authorBlock"><p class="post-authorName"><a href="https://twitter.com/mtopolnik" target="_blank" rel="noreferrer noopener">Marko Topolnik</a></p><div class="authorPhoto"><a href="https://twitter.com/mtopolnik" target="_blank" rel="noreferrer noopener"><img src="https://i.imgur.com/xuavzce.jpg" alt="Marko Topolnik"/></a></div></div></header><article class="post-content"><div><span><p>This post is a part of a series:</p>
<ul>
<li>Part 1 (you are here)</li>
<li><a href="/blog/2020/06/09/jdk-gc-benchmarks-part2">Part 2 (batch workload benchmark)</a></li>
<li><a href="/blog/2020/06/23/jdk-gc-benchmarks-rematch">Part 3 (low-latency benchmark)</a></li>
<li><a href="/blog/2020/08/05/gc-tuning-for-jet">Part 4 (concurrent GC with green threads)</a></li>
<li><a href="/blog/2021/03/17/billion-events-per-second">Part 5 (billion events per second)</a></li>
</ul>
<p>The Java runtime has been evolving more rapidly in recent years and,
after 15 years, we finally got a new default garbage collector: the
G1. Two more GCs are on their way to production and are available as
experimental features: Oracle's ZGC and OpenJDK's Shenandoah. We at
Hazelcast thought it was time to put all these new options to the test
and find which choices work well with workloads typical for our
distributed stream processing engine, <a href="https://github.com/hazelcast/hazelcast-jet">Hazelcast Jet</a>.</p>
<p>Jet is being used for a broad spectrum of use cases, with different
latency and throughput requirements. Here are three important
categories:</p>
<ol>
<li>Low-latency unbounded stream processing, with moderate state.
Example: detecting trends in 100 Hz sensor data from 10,000 devices
and sending corrective feedback within 10-20 milliseconds.</li>
<li>High-throughput, large-state unbounded stream processing. Example:
tracking GPS locations of millions of users, inferring their velocity
vectors.</li>
<li>Old-school batch processing of big data volumes. The relevant measure
is time to complete, which implies a high throughput demand. Example:
analyzing a day's worth of stock trading data to update the risk
exposure of a given portfolio.</li>
</ol>
<p>At the outset, we can observe the following:</p>
<ul>
<li>in scenario 1 the latency requirements enter the danger zone of GC
pauses: 100 milliseconds, something traditionally considered an
excellent result for a worst-case GC pause, may be a showstopper for
many use cases</li>
<li>scenarios 2 and 3 are similar in terms of demands on the garbage
collector. Less strict latency, but large pressure on the tenured
generation</li>
<li>scenario 2 is tougher because latency, even if less so than in
scenario 1, is still relevant</li>
</ul>
<p>We tried the following combinations:</p>
<ol>
<li>JDK 8 with the default Parallel collector and the optional
ConcurrentMarkSweep and G1</li>
<li>JDK 11 with the default G1 collector and the optional Parallel</li>
<li>JDK 14 with the default G1 as well as the experimental ZGC and
Shenandoah</li>
</ol>
<p>And here are our overall conclusions:</p>
<ol>
<li>On modern JDK versions, the G1 is one monster of a collector. It
handles heaps of dozens of GB with ease (we tried 60 GB), keeping
maximum GC pauses within 200 ms. Under extreme pressure it doesn't
show brittleness with catastrophic failure modes. Instead the Full GC
pauses rise into the low seconds range. Its Achilles' heel is the
upper bound on the GC pause in favorable low-pressure conditions,
which we couldn't push lower than 20-25 ms.</li>
<li>JDK 8 is an antiquated runtime. The default Parallel collector enters
huge Full GC pauses and the G1, although having less frequent Full
GCs, is stuck in an old version that uses just one thread to perform
it, resulting in even longer pauses. Even on a moderate heap of 12
GB, the pauses were exceeding 20 seconds for Parallel and a full
minute for G1. The ConcurrentMarkSweep collector is strictly worse
than G1 in all scenarios, and its failure mode are multi-minute Full
GC pauses.</li>
<li>The ZGC, while allowing substantially less throughput than G1, was
very good in that one weak area of G1, occasionally increasing our
latency by up to 10 ms under light load.</li>
<li>Shenandoah was a disappointment with occasional, but nevertheless
regular, latency spikes up to 220 ms in the low-pressure regime.</li>
<li>Neither ZGC nor Shenandoah showed as smooth failure modes as G1. They
exhibited brittleness, with the low-latency regime suddenly giving
way to very long pauses and even OOMEs.</li>
</ol>
<p>This post is Part 1 of a two-part series and presents our findings for
the two streaming scenarios. In <a href="/blog/2020/06/09/jdk-gc-benchmarks-part2">Part
2</a> we'll present the results
for batch processing.</p>
<h2><a class="anchor" aria-hidden="true" id="streaming-pipeline-benchmark"></a><a href="#streaming-pipeline-benchmark" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Streaming Pipeline Benchmark</h2>
<p>For the streaming benchmarks, we used the code available
<a href="https://github.com/mtopolnik/jet-gc-benchmark/blob/master/src/main/java/org/example/StreamingBenchmark.java">here</a>,
with some minor variations between the tests. Here is the main part, the
Jet pipeline:</p>
<pre><code class="hljs css language-java"><span class="token class-name">StreamStage</span><span class="token generics"><span class="token punctuation">&lt;</span><span class="token class-name">Long</span><span class="token punctuation">></span></span> source <span class="token operator">=</span> p<span class="token punctuation">.</span><span class="token function">readFrom</span><span class="token punctuation">(</span><span class="token function">longSource</span><span class="token punctuation">(</span><span class="token constant">ITEMS_PER_SECOND</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
                            <span class="token punctuation">.</span><span class="token function">withNativeTimestamps</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>
                            <span class="token punctuation">.</span><span class="token function">rebalance</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span> <span class="token comment">// Introduced in Jet 4.2</span>
source<span class="token punctuation">.</span><span class="token function">groupingKey</span><span class="token punctuation">(</span>n <span class="token operator">-></span> n <span class="token operator">%</span> <span class="token constant">NUM_KEYS</span><span class="token punctuation">)</span>
      <span class="token punctuation">.</span><span class="token function">window</span><span class="token punctuation">(</span><span class="token function">sliding</span><span class="token punctuation">(</span><span class="token constant">SECONDS</span><span class="token punctuation">.</span><span class="token function">toMillis</span><span class="token punctuation">(</span><span class="token constant">WIN_SIZE_SECONDS</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token constant">SLIDING_STEP_MILLIS</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
      <span class="token punctuation">.</span><span class="token function">aggregate</span><span class="token punctuation">(</span><span class="token function">counting</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
      <span class="token punctuation">.</span><span class="token function">filter</span><span class="token punctuation">(</span>kwr <span class="token operator">-></span> kwr<span class="token punctuation">.</span><span class="token function">getKey</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">%</span> <span class="token constant">DIAGNOSTIC_KEYSET_DOWNSAMPLING_FACTOR</span> <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">)</span>
      <span class="token punctuation">.</span><span class="token function">window</span><span class="token punctuation">(</span><span class="token function">tumbling</span><span class="token punctuation">(</span><span class="token constant">SLIDING_STEP_MILLIS</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
      <span class="token punctuation">.</span><span class="token function">aggregate</span><span class="token punctuation">(</span><span class="token function">counting</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
      <span class="token punctuation">.</span><span class="token function">writeTo</span><span class="token punctuation">(</span><span class="token class-name">Sinks</span><span class="token punctuation">.</span><span class="token function">logger</span><span class="token punctuation">(</span>wr <span class="token operator">-></span> <span class="token class-name">String</span><span class="token punctuation">.</span><span class="token function">format</span><span class="token punctuation">(</span><span class="token string">"time %,d: latency %,d ms, cca. %,d keys"</span><span class="token punctuation">,</span>
              <span class="token function">simpleTime</span><span class="token punctuation">(</span>wr<span class="token punctuation">.</span><span class="token function">end</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
              <span class="token constant">NANOSECONDS</span><span class="token punctuation">.</span><span class="token function">toMillis</span><span class="token punctuation">(</span><span class="token class-name">System</span><span class="token punctuation">.</span><span class="token function">nanoTime</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">-</span> wr<span class="token punctuation">.</span><span class="token function">end</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
              wr<span class="token punctuation">.</span><span class="token function">result</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">*</span> <span class="token constant">DIAGNOSTIC_KEYSET_DOWNSAMPLING_FACTOR</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
</code></pre>
<p>This pipeline represents use cases with an unbounded event stream where
the engine is asked to perform sliding window aggregation. You need this
kind of aggregation, for example, to obtain the time derivative of a
changing quantity, remove high-frequency noise from the data (smoothing)
or measure the intensity of the occurrence of some event (events per
second). The engine can first split the stream by some category (for
example, each distinct IoT device or smartphone) into substreams and
then independently track the aggregated value in each of them. In
Hazelcast Jet the sliding window moves in fixed-size steps that you
configure. For example, with a sliding step of 1 second you get a
complete set of results every second, and if the window size is 1
minute, the results reflect the events that occurred within the last
minute.</p>
<p>Some notes:</p>
<p>The code is entirely self-contained with no outside data sources or
sinks. We use a mock data source that simulates an event stream with
exactly the chosen number of events per second. Consecutive event
timestamps are an equal amount of time apart. The source never emits an
event whose timestamp is still in the future, but otherwise emits them
as fast as possible.</p>
<p>If the pipeline falls behind, events will be &quot;buffered&quot; but without any
storage. After falling behind, the pipeline must catch up by ingesting
data as fast as it can. Since our source is non-parallel, the limit on
its throughput was about 2.2 million events per second. We used 1
million simulated events per second, leaving a catching-up headroom of
1.2 million per second.</p>
<p>The pipeline measures its own latency by comparing the timestamp of an
emitted sliding window result with the actual wall-clock time. In more
detail, there are two aggregation stages with filtering between them. A
single sliding window result consists of many items, each for one
substream, and we're interested in the latency of the last-emitted
item. For this reason we first filter out most of the output, keeping
every 10,000th entry, and then direct the thinned-out stream to the
second, non-keyed tumbling window stage that notes the result size and
measures the latency. Non-keyed aggregation is not parallelized, so we
get a single point of measurement. The filtering stage is parallel and
data-local so the impact of the additional aggregation step is very
small (well below 1 ms).</p>
<p>We used a trivial aggregate function: counting, in effect obtaining the
events/second metric of the stream. It has minimal state (a single
<code>long</code> number) and produces no garbage. For any given heap usage in
gigabytes, such a small state per key implies the worst case for the
garbage collector: a very large number of objects. GC overheads scale
much more with object count than heap size. We also tested a variant
that computes the same aggregate function, but with a different
implementation that produces garbage.</p>
<p>We performed most of the streaming benchmarks on a single node since our
focus was the effect of memory management on pipeline performance and
network latency just adds noise into the picture. We repeated some key
tests on a three-node Amazon EC2 cluster to validate our prediction that
cluster performance won't affect our conclusions. You can find a more
detailed justification for this towards the end of <a href="/blog/2020/06/09/jdk-gc-benchmarks-part2">Part
2</a>.</p>
<p>We excluded the Parallel collector from the results for streaming
workloads because the latency spikes it introduces would be unacceptable
in pretty much any real-life scenario.</p>
<h3><a class="anchor" aria-hidden="true" id="scenario-1-low-latency-moderate-state"></a><a href="#scenario-1-low-latency-moderate-state" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Scenario 1: Low Latency, Moderate State</h3>
<p>For the first scenario we used these parameters:</p>
<ul>
<li>OpenJDK 14</li>
<li>JVM heap size 4 gigabytes</li>
<li>for G1, -XX:MaxGCPauseMillis=5</li>
<li>1 million events per second</li>
<li>50,000 distinct keys</li>
<li>30-second window sliding by 0.1 second</li>
</ul>
<p>In this scenario there's less than 1 GB heap usage. The collector is not
under high pressure, it has enough time to perform concurrent GC in the
background. These are the maximum pipeline latencies we observed with
the three garbage collectors we tested:</p>
<p><img src="/blog/assets/2020-06-01-light-streaming-latency.png" alt="Pipeline Latency with Light Streaming"></p>
<p>Note that these numbers include a fixed time of about 3 milliseconds to
emit the window results. The chart is pretty self-explanatory: the
default collector, G1, is pretty good on its own, but if you need even
better latency, you can use the experimental ZGC collector. We couldn't
reduce the latency spikes below 10 milliseconds, however we did note
that, in the case of ZGC and Shenandoah, they weren't due to outright GC
pauses but rather short periods of increased background GC work.
Shenandoah's overheads occasionally raised latency above 200 ms.</p>
<h3><a class="anchor" aria-hidden="true" id="scenario-2-large-state-less-strict-latency"></a><a href="#scenario-2-large-state-less-strict-latency" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Scenario 2: Large State, Less Strict Latency</h3>
<p>In scenario 2 we assume that, for various reasons outside our control,
(e.g., mobile network), the latency can grow into low seconds, which
relaxes the requirements we must impose on our stream processing
pipeline. On the other hand, we may be dealing with much larger data
volumes, on the order of millions or dozens of millions of keys.</p>
<p>In this scenario we can provision our hardware so it's heavily utilized,
relying on the GC to manage a large heap instead of spreading out the
data over many cluster nodes.</p>
<p>We performed many tests with different combinations to find out how the
interplay between various factors causes the runtime to either keep up
or fall behind. In the end we found two parameters that determine this:</p>
<ol>
<li>number of entries stored in the aggregation state</li>
<li>demand on the catching-up throughput</li>
</ol>
<p>The first one corresponds to the number of objects in the tenured
generation. Sliding window aggregation retains objects for a significant
time (the length of the window) and then releases them. This goes
directly against the Generational Garbage Hypothesis, which states that
objects will either die young or live forever. This regime puts the
strongest pressure on the GC, and since the GC effort scales with the
number of live objects, performance is highly sensitive to this
parameter.</p>
<p>The second parameter relates to how much GC overhead the application can
tolerate. To explain it better, let's use some diagrams. A pipeline
performing windowed aggregation goes through three distinct steps:</p>
<ol>
<li>processing events in real time, as they arrive</li>
<li>emitting the sliding window results</li>
<li>catching up with the events received while in step 2</li>
</ol>
<p>The three phases can be visualized as follows:</p>
<p><img src="/blog/assets/2020-06-01-sliding-window-1.png" alt="Phases of the sliding window computation"></p>
<p>If emitting the window result takes longer, we get a situation like this:</p>
<p><img src="/blog/assets/2020-06-01-sliding-window-2.png" alt="Phases of the sliding window computation"></p>
<p>Now the headroom has shrunk to almost nothing, the pipeline is barely
keeping up, and any temporary hiccups like an occasional GC pause will
cause latency to grow and recover at a very slow pace.</p>
<p>If we change this picture and present just the average event ingestion
rate after window emission, we get this:</p>
<p><img src="/blog/assets/2020-06-01-sliding-window-3.png" alt="Phases of the sliding window computation"></p>
<p>We call the height of the yellow rectangle &quot;catchup demand&quot;: it is the
demand on the throughput of the source. If it exceeds the actual maximum
throughput, the pipeline fails.</p>
<p>This is how it would look if window emission took way too long:</p>
<p><img src="/blog/assets/2020-06-01-sliding-window-4.png" alt="Phases of the sliding window computation"></p>
<p>The area of the red and the yellow rectangles is fixed, it corresponds
to the amount of data that must flow through the pipeline. Basically,
the red rectangle &quot;squeezes out&quot; the yellow one. But the yellow
rectangle's height is actually limited, in our case to 2.2 million
events per second. So whenever it would be taller than the limit, we'd
have a failing pipeline whose latency grows without bounds.</p>
<p>We worked out the formulas that predict the sizes of the rectangles for
a given combination of event rate, window size, sliding step and keyset
size, so that we could determine the catchup demand for each case.</p>
<p>Now that we have two more-or-less independent parameters derived from
many more parameters describing each individual setup, we can create a
2D-chart where each benchmark run has a point on it. We assigned a color
to each point, telling us whether the given combination worked or
failed. For example, for JDK 14 with G1 on a developer's laptop, we got
this picture:</p>
<p><img src="/blog/assets/2020-06-01-viable-combinations-jdk14.png" alt="Viable combinations of catchup demand and storage, JDK 14 and G1"></p>
<p>We made the distinction between &quot;yes&quot;, &quot;no&quot; and &quot;gc&quot;, meaning the
pipeline keeps up, doesn't keep up due to lack of throughput, or doesn't
keep up due to frequent long GC pauses. Note that the lack of throughput
can also be caused by concurrent GC activity and frequent short GC
pauses. In the end, the distinction doesn't matter a lot.</p>
<p>You can make out a contour that separates the lower-left area where
things work out from the rest of the space, where they fail. We made
the same kind of chart for other combinations of JDK and GC, extracted
the contours, and came up with this summary chart:</p>
<p><img src="/blog/assets/2020-06-01-viable-combinations.png" alt="Viable combinations of catchup demand and storage, JDK 14 and G1"></p>
<p>For reference, the hardware we used is a MacBook Pro 2018 with a 6-core
Intel Core i7 and 16 GB DDR4 RAM, configuring <code>-Xmx10g</code> for the JVM.
However, we do expect the overall relationship among the combinations to
remain the same on a broad range of hardware parameters. The chart
visualizes the superiority of the G1 over others, the weakness of the G1
on JDK 8, and the weakness of the experimental low-latency collectors
for this kind of workload.</p>
<p>The base latency, the time it takes to emit the window results, was in
the ballpark of 500 milliseconds, but latency would often take hikes due
to occasional Major GC's (which are not unreasonably long with the G1),
up to 10 seconds in the borderline cases (where the pipeline barely
keeps up), and still recover back to a second or two. We also noticed
the effects of JIT compilation in the borderline cases: the pipeline
would start out with a constantly increasing latency, but then after
around two minutes, its performance would improve and the latency would
make a full recovery.</p>
<p>Go to <a href="/blog/2020/06/09/jdk-gc-benchmarks-part2">Part 2: the Batch Pipeline Benchmarks</a>.</p>
<p><em>If you enjoyed reading this post, check out Jet at
<a href="https://github.com/hazelcast/hazelcast-jet">GitHub</a> and give us a
star!</em></p>
</span></div></article></div><div class="post"><header class="postHeader"><h1 class="postHeaderTitle"><a href="/blog/2020/05/25/grcp">Processing 10M queries / second on a single node using Jet and gRPC</a></h1><p class="post-meta">May 25, 2020</p><div class="authorBlock"><p class="post-authorName"><a href="https://www.linkedin.com/in/frantisek-hartman/" target="_blank" rel="noreferrer noopener">František Hartman, Marko Topolnik</a></p><div class="authorPhoto"><a href="https://www.linkedin.com/in/frantisek-hartman/" target="_blank" rel="noreferrer noopener"><img src="https://i.stack.imgur.com/3X7wE.png" alt="František Hartman, Marko Topolnik"/></a></div></div></header><article class="post-content"><div><span><p>Implementing data processing pipelines occasionally requires calling an
external service, for example: predicting/classifying based on a ML
model, looking up records from a database or a full-text search engine,
and using a dedicated platform that computes financial risk exposure.</p>
<p>These are the typical reasons why you'd want to have some processing
done outside of the Hazelcast Jet infrastructure:</p>
<ul>
<li>The service already exists</li>
<li>The service is implemented by another team or in a different language</li>
<li>The deployment of the service needs to be independent of the
deployment of the pipeline (e.g. you need to update ML model without
modifying the pipeline)</li>
<li>Scaling the service independently of the processing pipeline</li>
</ul>
<h2><a class="anchor" aria-hidden="true" id="grpc"></a><a href="#grpc" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>gRPC</h2>
<p><a href="https://grpc.io/">gRPC</a> is an RPC system that trades a bit of
convenience for much better performance, and also includes first-class
support for several critical concerns. Quoting from their website:</p>
<blockquote>
<p>gRPC is a modern open source high performance RPC framework that can run
in any environment. It can efficiently connect services in and across
data centers with pluggable support for load balancing, tracing, health
checking and authentication.</p>
</blockquote>
<p>With gRPC you define the service endpoint and the messages using
Protocol Buffers (Protobuf) and its interface description language
(IDL). The tooling then generates code for the server, serialization and
deserialization of the messages, and the client. Hazelcast Jet's gRPC
module makes it convenient to use the generated code to call the
endpoint from a Jet pipeline.</p>
<p>The gRPC framework provides several RPC types, but most commonly used
are unary RPC and bidirectional streaming RPC. Unary RPC is what's
usually called just &quot;RPC&quot;: the client sends a request and receives a
response. With bidirectional streaming RPC, the client starts a single
request, writes any number of messages to the request stream. Then the
client receives any number of messages in the response stream, which
makes it more similar to a messaging system with 2 topics than RPC.</p>
<h2><a class="anchor" aria-hidden="true" id="using-grpc-from-a-jet-pipeline"></a><a href="#using-grpc-from-a-jet-pipeline" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Using gRPC from a Jet Pipeline</h2>
<p>With Hazelcast Jet 4.1 we have released first-class support for
accessing gRPC endpoints from Jet pipelines. In this post we investigate
the performance of the <a href="https://github.com/grpc/grpc-java">gRPC-Java</a>
framework and the effects of various settings on maximum throughput both
in vanilla gRPC and in a Jet environment.</p>
<p>Let’s start with an example of calling a gRPC endpoint from Jet.  For
example, given this protobuf definition:</p>
<pre><code class="hljs css language-proto"><span class="hljs-class"><span class="hljs-keyword">service</span> <span class="hljs-title">Greeter</span> </span>{
  <span class="hljs-comment">// Sends a greeting rpc SayHello</span>
  (HelloRequest) returns (HelloReply) {}
}
</code></pre>
<p>We can create the following service factory:</p>
<pre><code class="hljs css language-java"><span class="token keyword">var</span> greeterService <span class="token operator">=</span> <span class="token function">unaryService</span><span class="token punctuation">(</span> <span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">-></span>
  <span class="token class-name">ManagedChannelBuilder</span><span class="token punctuation">.</span><span class="token function">forAddress</span><span class="token punctuation">(</span><span class="token string">"localhost"</span><span class="token punctuation">,</span> <span class="token number">5000</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">usePlaintext</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
  channel <span class="token operator">-></span> <span class="token class-name">GreeterGrpc</span><span class="token punctuation">.</span><span class="token function">newStub</span><span class="token punctuation">(</span>channel<span class="token punctuation">)</span><span class="token operator">::</span><span class="token function">sayHello</span>
<span class="token punctuation">)</span><span class="token punctuation">;</span>
</code></pre>
<p>The first lambda returns a client channel builder. Jet uses the builder
you provide to create gRPC channels, which correspond to network
connections. You can set any desired configuration options on the
builder, such as compression and encryption. These will turn out to be
relevant to our investigation.</p>
<p>The second lambda takes a gRPC client channel obtained from the builder
and returns a function which Jet will call for each item of the
pipeline. The class <code>GreeterGrpc</code> is auto-generated by gRPC and its
<code>newStub</code> method creates the client stub. You can change the default
settings before returning it from the lambda. We won’t modify this
further in our investigation.</p>
<p>Once you have constructed a <code>ServiceFactory</code>, you can pass it to a Jet
pipeline stage transform such as <code>mapUsingServiceAsync</code>. Jet uses the
factory to create instances of the service, and passes these to the
lambda you provide. Here's an example:</p>
<pre><code class="hljs css language-java"><span class="token class-name">BatchStage</span><span class="token generics"><span class="token punctuation">&lt;</span><span class="token class-name">Integer</span><span class="token punctuation">></span></span> stage <span class="token operator">=</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>
stage<span class="token punctuation">.</span><span class="token function">mapUsingServiceAsync</span><span class="token punctuation">(</span>greeterServiceFactory<span class="token punctuation">,</span> <span class="token punctuation">(</span>service<span class="token punctuation">,</span> item<span class="token punctuation">)</span> <span class="token operator">-></span>
    service<span class="token punctuation">.</span><span class="token function">call</span><span class="token punctuation">(</span><span class="token class-name">HelloRequest</span><span class="token punctuation">.</span><span class="token function">newBuilder</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">setValue</span><span class="token punctuation">(</span>item<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">build</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre>
<p>The word <code>Async</code> in the stage transform name means that the lambda
function must return a <code>CompletableFuture&lt;T&gt;</code>, and in our case
<code>service.call()</code> returns just that.</p>
<h2><a class="anchor" aria-hidden="true" id="the-environment"></a><a href="#the-environment" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>The environment</h2>
<p>We ran the benchmarks in AWS on
2 instances of type c5.9xlarge. This instance type has:</p>
<ul>
<li>36 vCPUs</li>
<li>76 GiB of RAM</li>
<li>10 Gigabit network.</li>
</ul>
<p>One instance ran a single Hazelcast Jet
member, the other one ran the gRPC server.</p>
<h2><a class="anchor" aria-hidden="true" id="the-grpc-benchmarks"></a><a href="#the-grpc-benchmarks" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>The gRPC Benchmarks</h2>
<p>The gRPC-Java framework provides its own set of
<a href="https://github.com/grpc/grpc-java/tree/master/benchmarks">benchmarks</a>.
We will use these to establish a baseline to compare with the
performance of our Jet pipeline. The benchmark consists of running a
gRPC server and a client.</p>
<p>We issued this command to start the server:</p>
<pre><code class="hljs css language-bash">./grpc-benchmarks/bin/qps_server --address=<span class="hljs-variable">$SERVER</span> --transport=netty_epoll
</code></pre>
<p>And to run the client:</p>
<pre><code class="hljs css language-bash">grpc-benchmarks/bin/qps_client --address=<span class="hljs-variable">$SERVER</span> --transport=netty_epoll \
  --channels=36 --outstanding_rpcs=64 --client_payload=8 --server_payload=8
</code></pre>
<p>The parameters relevant to our investigation are:</p>
<ul>
<li>--transport - possible values are netty_epoll and netty_nio,
netty_epoll should be most performant, but it is only available on
Linux</li>
<li>--channels - number of channels (i.e., network connections) to create
on the client side</li>
<li>--outstanding_rpcs - how many requests per channel to submit without
waiting for a response</li>
<li>--client_payload and --server_payload - size of the payload sent in a
request/response</li>
<li>--streaming_rpcs - when present, uses a bidirectional streaming
endpoint, otherwise a unary endpoint</li>
</ul>
<p>Here's an example of of the output:</p>
<pre><code class="hljs css language-text">Channels:                          36
Outstanding RPCs per Channel:      64
Server Payload Size:                8
Client Payload Size:                8
50%ile Latency (in micros):      1951
90%ile Latency (in micros):      3695
95%ile Latency (in micros):      5183
99%ile Latency (in micros):     10559
99.9%ile Latency (in micros):   15807
Maximum Latency (in micros):    37375
QPS:                           986128
</code></pre>
<p>Our focus for this benchmark was throughput, so we focused on the QPS
metric - Queries Per Second. The results are quite interesting: we could
clearly identify the number of channels as the most important factor.
Using 72-108 channels increased the throughput by 20x-30x, compared to
the baseline of one channel. Since we're using very small messages,
per-message overhead inside the gRPC layer dominates over the network
limits. This is why you can achieve more throughput by adding more
channels.</p>
<p><img src="/blog/assets/2020-05-25-grpc-channels.png" alt="Througput for channels, at 128 outstanding RPCs"></p>
<p>The number of outstanding RPCs seems to have a sweet spot with a value
of 128 or 256 for this particular configuration of instance type,
network and benchmark parameters. With very low message processing time,
the optimal number of outstanding RPCs is mostly dictated by the number
of in-flight messages in the network layer. To a first approximation,
this number equals network throughput in terms of messages per second,
multiplied by the roundtrip latency of one message. Our messages are
very small, so the optimal number of outstanding RPCs is quite high.</p>
<p>As for the transport epoll and NIO transport types - it seems that for
unary calls they both reach similar maximum performance, but overall,
NIO is better.  For streaming calls, epoll wins in maximum performance
with similar results using 108 channels and 128 outstanding RPCs or 72
channels and 256 outstanding RPCs. With other settings there doesn’t
seem to be a clear winner.</p>
<p><img src="/blog/assets/2020-05-25-grpc-epoll_nio.png" alt="netty_epoll vs netty_nio"></p>
<p>All charts present best results for various configurations of number of
channels, outstanding RPC etc. The full data is available in this
<a href="https://docs.google.com/spreadsheets/d/1psjHF5ZRlxYAwxn4LA_XhvYKB0KuLXMHW8iEjrUAteE/edit#gid=63601685">spreadsheet</a>
.</p>
<h2><a class="anchor" aria-hidden="true" id="jet-benchmarks"></a><a href="#jet-benchmarks" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Jet Benchmarks</h2>
<p>We created 2 types of workloads on the gRPC server side:</p>
<ul>
<li>Very fast computation (integer multiplication)</li>
<li>CPU-bound task taking 10 ms</li>
</ul>
<p>We ran each workload as:</p>
<ul>
<li>Unary RPC</li>
<li>Bidirectional streaming RPC</li>
</ul>
<p>In the Jet pipeline we don’t have the exact same parameters as in the
gRPC benchmark, but there are similar ones:</p>
<ul>
<li>Local parallelism of the <code>mapUsingServiceAsync</code> step dictates the
number of mapping processors, each processor has its own channel
instance so this is roughly equivalent to the number of channels.</li>
<li>The parameter <code>maxConcurrentOps</code> specifies how many concurrent
asynchronous mapping operations each Jet processor can issue without
waiting for a response. This gives you control over the same aspect as
the number of outstanding RPCs, but in a less direct way.</li>
</ul>
<p>This is the pipeline for a unary service, all the other benchmark
pipelines follow the same pattern:</p>
<pre><code class="hljs css language-java"><span class="token class-name">Pipeline</span> p <span class="token operator">=</span> <span class="token class-name">Pipeline</span><span class="token punctuation">.</span><span class="token function">create</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token class-name">BatchStage</span><span class="token generics"><span class="token punctuation">&lt;</span><span class="token class-name">Integer</span><span class="token punctuation">></span></span> stage <span class="token operator">=</span> p<span class="token punctuation">.</span><span class="token function">readFrom</span><span class="token punctuation">(</span><span class="token function">intSource</span><span class="token punctuation">(</span>jobBatchSize<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
stage<span class="token punctuation">.</span><span class="token function">mapUsingServiceAsync</span><span class="token punctuation">(</span>unaryService<span class="token punctuation">,</span>
       maxConcurrentOps<span class="token punctuation">,</span> <span class="token boolean">true</span><span class="token punctuation">,</span>
       <span class="token punctuation">(</span>service<span class="token punctuation">,</span> item<span class="token punctuation">)</span> <span class="token operator">-></span> service<span class="token punctuation">.</span><span class="token function">call</span><span class="token punctuation">(</span><span class="token class-name">HelloRequest</span><span class="token punctuation">.</span><span class="token function">newBuilder</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">setValue</span><span class="token punctuation">(</span>item<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">build</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token punctuation">.</span><span class="token function">setLocalParallelism</span><span class="token punctuation">(</span>localParallelism<span class="token punctuation">)</span>
    <span class="token punctuation">.</span><span class="token function">aggregate</span><span class="token punctuation">(</span><span class="token class-name">AggregateOperations</span><span class="token punctuation">.</span><span class="token function">counting</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token punctuation">.</span><span class="token function">writeTo</span><span class="token punctuation">(</span><span class="token class-name">Sinks</span><span class="token punctuation">.</span><span class="token function">observable</span><span class="token punctuation">(</span>runId<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
</code></pre>
<h2><a class="anchor" aria-hidden="true" id="fast-computation-benchmark"></a><a href="#fast-computation-benchmark" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Fast computation benchmark</h2>
<p>This benchmark is very similar to the benchmark from gRPC repo. It is
doing minimal work on the gRPC server side and measures overhead of the
Jet pipeline and network.</p>
<p>First let’s compare this with the gRPC benchmark. We achieve similar
(slightly lower) results for unary RPC, and about half for bidirectional
streaming. It’s good that we don’t get e.g. 10x less, but the drop for
bidirectional streaming is rather surprising.</p>
<p>Best throughput results achieved in gRPC benchmark and Jet benchmark:</p>
<p><img src="/blog/assets/2020-05-18-grpc-vs-jet.png" alt="gRPC benchmark and Jet pipeline"></p>
<p>Similar to the gRPC benchmark, we can observe that to achieve maximum
throughput we need to increase the number of channels and the
maxConcurrentOps parameter.</p>
<p><img src="/blog/assets/2020-05-25-fast_unary_bidi.png" alt="unary vs bidirectional"></p>
<h2><a class="anchor" aria-hidden="true" id="batch-endpoint"></a><a href="#batch-endpoint" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Batch endpoint</h2>
<p>We also tried each RPC with a modified message type containing a batch
of messages in combination with <code>mapUsingServiceAsynchBatched</code>. This is
not always possible to do - you need to be able to change the interface
of the server, but if you can, it provides a huge boost to the
throughput.</p>
<p>Jet uses what has been variously called natural or <a href="https://mechanical-sympathy.blogspot.com/2011/10/smart-batching.html">smart
batching</a>.
Typically, a batching algorithm creates batches of fixed size or waits
for a fixed time to create a batch. This increases latency in
low-throughput scenarios. Smart batching instead creates a batch from
whatever items came in while the previous batch was being processed.
When the traffic is low, this results in small batches (including single
items), preserving the best possible latency, and as the traffic grows,
larger batches automatically form, up to the limit set by the
maxBatchSize parameter.</p>
<p>After modifying the endpoint to work with batches, our results improved
dramatically:</p>
<ol>
<li>Smart batching increased the throughput by roughly 7x.</li>
<li>We needed less channels (i.e., less system resources) to reach the
maximum throughput.</li>
<li>Even though bidirectional streaming remained a winner, its advantage
over unary became very slim.</li>
</ol>
<p>Note that for this benchmark there is no maxConcurrentOps setting,
because <code>mapUsingServiceAsyncBatched</code> doesn't have it as a configurable
option.</p>
<p><img src="/blog/assets/2020-05-25-fast_unary_bidirectional_batching.png" alt="unary vs bidirectional with batching"></p>
<h2><a class="anchor" aria-hidden="true" id="cpu-bound-task-of-10-ms"></a><a href="#cpu-bound-task-of-10-ms" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>CPU bound task of 10 ms</h2>
<p>This benchmark simulates a task that takes 10 ms to complete and is
CPU-bound.  Because our testing machine has 36 vCPUs, the theoretical
maximum throughput is 3600 requests/s.</p>
<p>Interestingly, we can see that unary RPC reaches the maximum possible
throughput for all settings, however bidirectional only for 36 channels.
The results don’t change significantly for different maxConcurrentOps
settings.</p>
<p><img src="/blog/assets/2020-05-25-10ms_unary_bidirectional.png" alt="10ms task unary vs bidirectional"></p>
<p>In this scenario, smart batching no longer has a significant edge. Unary
only gets close to a maximum with 72 channels. Bidirectional achieves
maximum throughput with both 36 and 72 channels.</p>
<p><img src="/blog/assets/2020-05-25-10ms_unary_bidirectional_batching.png" alt="10ms task unary vs bidirectional"></p>
<h2><a class="anchor" aria-hidden="true" id="conclusion"></a><a href="#conclusion" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Conclusion</h2>
<p>We can draw several conclusions from our investigation:</p>
<ul>
<li>There is no silver bullet, the results vary significantly with
workload type, network speed and latency, resources available (e.g. on
a local development machine the benchmark yields different results).</li>
<li>Bidirectional streaming is faster than unary, roughly 2x in our case.</li>
<li>For both unary and bidirectional streaming endpoints increasing the
number of channels provides better throughput, number of CPUs on the
gRPC server side might be a good starting point.</li>
<li>Batched endpoints make a big difference, allowing much higher
throughput with less channels.</li>
<li>The execution duration of the task also makes a significant
difference, especially when taking longer than the network roundtrip
(our 10 ms CPU bound task).</li>
</ul>
<p>So in any case you should test in your environment and your own workload
to find the most advantageous setup.</p>
<h2><a class="anchor" aria-hidden="true" id="links"></a><a href="#links" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Links</h2>
<p>For a full guide on how to use Jet's gRPC functionality and API details,
please see the <a href="/docs/how-tos/grpc">manual</a>.</p>
<p>The benchmark code is published on
<a href="https://github.com/frant-hartm/hazelcast-jet-grpc-benchmark">github.com</a></p>
<p>The spreadsheet with all the results can be seen
<a href="https://docs.google.com/spreadsheets/d/1psjHF5ZRlxYAwxn4LA_XhvYKB0KuLXMHW8iEjrUAteE/edit#gid">here</a>.</p>
</span></div></article></div><div class="post"><header class="postHeader"><h1 class="postHeaderTitle"><a href="/blog/2020/05/18/spark-jet">How Hazelcast Jet Compares to Apache Spark</a></h1><p class="post-meta">May 18, 2020</p><div class="authorBlock"><p class="post-authorName"><a href="https://twitter.com/voloda" target="_blank" rel="noreferrer noopener">Vladimir Schreiner</a></p><div class="authorPhoto"><a href="https://twitter.com/voloda" target="_blank" rel="noreferrer noopener"><img src="https://3l0wd94f0qdd10om8642z9se-wpengine.netdna-ssl.com/wp-content/uploads/2018/10/speaker-vladimir-schreiner-e1551380845855-170x170.jpg" alt="Vladimir Schreiner"/></a></div></div></header><article class="post-content"><div><span><p>“How Jet compares to Spark” and “why should I choose Jet over Spark” are
arguably the most frequent questions I’ve been asked during the talks
and workshops. While it is hard to assess the product fit without
focusing on a concrete use-case, I’d still like to compare concepts and
architecture used under the hood of both frameworks.</p>
<p>Versions considered: <a href="https://jet-start.sh/download">Hazelcast Jet 4.1</a>
and <a href="https://spark.apache.org/downloads.html">Apache Spark 2.4.5</a>.</p>
<h2><a class="anchor" aria-hidden="true" id="computations-modeled-as-graphs"></a><a href="#computations-modeled-as-graphs" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Computations Modeled as Graphs</h2>
<p>Apache Spark and Hazelcast Jet (referred to as “frameworks”) are both
tools for clustered computing. They are applicable mostly for analytical
(OLAP) applications, including those that apply a series of processing
steps to many uniform data records (such as lines in a file, rows in a
table or records appended to a stream), as one example.</p>
<p>Both frameworks build on the principles of dataflow programming: a user
builds an application by chaining high-level coarse-grained operators
such as map, join or aggregate. The operators form a network that can be
modeled as a graph (<a href="https://en.wikipedia.org/wiki/Directed_acyclic_graph">directed acyclic
graphs</a> or DAG to
be specific) where nodes represent steps in the computation and edges
represent data exchange.</p>
<p>The dataflow model has some important properties that both frameworks
use for scaling and fault-tolerance:</p>
<ul>
<li>Pipeline Parallelism: operators can work independently, in parallel.</li>
<li>Data Parallelism: a single operator can run in multiple instances,
each instance processing a particular data partition</li>
<li>No Shared State: each operator instance manages its state exclusively.
There is no shared state to coordinate access to or to replicate.
Moreover, the state is only determined by the input data. As a result,
the operator can be recovered by replaying the input data.</li>
</ul>
<p>Spark and Jet differ in how they use and execute the DAG as explained in
the next section but fundamentally: no matter which API you use (RDDs,
Spark SQL or a Pipeline API of Jet), <strong>the physical execution plan is a
DAG representing the dataflow</strong>.</p>
<h2><a class="anchor" aria-hidden="true" id="staged-x-continuous-execution-mode"></a><a href="#staged-x-continuous-execution-mode" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Staged x Continuous Execution Mode</h2>
<p>In Spark, the DAG nodes represent execution stages. A stage must be
fully completed before Spark starts the next one. In Jet, DAG represents
connected operators. Jet executes all DAG nodes concurrently.</p>
<p>Let’s use a textbook OLAP example to elaborate: the log analysis (a
real-world application of notorious word count). Data from the access
logs are aggregated over different grouping keys, such as counting the
web sessions over several web applications using shared session id.</p>
<p>This is the Spark and Jet code to load the data, pre-process (parse) and
aggregate it:</p>
<p>Spark RDD API (Java)</p>
<pre><code class="hljs css language-java">sc<span class="token punctuation">.</span><span class="token function">textFile</span><span class="token punctuation">(</span><span class="token string">"/path/to/input/"</span><span class="token punctuation">)</span>
    <span class="token punctuation">.</span><span class="token function">flatMap</span><span class="token punctuation">(</span><span class="token class-name">LineIterator</span><span class="token operator">::</span><span class="token keyword">new</span><span class="token punctuation">)</span>
    <span class="token punctuation">.</span><span class="token function">mapToPair</span><span class="token punctuation">(</span>s <span class="token operator">-></span> <span class="token keyword">new</span> <span class="token class-name">Tuple2</span><span class="token generics"><span class="token punctuation">&lt;</span><span class="token punctuation">></span></span><span class="token punctuation">(</span>s<span class="token punctuation">,</span> <span class="token number">1L</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token punctuation">.</span><span class="token function">reduceByKey</span><span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token class-name">Function2</span><span class="token generics"><span class="token punctuation">&lt;</span><span class="token class-name">Long</span><span class="token punctuation">,</span> <span class="token class-name">Long</span><span class="token punctuation">,</span> <span class="token class-name">Long</span><span class="token punctuation">></span></span><span class="token punctuation">)</span> <span class="token punctuation">(</span>a<span class="token punctuation">,</span> b<span class="token punctuation">)</span> <span class="token operator">-></span> a <span class="token operator">+</span> b<span class="token punctuation">)</span>
    <span class="token punctuation">.</span><span class="token function">saveAsTextFile</span><span class="token punctuation">(</span><span class="token string">"/path/to/output/"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
</code></pre>
<p>Jet Pipeline API (Java)</p>
<pre><code class="hljs css language-java">p<span class="token punctuation">.</span><span class="token function">readFrom</span><span class="token punctuation">(</span><span class="token class-name">Sources</span><span class="token punctuation">.</span><span class="token function">files</span><span class="token punctuation">(</span><span class="token string">"/path/to/input/"</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token punctuation">.</span><span class="token function">map</span><span class="token punctuation">(</span><span class="token class-name">LogLine</span><span class="token operator">::</span><span class="token function">parse</span><span class="token punctuation">)</span>
    <span class="token punctuation">.</span><span class="token function">groupingKey</span><span class="token punctuation">(</span><span class="token function">wholeItem</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token punctuation">.</span><span class="token function">aggregate</span><span class="token punctuation">(</span><span class="token function">counting</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token punctuation">.</span><span class="token function">writeTo</span><span class="token punctuation">(</span><span class="token class-name">Sinks</span><span class="token punctuation">.</span><span class="token function">files</span><span class="token punctuation">(</span><span class="token string">"/path/to/output/"</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
</code></pre>
<h3><a class="anchor" aria-hidden="true" id="spark-and-staged-execution"></a><a href="#spark-and-staged-execution" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Spark and Staged Execution</h3>
<p>Spark splits the computation to non-overlapping stages. A reading stage
and a group-and-aggregate stage, in our case. During the reading stage,
Spark workers fetch data from disk files, parse it and cache it in the
cluster memory. Spark schedules more tasks if the source can be read in
parallel (e.g. data is partitioned). All reading stage tasks must be
finished before the first aggregating task is started.</p>
<p>This is the DAG representing execution stages (
<a href="https://www.tutorialkart.com/apache-spark/dag-and-physical-execution-plan/">source</a>).</p>
<p><img src="/blog/assets/2020-05-18-spark-dag-stages.svg" alt="Spark Staged Execution"></p>
<p>Staged execution was designed to support an iterative analytics use-case
where the results of one stage stay cached in a cluster memory to be
reused by a following step in the analysis. This makes Spark a popular
choice for ML research where a data scientist gradually evolves the
dataset with new experiments, evicting the data when their Spark session
is over. It is also a powerful debugging tool.</p>
<p>On the other hand, staged execution doesn’t perform well for
latency-sensitive use-cases, namely stream processing.</p>
<p>Streaming data is continuously incrementing. Staged execution is however
designed for finite datasets. Whole input must be read before Spark
starts subsequent steps. Spark Streaming works around this by batching
the input data, e.g. creating finite chunks from an infinite stream.
Buffering adds to the job latency as the data are waiting for the batch
to fill, staying idle.</p>
<p>The stages are planned and scheduled again and again for every batch.
The overhead of the planning process increases the latency further.</p>
<p>Another latency penalty comes if the data partitions are not balanced
evenly. If a single partition of data takes longer to read or process,
it would block the whole job from progressing since the next stage can’t
be started. Jet would be impacted by this scenario, too, but it can
still provide early results – in-complete, indicative results based on
already processed partitions.</p>
<h3><a class="anchor" aria-hidden="true" id="jet-and-continuous-execution"></a><a href="#jet-and-continuous-execution" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Jet and Continuous Execution</h3>
<p>Jet executes all DAG nodes concurrently. The DAG is deployed to all
cluster nodes when the job is submitted and runs until a termination.
The instances of running DAG nodes, called Processors, then run in
parallel and continuously exchange data. For partitioned data sets, the
data partitions are evenly distributed among available processors (see
the
<a href="https://jet-start.sh/docs/architecture/distributed-computing">docs</a>).</p>
<p>This is the DAG representing the execution plan for the log aggregation.
Jet would create multiple instances of each and route data among it
following the routing strategy (<a href="https://jet-start.sh/docs/next/architecture/distributed-computing">source</a>):</p>
<p><img src="/blog/assets/2020-05-18-jet-dag.svg" alt="Jet Execution DAG"></p>
<p>A reading Processor keeps fetching data from the data source and sends
it to a downstream channel immediately. The channel routes data to the
respective aggregating processor, following the grouping key. The
aggregating processor is observing the input channel and updates the
aggregate with each input item. It’s an application concern to specify
when the aggregator emits the aggregate downstream – with every input
item, after a period of time, after the whole dataset has been processed
or based on a data-driven trigger.</p>
<p>The continuous execution model is a natural fit for streaming use-cases
that stress low latency. Jet Jobs can keep millisecond latencies on a
large scale.</p>
<p>Spark has introduced the continuous execution mode in 2.4. The mode is
still experimental and is limited to stateless operators (mapping,
filtering) so it wasn’t considered for this comparison.</p>
<h2><a class="anchor" aria-hidden="true" id="in-memory-execution"></a><a href="#in-memory-execution" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>In-Memory Execution</h2>
<p>Spark and Jet both rely on an in-memory execution. That means that <em>data
transfer</em> and <em>execution state</em> both use the cluster RAM.</p>
<h3><a class="anchor" aria-hidden="true" id="data-transfer"></a><a href="#data-transfer" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Data Transfer</h3>
<p>In-memory data transfer means that the data between the consecutive DAG
nodes are exchanged using shared memory instead of a disk (shuffling the
data among cluster nodes still requires a network, of course).</p>
<p>Spark exchanges data between stages by saving the complete output of an
upstream stage in the memory of the worker to be used as an input of a
downstream stage. Jet uses in-memory queues to connect upstream and
downstream Processors.</p>
<p>That makes Spark more memory demanding as it caches the whole dataset
exchanged between two steps which can easily be hundreds of GB of data.
Spark workers are therefore able to spill data to disk not to run out of
memory. Jet processors run all in parallel and exchange data
continuously. The in-flight data are no more than a few thousand
records.</p>
<h3><a class="anchor" aria-hidden="true" id="execution-state"></a><a href="#execution-state" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Execution State</h3>
<p>Execution state refers to the temporary data of the computation, such as
the value of an ongoing aggregation or join. Both Jet and Spark keep the
state data on heap by default.</p>
<p>Spark can however also place execution state off-heap and even spill it
to disk. It can, therefore, perform calculations that require a large
state such as joins or sorts on huge datasets. For Jet, the execution
state must fit to cluster memory.</p>
<h2><a class="anchor" aria-hidden="true" id="dedicated-x-shared-resources"></a><a href="#dedicated-x-shared-resources" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Dedicated x Shared Resources</h2>
<p>Spark applications running in a cluster are isolated from each other.
Jet shares the cluster resources between applications (called Jobs). No
approach is “the right one”. It’s trading-off isolation and performance.</p>
<h3><a class="anchor" aria-hidden="true" id="spark-assigns-dedicated-resources"></a><a href="#spark-assigns-dedicated-resources" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Spark Assigns Dedicated Resources</h3>
<p>For each application, Spark runs dedicated processes for both scheduling
and execution.</p>
<p>The processes are created with the resources (CPU, memory and disk)
allocated to the application upon startup and reserved during job
lifetime. After the Spark Application ends, the processes are terminated
and the resources are freed.</p>
<p>This design clearly favours isolation. A noisy application doesn’t
affect the neighbours using the same computer. It can, however, lead to
overprovisioning as an Application holds allocated resources even if it
doesn’t require it.</p>
<p>Spark was designed in the age of Hadoop – huge clusters of heterogeneous
machines running many workloads. Multi-tenancy was, therefore, a
first-level design concern.</p>
<h3><a class="anchor" aria-hidden="true" id="jet-shares-resources"></a><a href="#jet-shares-resources" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Jet Shares Resources</h3>
<p>The Jet cluster is also formed by multiple member processes. Those
processes are started when the cluster starts and aren’t coupled with a
lifecycle of individual hosted Job.</p>
<p>Jobs share the cluster resources and run in a <a href="https://jet-start.sh/docs/architecture/execution-engine">cooperative
mode</a>. Each job
does a small amount of work and yields to the next one. Job is removed
from this round-robin after it finishes.</p>
<p>This design leads to efficient resource utilization. All jobs get a fair
amount of CPU time. If a job gets idle (e.g. waiting for more input
data), it simply backs off and Jet excludes it from the round-robin
rotation for a few milliseconds, giving busy Jobs more CPU to keep up.</p>
<p>Resource sharing is of course prone to noisy neighbours – a greedy job
can starve others. To prevent this, Jet recommends starting a cluster
per tenant or even per job, increasing the isolation to a Spark level.</p>
<h3><a class="anchor" aria-hidden="true" id="shared-datasets"></a><a href="#shared-datasets" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Shared Datasets</h3>
<p>Another benefit of job sharing processes is exchanging data over the
shared memory.</p>
<p>A job can load and pre-process data, caching it in cluster memory (Jet
comes with distributed storage). The cached collection then becomes a
source for further processing jobs, leading to <a href="https://hazelcast.com/resources/jet-0-4-vs-spark-flink-batch-benchmark/">significant performance
gains</a>
from reading the local memory instead of a remote data source. Another
use-case is shared reference data (such as lookup tables or parameters)
or queues connecting the output stream of one job to an input of another
one.</p>
<p>Spark applications run in isolated processes so they must use external
storage to exchange data.</p>
<h2><a class="anchor" aria-hidden="true" id="conclusion"></a><a href="#conclusion" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Conclusion</h2>
<p>Jet and Spark are frameworks that use principles of dataflow programming
to run analytical computations on clusters of machines for scalability
and resiliency.</p>
<p>They differ in how they implement and execute the data flow. Jet’s
design favours streaming use-cases that benefit from the low-latency
continuous execution. Spark can spill data to disk and isolates jobs on
a process level. Therefore it’s a good fit for large, multi-tenant
clusters.</p>
<p>Other areas worth comparing are the cluster architecture and APIs. They
will be covered in the next part of the article.</p>
</span></div></article></div><div class="post"><header class="postHeader"><h1 class="postHeaderTitle"><a href="/blog/2020/04/29/jet-41-is-released">Jet 4.1 is Released</a></h1><p class="post-meta">April 29, 2020</p><div class="authorBlock"><p class="post-authorName"><a href="http://twitter.com/cgencer" target="_blank" rel="noreferrer noopener">Can Gencer</a></p><div class="authorPhoto"><a href="http://twitter.com/cgencer" target="_blank" rel="noreferrer noopener"><img src="https://pbs.twimg.com/profile_images/1187734846749196288/elqWdrPj_400x400.jpg" alt="Can Gencer"/></a></div></div></header><article class="post-content"><div><span><p>We are happy to present the new release of Hazelcast Jet 4.1. Here's a
quick overview of the new features.</p>
<h2><a class="anchor" aria-hidden="true" id="extended-grpc-support"></a><a href="#extended-grpc-support" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Extended gRPC Support</h2>
<p>We've applied the lessons learned from the Jet-Python integration and
made it easier to integrate a Jet pipeline with <a href="https://grpc.io">gRPC</a>
services. The utility class <code>GrpcServices</code> introduces two new
<code>ServiceFactory</code>s you can use with the <code>mapUsingServiceAsync</code> transform.
Using this feature can be a significant performance boost vs. using the
sync <code>mapUsingService</code> call.</p>
<p>Here's a quick example on how you can use the gRPC service factory:</p>
<pre><code class="hljs css language-java"><span class="token keyword">var</span> greeterService <span class="token operator">=</span> <span class="token function">unaryService</span><span class="token punctuation">(</span>
    <span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">-></span> <span class="token class-name">ManagedChannelBuilder</span><span class="token punctuation">.</span><span class="token function">forAddress</span><span class="token punctuation">(</span><span class="token string">"localhost"</span><span class="token punctuation">,</span> <span class="token number">5000</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">usePlaintext</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    channel <span class="token operator">-></span> <span class="token class-name">GreeterGrpc</span><span class="token punctuation">.</span><span class="token function">newStub</span><span class="token punctuation">(</span>channel<span class="token punctuation">)</span><span class="token operator">::</span><span class="token function">sayHello</span>
<span class="token punctuation">)</span><span class="token punctuation">;</span>

<span class="token class-name">Pipeline</span> p <span class="token operator">=</span> <span class="token class-name">Pipeline</span><span class="token punctuation">.</span><span class="token function">create</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
p<span class="token punctuation">.</span><span class="token function">readFrom</span><span class="token punctuation">(</span><span class="token class-name">TestSources</span><span class="token punctuation">.</span><span class="token function">items</span><span class="token punctuation">(</span><span class="token string">"one"</span><span class="token punctuation">,</span> <span class="token string">"two"</span><span class="token punctuation">,</span> <span class="token string">"three"</span><span class="token punctuation">,</span> <span class="token string">"four"</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
 <span class="token punctuation">.</span><span class="token function">mapUsingServiceAsync</span><span class="token punctuation">(</span>greeterService<span class="token punctuation">,</span> <span class="token punctuation">(</span>service<span class="token punctuation">,</span> input<span class="token punctuation">)</span> <span class="token operator">-></span> <span class="token punctuation">{</span>
    <span class="token class-name">HelloRequest</span> request <span class="token operator">=</span> <span class="token class-name">HelloRequest</span><span class="token punctuation">.</span><span class="token function">newBuilder</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">setName</span><span class="token punctuation">(</span>input<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">build</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token keyword">return</span> service<span class="token punctuation">.</span><span class="token function">call</span><span class="token punctuation">(</span>request<span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token punctuation">}</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">writeTo</span><span class="token punctuation">(</span><span class="token class-name">Sinks</span><span class="token punctuation">.</span><span class="token function">logger</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
</code></pre>
<p>In addition to the unary gRPC service, we support bidirectional
streaming as well as request batching. For a more in-depth look, see the
<a href="/docs/how-tos/grpc">Call a gRPC Service how-to guide</a> and the <a href="/docs/design-docs/007-grpc-support">design
document</a>.</p>
<h2><a class="anchor" aria-hidden="true" id="transactional-jdbc-and-jms-sinks"></a><a href="#transactional-jdbc-and-jms-sinks" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Transactional JDBC and JMS sinks</h2>
<p>In Jet 4.0 we added support for <a href="/blog/2020/02/20/transactional-processors">transactional sources and
sinks</a> through the use of
two-phase commit. We're now extending this support for two additional
sinks: JDBC and JMS. The support requires the broker or the database to
support XA transactions. To test your database's support for XA
transactions, we've also released a <a href="/docs/how-tos/xa">how-to guide</a>.</p>
<p>You can also see a full summary of sinks and sources and the variety of
transaction support on the <a href="/docs/api/sources-sinks#summary">sources and sinks
page</a>.</p>
<h2><a class="anchor" aria-hidden="true" id="code-deployment-improvements"></a><a href="#code-deployment-improvements" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Code Deployment Improvements</h2>
<p>When you're deploying a Jet job programmatically (not using the <code>jet submit</code> command-line tool), you must add every class the job needs to
the job's configuration. So far, Jet has supported adding classes one by
one with <code>JobConfig.addClass()</code> and that wouldn't add any of the class's
nested classes. This was especially problematic for anonymous classes,
which you can't even refer to from Java code. In 4.1 we improved
<code>addClass()</code> so that it adds all the nested classes and we added
<code>JobConfig.addPackage()</code> so you can add the whole package in a
one-liner, and don't have to manually maintain the list of classes as
you develop your pipeline. Take a look at the <a href="/docs/design-docs/001-code-deployment-improvements">design
document</a> for more
details.</p>
<h2><a class="anchor" aria-hidden="true" id="job-scoped-serializer-support"></a><a href="#job-scoped-serializer-support" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Job-Scoped Serializer Support</h2>
<p>So far Jet has had a pain point in terms of serialization. The objects
that travel through the pipeline must sometimes be sent from one cluster
member to the other, so they must be serialized. You can let the object
implement <code>Serializable</code>, but that's inefficient. If you wanted to use
a better serialization scheme, you had to register a serializer object
with the Jet cluster and restart the whole cluster.</p>
<p>It is now possible to attach a serializer directly to the job you're
submitting.</p>
<pre><code class="hljs css language-java"><span class="token class-name">JobConfig</span> config <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">JobConfig</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
  <span class="token punctuation">.</span><span class="token function">registerSerializer</span><span class="token punctuation">(</span><span class="token class-name">Person</span><span class="token punctuation">.</span><span class="token keyword">class</span><span class="token punctuation">,</span> <span class="token class-name">PersonSerializer</span><span class="token punctuation">.</span><span class="token keyword">class</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

jet<span class="token punctuation">.</span><span class="token function">newJob</span><span class="token punctuation">(</span>pipeline<span class="token punctuation">,</span> config<span class="token punctuation">)</span><span class="token punctuation">;</span>
</code></pre>
<p>Jet will use these serializers only inside the job. You can read more
about how serialization in Hazelcast Jet works in the <a href="/docs/api/serialization">serialization
guide</a>.</p>
<h2><a class="anchor" aria-hidden="true" id="protocol-buffers-support"></a><a href="#protocol-buffers-support" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Protocol Buffers Support</h2>
<p>Having added the job-level serializers, we also added an extra layer of
convenience to use <a href="https://developers.google.com/protocol-buffers">Google Protocol
Buffers</a> for
serialization. You just need to write a simple class that delegates the
work to the Protobuf compiler-generated serializer class (<code>Person</code> in
this case):</p>
<pre><code class="hljs css language-java"><span class="token keyword">class</span> <span class="token class-name">PersonSerializer</span> <span class="token keyword">extends</span> <span class="token class-name">ProtobufSerializer</span><span class="token generics"><span class="token punctuation">&lt;</span><span class="token class-name">Person</span><span class="token punctuation">></span></span> <span class="token punctuation">{</span>

    <span class="token keyword">private</span> <span class="token keyword">static</span> <span class="token keyword">final</span> <span class="token keyword">int</span> <span class="token constant">TYPE_ID</span> <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">;</span>

    <span class="token class-name">PersonSerializer</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>
        <span class="token keyword">super</span><span class="token punctuation">(</span><span class="token class-name">Person</span><span class="token punctuation">.</span><span class="token keyword">class</span><span class="token punctuation">,</span> <span class="token constant">TYPE_ID</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token punctuation">}</span>
<span class="token punctuation">}</span>
</code></pre>
<p>For more information, see the <a href="/docs/api/serialization#google-protocol-buffers">serialization guide</a>.</p>
<h2><a class="anchor" aria-hidden="true" id="spring-boot-starter"></a><a href="#spring-boot-starter" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Spring Boot Starter</h2>
<p>Spring Boot is a framework that helps you create standalone Spring-based
applications that just run. Spring Boot provides auto-configuration of
some of the commonly used libraries through spring-boot-starters.
Hazelcast Jet now provides its own <a href="https://github.com/hazelcast/hazelcast-jet-contrib/tree/master/hazelcast-jet-spring-boot-starter">Spring Boot
Starter</a>
which can be used to auto-configure and start a Hazelcast Jet instance.</p>
<p>Just by adding the starter dependency to your Spring Boot application,
you can start a <code>JetInstance</code> with the default configuration. If you
want to customize the configuration, just add a configuration file
(<code>hazelcast-jet.yaml</code>) to your classpath or working directory. The
starter will pick it up and configure your Hazelcast Jet instance.  If
you want a client instance, add the client configuration file
(<code>hazelcast-client.yaml</code>).</p>
<p>For more details, see the <a href="/docs/design-docs/004-spring-boot-starter">design
document</a>.</p>
<h2><a class="anchor" aria-hidden="true" id="kubernetes-operator-and-openshift-support"></a><a href="#kubernetes-operator-and-openshift-support" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Kubernetes Operator and OpenShift Support</h2>
<p>With version 4.1 we are introducing our <a href="https://operatorhub.io/?keyword=jet">Hazelcast Jet Kubernetes
Operator</a>. It's available for both
Hazelcast Jet open-source and Enterprise editions. Hazelcast Jet
Enterprise Operator is also a certified by Red Hat and available on the
Red Hat Marketplace.</p>
<h2><a class="anchor" aria-hidden="true" id="discovery-support-for-microsoft-azure"></a><a href="#discovery-support-for-microsoft-azure" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Discovery Support for Microsoft Azure</h2>
<p>We have extended the list of cloud environments where Hazelcast Jet
instances are able to automatically discover each other and form a
cluster. Self-discovery now works in the Microsoft Azure environment.
Here's a quick example on how to enable it:</p>
<pre><code class="hljs css language-yaml"><span class="hljs-attr">hazelcast:</span>
  <span class="hljs-attr">network:</span>
    <span class="hljs-attr">join:</span>
      <span class="hljs-attr">multicast:</span>
        <span class="hljs-attr">enabled:</span> <span class="hljs-literal">false</span>
      <span class="hljs-attr">azure:</span>
        <span class="hljs-attr">enabled:</span> <span class="hljs-literal">true</span>
        <span class="hljs-attr">tag:</span> <span class="hljs-string">TAG-NAME=HZLCAST001</span>
        <span class="hljs-attr">hz-port:</span> <span class="hljs-number">5701</span><span class="hljs-number">-5703</span>
</code></pre>
<p>For more details, please see the <a href="/docs/operations/discovery#azure-cloud">discovery
guide</a>.</p>
<h2><a class="anchor" aria-hidden="true" id="full-release-notes"></a><a href="#full-release-notes" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Full Release Notes</h2>
<p>Members of the open source community that appear in these release notes:</p>
<ul>
<li>@TomaszGaweda</li>
<li>@caioguedes</li>
<li>@SapnaDerajeRadhakrishna</li>
</ul>
<p>Thank you for your valuable contributions!</p>
<h3><a class="anchor" aria-hidden="true" id="new-features"></a><a href="#new-features" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>New Features</h3>
<ul>
<li>[jms] Exactly-once guarantee for JMS sink (#1813)</li>
<li>[jdbc] Exactly-once guarantee for JDBC sink (#1813)</li>
<li>[core] JobConfig.addClass() automatically adds nested classes to the
job (#1932)</li>
<li>[core] JobConfig.addPackage() adds a whole Java package to the job
(#1932, #2077)</li>
<li>[core] Job-scoped serializer deployment (#2020, #2038, #2039, #2043,
#2071, #2075, #2082, #2190)</li>
<li>[core] [006] Protobuf serializer support (#2100)</li>
<li>[pipeline-api] [007] Support gRPC for mapUsingService (#2095, #2185)</li>
</ul>
<h3><a class="anchor" aria-hidden="true" id="enhancements"></a><a href="#enhancements" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Enhancements</h3>
<ul>
<li>[jet-cli] Use log4j2 instead of log4j (#1981)</li>
<li>[jet-cli] Simplify default log output (#2047)</li>
<li>[core] Add useful error message when serializer not registered (#2061)</li>
<li>[jet-cli] Add hazelcast-azure cluster self-discovery plugin to the
fat JAR in the distribution archive (#2079)</li>
<li>[pipeline-api] First-class support for inner hash join (@TomaszGaweda
#2089)</li>
<li>[core] When Jet starts up, it now logs the cluster name (@caioguedes
#2105)</li>
<li>[core] Add useful error message when trying to deploy a JDK class with
JobConfig (#2108)</li>
<li>[core] Implement JobConfig.toString (@SapnaDerajeRadhakrishna #2152)</li>
<li>[core] Do not destroy Observable on shutdown (#2170)</li>
</ul>
<h3><a class="anchor" aria-hidden="true" id="fixes"></a><a href="#fixes" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Fixes</h3>
<ul>
<li>[core] Don't send the interrupt signal to blocking threads when a job
is terminating (#1971)</li>
<li>[core] Consistently prefer YAML over XML config files when both
present (#2033)</li>
</ul>
<h3><a class="anchor" aria-hidden="true" id="breaking-changes"></a><a href="#breaking-changes" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Breaking Changes</h3>
<ul>
<li>[avro] Replace Supplier<Schema> with just Schema for Avro Sink (#2005)</li>
<li>[jms] Reorder parameters in JMS source so the lambda comes last
(#2062)</li>
<li>[jet-cli] Change smart routing (connecting to all cluster members)
default to disabled (#2104)</li>
<li>[pipeline-api] For xUsingServiceAsync transforms, reduce the default
number of concurrent service calls per processor. Before: 256; now: 4.
(#2204)</li>
</ul>
</span></div></article></div><div class="post"><header class="postHeader"><h1 class="postHeaderTitle"><a href="/blog/2020/04/01/upgrading-to-jet-40">Upgrading to Jet 4.0</a></h1><p class="post-meta">April 1, 2020</p><div class="authorBlock"><p class="post-authorName"><a href="https://www.linkedin.com/in/bjozsef/" target="_blank" rel="noreferrer noopener">Bartók József</a></p><div class="authorPhoto"><a href="https://www.linkedin.com/in/bjozsef/" target="_blank" rel="noreferrer noopener"><img src="https://www.itdays.ro/public/images/speakers-big/Jozsef_Bartok.jpg" alt="Bartók József"/></a></div></div></header><article class="post-content"><div><span><p>As we have announce earlier <a href="/blog/2020/03/02/jet-40-is-released">Jet 4.0 is out</a>!
In this blog post we aim to give you the lower level details needed for
migrating from older versions.</p>
<p>Jet 4.0 is a major version release. According to the semantic versioning
we apply, this means that in version 4.0 some of the API has changed in
a breaking way and code written for 3.x may no longer compile against
it.</p>
<h2><a class="anchor" aria-hidden="true" id="jet-on-imdg-40"></a><a href="#jet-on-imdg-40" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Jet on IMDG 4.0</h2>
<p>Jet 4.0 uses IMDG 4.0, which is also a major release with its own
breaking changes. For details see <a href="https://docs.hazelcast.org/docs/rn/index.html#4-0">IMDG Release Notes</a>
and <a href="https://docs.hazelcast.org/docs/4.0/manual/html-single/#migration-guides">IMDG Migration Guides</a>.</p>
<p>The most important changes we made and which have affected Jet too are
as follows:</p>
<ul>
<li><p>We renamed many packages and moved classes around. For details see the
<a href="https://docs.hazelcast.org/docs/rn/index.html#4-0">IMDG Release Notes</a>.
The most obvious change is that many classes that used to be in the
general <code>com.hazelcast.core</code> package are now in specific packages like
<code>com.hazelcast.map</code> or <code>com.hazelcast.collection</code>.</p></li>
<li><p><code>com.hazelcast.jet.function</code>, the package containing serializable
variants of <code>java.util.function</code>, is now merged into
<code>com.hazelcast.function</code>: <code>BiConsumerEx</code>, <code>BiFunctionEx</code>,
<code>BinaryOperatorEx</code>, <code>BiPredicateEx</code>, <code>ComparatorEx</code>, <code>ComparatorsEx</code>,
<code>ConsumerEx</code>, <code>FunctionEx</code>, <code>Functions</code>, <code>PredicateEx</code>, <code>SupplierEx</code>,
<code>ToDoubleFunctionEx</code>, <code>ToIntFunctionEx</code>, <code>ToLongFunctionEx</code>.</p></li>
<li><p><code>EntryProcessor</code> and several other classes and methods received a
cleanup of their type parameters. See the <a href="https://docs.hazelcast.org/docs/4.0/manual/html-single/#introducing-lambda-friendly-interfaces">relevant section</a>
in the IMDG Migration Guide.</p></li>
<li><p>The term &quot;group&quot; in configuration was replaced with &quot;cluster&quot;. See the
code snippet below for an example. This changes a Jet Command Line
parameter as well (<code>-g/--groupName</code> renamed to <code>-n/--cluster-name</code>).</p>
<pre><code class="hljs css language-java">clientConfig<span class="token punctuation">.</span><span class="token function">setClusterName</span><span class="token punctuation">(</span><span class="token string">"cluster_name"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token comment">//clientConfig.getGroupConfig().setName("cluster_name")</span>
</code></pre></li>
<li><p><code>EventJournalConfig</code> moved from the top-level Config class to data
structure-specific configs (<code>MapConfig</code>, <code>CacheConfig</code>):</p>
<pre><code class="hljs css language-java">config<span class="token punctuation">.</span><span class="token function">getMapConfig</span><span class="token punctuation">(</span><span class="token string">"map_name"</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">getEventJournalConfig</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token comment">//config.getMapEventJournalConfig("map_name")</span>
</code></pre></li>
<li><p><code>ICompletableFuture</code> was removed and replaced with the JDK-standard
<code>CompletionStage</code>. This affects the return type of async methods. See
the <a href="https://docs.hazelcast.org/docs/4.0/manual/html-single/#removal-of-icompletablefuture">relevant section</a>
in the IMDG Migration Guide.</p></li>
</ul>
<h2><a class="anchor" aria-hidden="true" id="jet-api-changes"></a><a href="#jet-api-changes" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Jet API Changes</h2>
<p>We made multiple breaking changes in Jet’s own APIs too:</p>
<ul>
<li><p><code>IMapJet</code>, <code>ICacheJet</code> and <code>IListJet</code>, which used to be Jet-specific
wrappers around IMDG’s standard <code>IMap</code>, <code>ICache</code> and <code>IList</code>, were
removed. The methods that used to return these types now return the
standard ones.</p></li>
<li><p>Renamed <code>Pipeline.drawFrom</code> to <code>Pipeline.readFrom</code> and
<code>GeneralStage.drainTo</code> to <code>GeneralStage.writeTo</code>:</p>
<pre><code class="hljs css language-java">pipeline<span class="token punctuation">.</span><span class="token function">readFrom</span><span class="token punctuation">(</span><span class="token class-name">TestSources</span><span class="token punctuation">.</span><span class="token function">items</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">writeTo</span><span class="token punctuation">(</span><span class="token class-name">Sinks</span><span class="token punctuation">.</span><span class="token function">logger</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token comment">//pipeline.drawFrom(TestSources.items(1, 2, 3)).drainTo(Sinks.logger());</span>
</code></pre></li>
<li><p><code>ContextFactory</code> was renamed to <code>ServiceFactory</code> and we added support
for instance-wide initialization. createFn now takes
<code>ProcessorSupplier.Context</code> instead of just <code>JetInstance</code>. We also
added convenience methods in <code>ServiceFactories</code> to simplify
constructing the common variants:</p>
<pre><code class="hljs css language-java"><span class="token class-name">ServiceFactories</span><span class="token punctuation">.</span><span class="token function">sharedService</span><span class="token punctuation">(</span>ctx <span class="token operator">-></span> <span class="token class-name">Executors</span><span class="token punctuation">.</span><span class="token function">newFixedThreadPool</span><span class="token punctuation">(</span><span class="token number">8</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token class-name">ExecutorService</span><span class="token operator">::</span><span class="token function">shutdown</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token comment">//ContextFactory.withCreateFn(jet -> Executors.newFixedThreadPool(8)).withLocalSharing();</span>

<span class="token class-name">ServiceFactories</span><span class="token punctuation">.</span><span class="token function">nonSharedService</span><span class="token punctuation">(</span>ctx <span class="token operator">-></span> <span class="token class-name">DateTimeFormatter</span><span class="token punctuation">.</span><span class="token function">ofPattern</span><span class="token punctuation">(</span><span class="token string">"HH:mm:ss.SSS"</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token class-name">ConsumerEx</span><span class="token punctuation">.</span><span class="token function">noop</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token comment">//ContextFactory.withCreateFn(jet -> DateTimeFormatter.ofPattern("HH:mm:ss.SSS"))</span>
</code></pre></li>
<li><p><code>map/filter/flatMapUsingContext</code> was renamed to
<code>map/filter/flatMapUsingService</code>:</p>
<pre><code class="hljs css language-java">pipeline<span class="token punctuation">.</span><span class="token function">readFrom</span><span class="token punctuation">(</span><span class="token class-name">TestSources</span><span class="token punctuation">.</span><span class="token function">items</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token punctuation">.</span><span class="token function">filterUsingService</span><span class="token punctuation">(</span>
                <span class="token class-name">ServiceFactories</span><span class="token punctuation">.</span><span class="token function">sharedService</span><span class="token punctuation">(</span>pctx <span class="token operator">-></span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                <span class="token punctuation">(</span>svc<span class="token punctuation">,</span> i<span class="token punctuation">)</span> <span class="token operator">-></span> i <span class="token operator">%</span> <span class="token number">2</span> <span class="token operator">==</span> svc<span class="token punctuation">)</span>
        <span class="token punctuation">.</span><span class="token function">writeTo</span><span class="token punctuation">(</span><span class="token class-name">Sinks</span><span class="token punctuation">.</span><span class="token function">logger</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

<span class="token comment">/*
pipeline.drawFrom(TestSources.items(1, 2, 3))
        .filterUsingContext(
                ContextFactory.withCreateFn(i -> 1),
                (ctx, i) -> i % 2 == ctx)
        .drainTo(Sinks.logger());
*/</span>
</code></pre></li>
<li><p><code>filterUsingServiceAsync</code> has been removed. Usages can be replaced
with <code>mapUsingServiceAsync</code>, which behaves like a filter if it returns
a <code>null</code> future or the returned future contains a <code>null</code> result:</p>
<pre><code class="hljs css language-java">stage<span class="token punctuation">.</span><span class="token function">mapUsingServiceAsync</span><span class="token punctuation">(</span>serviceFactory<span class="token punctuation">,</span>
        <span class="token punctuation">(</span>executor<span class="token punctuation">,</span> item<span class="token punctuation">)</span> <span class="token operator">-></span> <span class="token punctuation">{</span>
            <span class="token class-name">CompletableFuture</span><span class="token generics"><span class="token punctuation">&lt;</span><span class="token class-name">Long</span><span class="token punctuation">></span></span> f <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">CompletableFuture</span><span class="token generics"><span class="token punctuation">&lt;</span><span class="token punctuation">></span></span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
            executor<span class="token punctuation">.</span><span class="token function">submit</span><span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">-></span> f<span class="token punctuation">.</span><span class="token function">complete</span><span class="token punctuation">(</span>item <span class="token operator">%</span> <span class="token number">2</span> <span class="token operator">==</span> <span class="token number">0</span> <span class="token operator">?</span> item <span class="token operator">:</span> <span class="token keyword">null</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
            <span class="token keyword">return</span> f<span class="token punctuation">;</span>
        <span class="token punctuation">}</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token comment">/*
stage.filterUsingServiceAsync(serviceFactory,
        (executor, item) -> {
            CompletableFuture&lt;Boolean> f = new CompletableFuture&lt;>();
            executor.submit(() -> f.complete(item % 2 == 0));
            return f;
        });
*/</span>
</code></pre></li>
<li><p><code>flatMapUsingServiceAsync</code> has been removed. Usages can be replaced
with <code>mapUsingServiceAsync</code> followed by non-async <code>flatMap</code>:</p>
<pre><code class="hljs css language-java">stage<span class="token punctuation">.</span><span class="token function">mapUsingServiceAsync</span><span class="token punctuation">(</span>serviceFactory<span class="token punctuation">,</span>
        <span class="token punctuation">(</span>executor<span class="token punctuation">,</span> item<span class="token punctuation">)</span> <span class="token operator">-></span> <span class="token punctuation">{</span>
            <span class="token class-name">CompletableFuture</span><span class="token generics"><span class="token punctuation">&lt;</span><span class="token class-name">List</span><span class="token punctuation">&lt;</span><span class="token class-name">String</span><span class="token punctuation">></span><span class="token punctuation">></span></span> f <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">CompletableFuture</span><span class="token generics"><span class="token punctuation">&lt;</span><span class="token punctuation">></span></span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
            executor<span class="token punctuation">.</span><span class="token function">submit</span><span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">-></span> f<span class="token punctuation">.</span><span class="token function">complete</span><span class="token punctuation">(</span><span class="token class-name">Arrays</span><span class="token punctuation">.</span><span class="token function">asList</span><span class="token punctuation">(</span>item <span class="token operator">+</span> <span class="token string">"-1"</span><span class="token punctuation">,</span> item <span class="token operator">+</span> <span class="token string">"-2"</span><span class="token punctuation">,</span> item <span class="token operator">+</span> <span class="token string">"-3"</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
            <span class="token keyword">return</span> f<span class="token punctuation">;</span>
        <span class="token punctuation">}</span><span class="token punctuation">)</span>
        <span class="token punctuation">.</span><span class="token function">flatMap</span><span class="token punctuation">(</span><span class="token class-name">Traversers</span><span class="token operator">::</span><span class="token function">traverseIterable</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token comment">/*
stage.flatMapUsingServiceAsync(serviceFactory,
        (executor, item) -> {
            CompletableFuture&lt;Traverser&lt;String>> f = new CompletableFuture&lt;>();
            executor.submit(() -> f.complete(traverseItems(item + "-1", item + "-2", item + "-3")));
            return f;
        })
*/</span>
</code></pre></li>
<li><p>The methods <code>withMaxPendingCallsPerProcessor(int)</code> and
<code>withUnorderedAsyncResponses()</code> were removed from <code>ServiceFactory</code>.
These properties are relevant only in the context of asynchronous
operations and were used in conjunction with
<code>GeneralStage.mapUsingServiceAsync(…)</code>. In Jet 4.0 the
<code>GeneralStage.mapUsingServiceAsync(…)</code> method has a new variant with
explicit parameters for the above settings:</p>
<pre><code class="hljs css language-java">stage<span class="token punctuation">.</span><span class="token function">mapUsingServiceAsync</span><span class="token punctuation">(</span>
        <span class="token class-name">ServiceFactories</span><span class="token punctuation">.</span><span class="token function">sharedService</span><span class="token punctuation">(</span>ctx <span class="token operator">-></span> <span class="token class-name">Executors</span><span class="token punctuation">.</span><span class="token function">newFixedThreadPool</span><span class="token punctuation">(</span><span class="token number">8</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
        <span class="token number">2</span><span class="token punctuation">,</span>
        <span class="token boolean">false</span><span class="token punctuation">,</span>
        <span class="token punctuation">(</span>exec<span class="token punctuation">,</span> task<span class="token punctuation">)</span> <span class="token operator">-></span> <span class="token class-name">CompletableFuture</span><span class="token punctuation">.</span><span class="token function">supplyAsync</span><span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">-></span> task<span class="token punctuation">,</span> exec<span class="token punctuation">)</span>
<span class="token punctuation">)</span><span class="token punctuation">;</span>

<span class="token comment">/*
stage.mapUsingContextAsync(
        ContextFactory.withCreateFn(jet -> Executors.newFixedThreadPool(8))
                .withMaxPendingCallsPerProcessor(2)
                .withUnorderedAsyncResponses(),
        (exec, task) -> CompletableFuture.supplyAsync(() -> task, exec)
);
*/</span>
</code></pre></li>
<li><p><code>com.hazelcast.jet.pipeline.Sinks#mapWithEntryProcessor</code> got a new
signature in order to accommodate the improved <code>EntryProcessor</code>, which
became more lambda-friendly in IMDG (see the <a href="https://docs.hazelcast.org/docs/4.0/manual/html-single/#introducing-lambda-friendly-interfaces">relevant section</a>
in the IMDG Migration Guide). The return type of <code>EntryProcessor</code> is
now an explicit parameter in <code>mapWithEntryProcessor</code>'s method
signature:</p>
<pre><code class="hljs css language-java"><span class="token class-name">FunctionEx</span><span class="token generics"><span class="token punctuation">&lt;</span><span class="token class-name">Map<span class="token punctuation">.</span>Entry</span><span class="token punctuation">&lt;</span><span class="token class-name">String</span><span class="token punctuation">,</span> <span class="token class-name">Integer</span><span class="token punctuation">></span><span class="token punctuation">,</span> <span class="token class-name">EntryProcessor</span><span class="token punctuation">&lt;</span><span class="token class-name">String</span><span class="token punctuation">,</span> <span class="token class-name">Integer</span><span class="token punctuation">,</span> <span class="token class-name">Void</span><span class="token punctuation">></span><span class="token punctuation">></span></span> entryProcFn <span class="token operator">=</span>
        entry <span class="token operator">-></span>
                <span class="token punctuation">(</span><span class="token class-name">EntryProcessor</span><span class="token generics"><span class="token punctuation">&lt;</span><span class="token class-name">String</span><span class="token punctuation">,</span> <span class="token class-name">Integer</span><span class="token punctuation">,</span> <span class="token class-name">Void</span><span class="token punctuation">></span></span><span class="token punctuation">)</span> e <span class="token operator">-></span> <span class="token punctuation">{</span>
                    e<span class="token punctuation">.</span><span class="token function">setValue</span><span class="token punctuation">(</span>e<span class="token punctuation">.</span><span class="token function">getValue</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token keyword">null</span> <span class="token operator">?</span> <span class="token number">1</span> <span class="token operator">:</span> e<span class="token punctuation">.</span><span class="token function">getValue</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
                    <span class="token keyword">return</span> <span class="token keyword">null</span><span class="token punctuation">;</span>
                <span class="token punctuation">}</span><span class="token punctuation">;</span>
<span class="token class-name">Sinks</span><span class="token punctuation">.</span><span class="token function">mapWithEntryProcessor</span><span class="token punctuation">(</span>map<span class="token punctuation">,</span> <span class="token class-name">Map<span class="token punctuation">.</span>Entry</span><span class="token operator">::</span><span class="token function">getKey</span><span class="token punctuation">,</span> entryProcFn<span class="token punctuation">)</span><span class="token punctuation">;</span>

<span class="token comment">/*
FunctionEx&lt;Map.Entry&lt;String, Integer>, EntryProcessor&lt;String, Integer>> entryProcFn =
        entry ->
                (EntryProcessor&lt;String, Integer>) e -> {
                    e.setValue(e.getValue() == null ? 1 : e.getValue() + 1);
                    return null;
                };
Sinks.mapWithEntryProcessor(map, Map.Entry::getKey, entryProcFn);
*/</span>
</code></pre></li>
<li><p>HDFS source and sink methods are now <code>Hadoop.inputFormat</code> and
<code>Hadoop.outputFormat</code>.</p></li>
<li><p><code>MetricsConfig</code> is no longer part of <code>JetConfig</code>, but resides in the
IMDG <code>Config</code> class:</p>
<pre><code class="hljs css language-java">jetConfig<span class="token punctuation">.</span><span class="token function">getHazelcastConfig</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">getMetricsConfig</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">setCollectionFrequencySeconds</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token comment">//jetConfig.getMetricsConfig().setCollectionIntervalSeconds(1);</span>
</code></pre></li>
<li><p><code>Traverser</code> type got a slight change in the <code>flatMap</code> lambda’s generic
type wildcards. This change shouldn’t affect anything in practice.</p></li>
<li><p>In sources and sinks we changed the method signatures so that the
lambda becomes the last parameter, where applicable.</p></li>
<li><p><code>JetBootstrap.getInstance()</code> moved to <code>Jet.bootstrappedInstance()</code> and
now it automatically creates an isolated local instance when not
running through <code>jet submit</code>. If used from <code>jet submit</code>, the behaviour
remains the same.</p></li>
<li><p><code>JobConfig.addResource(…)</code> is now <code>addClasspathResource(…)</code>.</p></li>
<li><p><code>ResourceType</code>, <code>ResourceConfig</code> and <code>JobConfig.getResourceConfigs()</code>
are now labeled as private API and we discourage their direct usage.
We also renamed <code>ResourceType.REGULAR_FILE</code> to <code>ResourceType.FILE</code>,
but this is now an internal change.</p></li>
</ul>
<h2><a class="anchor" aria-hidden="true" id="further-help"></a><a href="#further-help" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Further help</h2>
<p>In case you encounter any difficulties with migrating to Jet 4.0 feel
free to <a href="https://gitter.im/hazelcast/hazelcast-jet">contact us any time</a>.</p>
</span></div></article></div><div class="post"><header class="postHeader"><h1 class="postHeaderTitle"><a href="/blog/2020/03/30/ml-inference">Machine Learning Inference at Scale</a></h1><p class="post-meta">March 30, 2020</p><div class="authorBlock"><p class="post-authorName"><a href="https://twitter.com/voloda" target="_blank" rel="noreferrer noopener">Vladimir Schreiner</a></p><div class="authorPhoto"><a href="https://twitter.com/voloda" target="_blank" rel="noreferrer noopener"><img src="https://3l0wd94f0qdd10om8642z9se-wpengine.netdna-ssl.com/wp-content/uploads/2018/10/speaker-vladimir-schreiner-e1551380845855-170x170.jpg" alt="Vladimir Schreiner"/></a></div></div></header><article class="post-content"><div><span><p>Machine learning projects can be split into two phases:</p>
<ul>
<li>Training</li>
<li>Inference</li>
</ul>
<p>During the training phase, data science teams have to obtain, analyze
and understand available data and generalize it into a mathematical
model. The model uses the features of the sample data to reason about
data it has never seen. Although it can be completely custom code, it is
usually based on proven machine learning algorithms, such as Naïve
Bayes, K Means, Linear Regression, Deep Learning, Random Forests or
Decision Trees. The act of building the model from the sample (training)
data is referred to as training.</p>
<p>The inference phase refers to using the model to predict an unknown
property of the input data. This requires deploying the model into a
production environment and operating it.</p>
<h2><a class="anchor" aria-hidden="true" id="operating-machine-learning"></a><a href="#operating-machine-learning" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Operating Machine Learning</h2>
<p>The most straightforward way to deploy the model is to wrap it in a REST
web service and let other applications remotely invoke the inference
service. Many machine learning frameworks provide such a service
out-of-the-box to support simple deployments that don’t deal with much
data.</p>
<p>What Hazelcast Jet adds to this story is a simple way to deploy the
model so that it is automatically parallelized and scaled out across a
cluster of machines.</p>
<p><img src="/blog/assets/2020-03-30-parallel-inference.png" alt="Parallel ML inference"></p>
<p>Jet uses its parallel, distributed and resilient execution engine to
turn the model into a high-performance inference service. To use all
available CPU cores, Jet spins up multiple parallel instances of the
model and spreads the inference requests among them. The Jet cluster is
elastic; to scale with the workload, add or remove cluster members on
the fly with no downtime.</p>
<p>Another trick is using a <em>pipelined</em> design instead of a request-reply
pattern. It allows Jet to batch inference requests together and reduce
fixed overheads of serving each request individually. This improves the
overall throughput of the model significantly! The pipelined design
requires a change in the client’s workflow. Instead of calling the
inference service directly, it sends its inference request to an inbox.
It may be implemented using a message broker such as JMS topic, Kafka or
distributed topic of Hazelcast. Jet watches the inbox and groups
multiple requests together to use the model service efficiently. It uses
<a href="https://mechanical-sympathy.blogspot.com/2011/10/smart-batching.html">smart
batching</a>
where the batch size changes with the data volume to keep the latency
always low. The inference results are published to an outbox for callers
to pick it up.</p>
<h2><a class="anchor" aria-hidden="true" id="models-supported"></a><a href="#models-supported" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Models Supported</h2>
<h3><a class="anchor" aria-hidden="true" id="python-models"></a><a href="#python-models" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Python models</h3>
<p>Python is the lingua franca of the data science world. There is a wide
ecosystem of libraries and tools to build and train models in Python:
<a href="https://www.tensorflow.org/">TensorFlow</a>, <a href="https://keras.io/">Keras</a>,
<a href="http://deeplearning.net/software/theano/">Theano</a>,
<a href="https://scikit-learn.org/stable/">Scikit-learn</a> or
<a href="https://pytorch.org/">PyTorch</a> to name a few. Jet can host any Python
model.</p>
<p>Upon model deployment, Jet’s JVM runtime launches Python processes and
establishes bi-directional gRPC communication channels to stream
inference requests through it. So, the model runs natively in a Python
process that is completely managed by Jet. It can be tuned to spin
multiple Python processes on each machine to make use of multicore
processors.</p>
<p>Jet makes sure that the Python code is distributed to all machines that
participate in the cluster. If you add another machine to a Jet cluster,
it creates a directory on it and deploys the Python code there.
Moreover, Jet can install all required Python libraries to prepare the
runtime for your Python model.</p>
<p>Documentation:
<a href="https://docs.hazelcast.org/docs/jet/latest/manual/#map-using-python">https://docs.hazelcast.org/docs/jet/latest/manual/#map-using-python</a></p>
<p>Code sample:
<a href="https://github.com/hazelcast/hazelcast-jet/tree/master/examples/python">https://github.com/hazelcast/hazelcast-jet/tree/master/examples/python</a></p>
<p><img src="/blog/assets/2020-03-30-python-vms.svg" alt="Python integration architecture"></p>
<h3><a class="anchor" aria-hidden="true" id="java-models"></a><a href="#java-models" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Java models</h3>
<p>Java models are used for high-performance inference execution. Favourite
Java model libraries include
<a href="https://github.com/jpmml/jpmml-evaluator">JPMML</a>, <a href="https://www.tensorflow.org/install/lang_java">TensorFlow for
Java</a>,
<a href="https://mxnet.apache.org/api/java">MXNet</a>, <a href="https://xgboost.readthedocs.io/en/latest/jvm/index.html">XGBoost JVM
Package</a> and
<a href="https://www.h2o.ai/">H20</a>.</p>
<p>Similarly to Python, the model is packaged as a Jet Job resource. The
Job usually includes model inference code (the ML library) and a
serialized model. Jet runs the Java models in-process with the cluster
members so there is no need to start extra processes and there is no
communication overhead (serialization, deserialization, networking).
This makes Java model the best performing option. The inference job can
be configured to use one model instance per JVM or multiple model
instances.</p>
<p>Documentation:
<a href="https://docs.hazelcast.org/docs/jet/latest/manual/#machine-learning-model-prediction">https://docs.hazelcast.org/docs/jet/latest/manual/#machine-learning-model-prediction</a></p>
<p>Code samples:</p>
<ul>
<li><a href="https://github.com/hazelcast/hazelcast-jet-demos/tree/master/h2o-breast-cancer-classification">H2O Model</a></li>
<li><a href="https://github.com/hazelcast/hazelcast-jet-demos/tree/master/tensorflow">TensorFlow Model</a></li>
<li><a href="https://github.com/hazelcast/hazelcast-jet-demos/tree/master/realtime-image-recognition">Custom Java Model</a></li>
</ul>
<h3><a class="anchor" aria-hidden="true" id="remote-services"></a><a href="#remote-services" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Remote services</h3>
<p>We started this article by saying that using a model as an RPC service
is simple but requires extra effort when scaling. Jet supports this
pattern, too. The Jet Job can invoke a remote inference service. The
model isn’t managed by Jet in this case, so the operational and
performance advantages are gone. Jet still provides the convenience of
smart batching, inbox/outbox <a href="/docs/api/sources-sinks">connectors</a> and
many <a href="/docs/api/pipeline#types-of-transforms">pipeline operators</a>. Smart
batching works only if the RPC service can operate on batches of input
items.</p>
<p>Benefits of this setup</p>
<ul>
<li>Isolating the model service and the data pipeline</li>
<li>Sharing the model among many Jet pipelines</li>
</ul>
<p>Code samples:</p>
<ul>
<li><a href="https://github.com/hazelcast/hazelcast-jet/tree/master/examples/grpc">Invoking remote gRPC service</a></li>
<li><a href="https://github.com/hazelcast/hazelcast-jet-demos/tree/master/tensorflow">Remote TensorFlow</a></li>
</ul>
<h3><a class="anchor" aria-hidden="true" id="execution-mode-overview"></a><a href="#execution-mode-overview" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Execution Mode Overview</h3>
<table>
<thead>
<tr><th>Execution Mode</th><th>Java Model</th><th>Python Model</th><th>Remote Model</th></tr>
</thead>
<tbody>
<tr><td>Model managed by Jet</td><td>✅</td><td>✅</td><td>✅</td></tr>
<tr><td>Model shared between Jobs</td><td>❌</td><td>❌</td><td>✅</td></tr>
<tr><td>Jet ↔ Model Communication</td><td>Shared memory</td><td>gRPC<br>(processes collocated)</td><td>RPC<br>(processes usually on different machines)</td></tr>
<tr><td>Throughput (single node)</td><td>1M / sec</td><td>50k / sec</td><td>Depends on underlying architecture</td></tr>
<tr><td>Prerequisites</td><td>Model runs in JVM</td><td>Python runtime installed on all cluster machines</td><td>Model available as a RPC service</td></tr>
</tbody>
</table>
<h3><a class="anchor" aria-hidden="true" id="framework-integration-overview"></a><a href="#framework-integration-overview" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Framework Integration Overview</h3>
<table>
<thead>
<tr><th>Framework</th><th>Execution Mode</th><th>Code Sample</th></tr>
</thead>
<tbody>
<tr><td>H2O</td><td>Java</td><td><a href="https://github.com/hazelcast/hazelcast-jet-demos/tree/master/h2o-breast-cancer-classification">Code Sample</a></td></tr>
<tr><td>TensorFlow for Java</td><td>Java</td><td><a href="https://github.com/hazelcast/hazelcast-jet-demos/blob/master/tensorflow/src/main/java/InProcessClassification.java">Code Sample</a></td></tr>
<tr><td>Custom Java Model</td><td>Java</td><td><a href="https://github.com/hazelcast/hazelcast-jet-demos/tree/master/realtime-image-recognition">Code Sample</a></td></tr>
<tr><td>PMML</td><td>Java</td><td>N/A, use <a href="https://github.com/jpmml/jpmml-evaluator">JPMML Evaluator</a> as a Custom Java Model</td></tr>
<tr><td>MXNet</td><td>Java</td><td>N/A, use <a href="https://mxnet.apache.org/api/java.html">MXNet Java Inference API</a> as a Custom Java Model</td></tr>
<tr><td>XGBoost</td><td>Java</td><td>N/A, use <a href="https://xgboost.readthedocs.io/en/latest/jvm/index.html">XGBoost JVM Package</a> as a Custom Java Model</td></tr>
<tr><td><a href="https://keras.io/">Keras</a>, <a href="http://deeplearning.net/software/theano/">Theano</a>, <a href="https://scikit-learn.org/stable/">Scikit-learn</a> or <a href="https://pytorch.org/">PyTorch</a></td><td>Python</td><td>N/A, use the <a href="https://github.com/hazelcast/hazelcast-jet/tree/master/examples/python">Custom Python Model</a></td></tr>
<tr><td>Custom Python Model</td><td>Python</td><td><a href="https://github.com/hazelcast/hazelcast-jet/tree/master/examples/python">Code Sample</a></td></tr>
<tr><td>Remote gRPC service</td><td>Remote</td><td><a href="https://github.com/hazelcast/hazelcast-jet/tree/master/examples/grpc">Code Sample</a></td></tr>
<tr><td>Remote TensorFlow service</td><td>Remote</td><td><a href="https://github.com/hazelcast/hazelcast-jet-demos/blob/master/tensorflow/src/main/java/ModelServerClassification.java">Code Sample</a></td></tr>
</tbody>
</table>
</span></div></article></div><div class="post"><header class="postHeader"><h1 class="postHeaderTitle"><a href="/blog/2020/03/02/jet-40-is-released">Jet 4.0 is Released</a></h1><p class="post-meta">March 2, 2020</p><div class="authorBlock"><p class="post-authorName"><a href="http://twitter.com/cgencer" target="_blank" rel="noreferrer noopener">Can Gencer</a></p><div class="authorPhoto"><a href="http://twitter.com/cgencer" target="_blank" rel="noreferrer noopener"><img src="https://pbs.twimg.com/profile_images/1187734846749196288/elqWdrPj_400x400.jpg" alt="Can Gencer"/></a></div></div></header><article class="post-content"><div><span><p>We're happy to introduce the release of Jet 4.0 which brings several new
features. This release was a big effort and a total of <a href="https://github.com/hazelcast/hazelcast-jet/pulls?q=is%3Apr+milestone%3A4.0">230
PRs</a>
were merged, making it one of our biggest in terms of new features.</p>
<h2><a class="anchor" aria-hidden="true" id="distributed-transactions"></a><a href="#distributed-transactions" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Distributed Transactions</h2>
<p>Jet previously had first-class support for fault tolerance through an
implementation of the <a href="https://lamport.azurewebsites.net/pubs/chandy.pdf">Chandy-Lamport distributed snapshotting</a>
algorithm which requires participation from the whole pipeline,
including sources and sinks. Previously, the at-least-once and
exactly-once processing guarantees were only limited to replayable
sources such as Kafka. Jet 4.0 comes with a full two-phase commit (2PC)
implementation which makes it possible to have end-to-end exactly-once
processing with acknowledgement-based sources such as JMS. Jet is now
also able to work with transactional sinks to avoid duplicate writes, and
this version adds transactional file and Kafka sinks, with transactional
JMS and JDBC sinks utilizing XA transactions coming in the next release.</p>
<p>We will have additional posts about this topic in the future detailing
the mechanism and also results of our tests done with 2PC for various
message brokers and databases.</p>
<h2><a class="anchor" aria-hidden="true" id="python-user-defined-functions"></a><a href="#python-user-defined-functions" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Python User-Defined Functions</h2>
<p>Python is a popular language with a very large ecosystem of libraries,
and has especially become popular in the domain of data processing and
machine learning. Jet itself is a data processing framework for both
streams and batches of data, but the API for defining the pipeline
itself has been previously limited to Java and Java functions only.</p>
<p>In this version we have added a native way to execute Python code within
a Jet pipeline. Jet can now spawn separate Python processes on
each node which communicate back using
<a href="https://github.com/hazelcast/hazelcast-jet-demos/tree/master/debezium-cdc-without-kafka">gRPC</a>.
The processes are fully managed by Jet and can make use of techniques
such as smart batching of events.</p>
<p>The user defines a mapping stage which takes an input item, and
transforms it using a supplied Python function. The function can make
use of libraries such as scikit, numpy and many others. This makes it
possible to use Jet for deploying ML models into production. For
example, given this pipeline:</p>
<pre><code class="hljs css language-java"><span class="token class-name">Pipeline</span> p <span class="token operator">=</span> <span class="token class-name">Pipeline</span><span class="token punctuation">.</span><span class="token function">create</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
p<span class="token punctuation">.</span><span class="token function">readFrom</span><span class="token punctuation">(</span><span class="token class-name">TestSources</span><span class="token punctuation">.</span><span class="token function">itemStream</span><span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span> <span class="token punctuation">(</span>ts<span class="token punctuation">,</span> seq<span class="token punctuation">)</span> <span class="token operator">-></span> <span class="token function">bigRandomNumberAsString</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token punctuation">.</span><span class="token function">withoutTimestamps</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token punctuation">.</span><span class="token function">apply</span><span class="token punctuation">(</span><span class="token function">mapUsingPython</span><span class="token punctuation">(</span><span class="token keyword">new</span> <span class="token class-name">PythonServiceConfig</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
            <span class="token punctuation">.</span><span class="token function">setBaseDir</span><span class="token punctuation">(</span>baseDir<span class="token punctuation">)</span>
            <span class="token punctuation">.</span><span class="token function">setHandlerModule</span><span class="token punctuation">(</span><span class="token string">"take_sqrt"</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token punctuation">.</span><span class="token function">writeTo</span><span class="token punctuation">(</span><span class="token class-name">Sinks</span><span class="token punctuation">.</span><span class="token function">observable</span><span class="token punctuation">(</span><span class="token constant">RESULTS</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
</code></pre>
<p>The user only has to supply the following Python function:</p>
<pre><code class="hljs css language-python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np

<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">transform_list</span><span class="hljs-params">(input_list)</span>:</span>
    <span class="hljs-string">"""
    Uses NumPy to transform a list of numbers into a list of their square
    roots.

    :param input_list: the list with input items
    :return: the list with input items' square roots
    """</span>
    num_list = [float(it) <span class="hljs-keyword">for</span> it <span class="hljs-keyword">in</span> input_list]
    sqrt_list = np.sqrt(num_list)
    <span class="hljs-keyword">return</span> [str(it) <span class="hljs-keyword">for</span> it <span class="hljs-keyword">in</span> sqrt_list]
</code></pre>
<p>For a more in-depth discussion on this topic, I recommend Jet Core
Engineer Marko Topolnik's presentation,
<a href="https://www.youtube.com/watch?v=q1vBbqxnJIQ">Deploying ML models at scale</a>.</p>
<h2><a class="anchor" aria-hidden="true" id="observables"></a><a href="#observables" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Observables</h2>
<p>When you submit a Jet pipeline, typically it reads the data from a
source and writes to a sink (such as a <code>IMap</code>). When the submitter of
the pipeline wants to read the results, the sink must be read outside of
the pipeline, which is not always very convenient.</p>
<p>In Jet 4.0, a new sink type called <code>Observable</code> is added which can be
used to publish messages directly to the caller. It utilizes a Hazelcast
Ringbuffer as the underlying data store which allows the decoupling of
the producer and consumer.</p>
<pre><code class="hljs css language-java"><span class="token class-name">Observable</span><span class="token generics"><span class="token punctuation">&lt;</span><span class="token class-name">SimpleEvent</span><span class="token punctuation">></span></span> o <span class="token operator">=</span> jet<span class="token punctuation">.</span><span class="token function">newObservable</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
o<span class="token punctuation">.</span><span class="token function">addObserver</span><span class="token punctuation">(</span>event <span class="token operator">-></span> <span class="token class-name">System</span><span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span>event<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
p<span class="token punctuation">.</span><span class="token function">readFrom</span><span class="token punctuation">(</span><span class="token class-name">TestSources</span><span class="token punctuation">.</span><span class="token function">itemStream</span><span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
 <span class="token punctuation">.</span><span class="token function">withoutTimestamps</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
 <span class="token punctuation">.</span><span class="token function">writeTo</span><span class="token punctuation">(</span><span class="token class-name">Sinks</span><span class="token punctuation">.</span><span class="token function">observable</span><span class="token punctuation">(</span>o<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
jet<span class="token punctuation">.</span><span class="token function">newJob</span><span class="token punctuation">(</span>p<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">join</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
</code></pre>
<p>The <code>Observable</code> can also be used to be notified of a job's completion
and any errors that may occur during processing.</p>
<h2><a class="anchor" aria-hidden="true" id="custom-metrics"></a><a href="#custom-metrics" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Custom Metrics</h2>
<p>Over the last few releases we've been improving the metrics support in
Jet, such as being able to get metrics directly from running or
completed jobs through the use of <code>Job.getMetrics()</code>. In this release,
we've made it possible to also add your own custom metrics into a
pipeline through the use of a simple API:</p>
<pre><code class="hljs css language-java">p<span class="token punctuation">.</span><span class="token function">readFrom</span><span class="token punctuation">(</span><span class="token class-name">TestSources</span><span class="token punctuation">.</span><span class="token function">itemStream</span><span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
 <span class="token punctuation">.</span><span class="token function">withoutTimestamps</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
 <span class="token punctuation">.</span><span class="token function">map</span><span class="token punctuation">(</span>event <span class="token operator">-></span> <span class="token punctuation">{</span>
     <span class="token keyword">if</span> <span class="token punctuation">(</span>event<span class="token punctuation">.</span><span class="token function">sequence</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">%</span> <span class="token number">2</span> <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>
         <span class="token class-name">Metrics</span><span class="token punctuation">.</span><span class="token function">metric</span><span class="token punctuation">(</span><span class="token string">"numEvens"</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">increment</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
     <span class="token punctuation">}</span>
     <span class="token keyword">return</span> event<span class="token punctuation">;</span>
 <span class="token punctuation">}</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">writeTo</span><span class="token punctuation">(</span><span class="token class-name">Sinks</span><span class="token punctuation">.</span><span class="token function">logger</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
</code></pre>
<p>These custom metrics will then be available as part of
<code>Job.getMetrics()</code> or through JMX along with the rest of the metrics.</p>
<h2><a class="anchor" aria-hidden="true" id="debezium-kafka-connect-and-twitter-connectors"></a><a href="#debezium-kafka-connect-and-twitter-connectors" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Debezium, Kafka Connect and Twitter Connectors</h2>
<p>As part of Jet 4.0, we're releasing three new connectors:</p>
<h3><a class="anchor" aria-hidden="true" id="debezium"></a><a href="#debezium" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Debezium</h3>
<p>Debezium is a Change Data Capture (CDC) platform and the new
<a href="https://debezium.io/">Debezium</a> connector for Jet allows you to stream
changes directly from databases such as MySQL and PostgreSQL without
requiring any other dependencies.</p>
<p>Although Debezium typically requires use of Kafka and Kafka Connect, the
native Jet integration means you can directly stream changes without
having to use Kafka. The integration also supports fault-tolerance so
that when a Jet job is scaled up or down, old changes do not need to
replayed.</p>
<p>This makes it suitable to build an end-to-end solution where for example
an in-memory cache supported by <code>IMap</code> is always kept up to date with the
latest changes in the database.</p>
<pre><code class="hljs css language-java"><span class="token class-name">Configuration</span> configuration <span class="token operator">=</span> <span class="token class-name">Configuration</span>
        <span class="token punctuation">.</span><span class="token function">create</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token punctuation">.</span><span class="token keyword">with</span><span class="token punctuation">(</span><span class="token string">"name"</span><span class="token punctuation">,</span> <span class="token string">"mysql-inventory-connector"</span><span class="token punctuation">)</span>
        <span class="token punctuation">.</span><span class="token keyword">with</span><span class="token punctuation">(</span><span class="token string">"connector.class"</span><span class="token punctuation">,</span> <span class="token string">"io.debezium.connector.mysql.MySqlConnector"</span><span class="token punctuation">)</span>
        <span class="token comment">/* begin connector properties */</span>
        <span class="token punctuation">.</span><span class="token keyword">with</span><span class="token punctuation">(</span><span class="token string">"database.hostname"</span><span class="token punctuation">,</span> mysql<span class="token punctuation">.</span><span class="token function">getContainerIpAddress</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token punctuation">.</span><span class="token keyword">with</span><span class="token punctuation">(</span><span class="token string">"database.port"</span><span class="token punctuation">,</span> mysql<span class="token punctuation">.</span><span class="token function">getMappedPort</span><span class="token punctuation">(</span><span class="token constant">MYSQL_PORT</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token punctuation">.</span><span class="token keyword">with</span><span class="token punctuation">(</span><span class="token string">"database.user"</span><span class="token punctuation">,</span> <span class="token string">"debezium"</span><span class="token punctuation">)</span>
        <span class="token punctuation">.</span><span class="token keyword">with</span><span class="token punctuation">(</span><span class="token string">"database.password"</span><span class="token punctuation">,</span> <span class="token string">"dbz"</span><span class="token punctuation">)</span>
        <span class="token punctuation">.</span><span class="token keyword">with</span><span class="token punctuation">(</span><span class="token string">"database.server.id"</span><span class="token punctuation">,</span> <span class="token string">"184054"</span><span class="token punctuation">)</span>
        <span class="token punctuation">.</span><span class="token keyword">with</span><span class="token punctuation">(</span><span class="token string">"database.server.name"</span><span class="token punctuation">,</span> <span class="token string">"dbserver1"</span><span class="token punctuation">)</span>
        <span class="token punctuation">.</span><span class="token keyword">with</span><span class="token punctuation">(</span><span class="token string">"database.whitelist"</span><span class="token punctuation">,</span> <span class="token string">"inventory"</span><span class="token punctuation">)</span>
        <span class="token punctuation">.</span><span class="token keyword">with</span><span class="token punctuation">(</span><span class="token string">"database.history.hazelcast.list.name"</span><span class="token punctuation">,</span> <span class="token string">"test"</span><span class="token punctuation">)</span>
        <span class="token punctuation">.</span><span class="token function">build</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

<span class="token class-name">Pipeline</span> p <span class="token operator">=</span> <span class="token class-name">Pipeline</span><span class="token punctuation">.</span><span class="token function">create</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
p<span class="token punctuation">.</span><span class="token function">readFrom</span><span class="token punctuation">(</span><span class="token class-name">DebeziumSources</span><span class="token punctuation">.</span><span class="token function">cdc</span><span class="token punctuation">(</span>configuration<span class="token punctuation">)</span><span class="token punctuation">)</span>
 <span class="token punctuation">.</span><span class="token function">withoutTimestamps</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
 <span class="token punctuation">.</span><span class="token function">map</span><span class="token punctuation">(</span>record <span class="token operator">-></span> <span class="token class-name">Values</span><span class="token punctuation">.</span><span class="token function">convertToString</span><span class="token punctuation">(</span>record<span class="token punctuation">.</span><span class="token function">valueSchema</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> record<span class="token punctuation">.</span><span class="token function">value</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
 <span class="token punctuation">.</span><span class="token function">writeTo</span><span class="token punctuation">(</span><span class="token class-name">Sinks</span><span class="token punctuation">.</span><span class="token function">logger</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
</code></pre>
<p>The Debezium connector is currently available in the
<a href="https://github.com/hazelcast/hazelcast-jet-contrib/tree/master/debezium">hazelcast-jet-contrib repository</a>,
along with a <a href="https://github.com/hazelcast/hazelcast-jet-demos/tree/master/debezium-cdc-without-kafka">demo application</a>.</p>
<h3><a class="anchor" aria-hidden="true" id="kafka-connect"></a><a href="#kafka-connect" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Kafka Connect</h3>
<p>The <a href="https://github.com/hazelcast/hazelcast-jet-contrib/tree/master/kafka-connect">Kafka Connect source</a>
allows you to use any existing Kafka Connect source and use it natively
with Jet, without requiring presence of a Kafka Cluster. The records
will be streamed as Jet events instead, which can be processed further
and it has full support for fault-tolerance and replaying. A full list
of connectors can be viewed through <a href="https://www.confluent.io/hub/">Confluent Hub</a>.</p>
<h3><a class="anchor" aria-hidden="true" id="twitter"></a><a href="#twitter" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Twitter</h3>
<p>We've also released a simple <a href="https://github.com/hazelcast/hazelcast-jet-contrib/tree/master/twitter">Twitter source</a>
that uses the Twitter client, which can be used to process a stream of
Tweets.</p>
<pre><code class="hljs css language-java"><span class="token class-name">Properties</span> credentials <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">Properties</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
properties<span class="token punctuation">.</span><span class="token function">setProperty</span><span class="token punctuation">(</span><span class="token string">"consumerKey"</span><span class="token punctuation">,</span> <span class="token string">"???"</span><span class="token punctuation">)</span><span class="token punctuation">;</span> <span class="token comment">// OAuth1 Consumer Key</span>
properties<span class="token punctuation">.</span><span class="token function">setProperty</span><span class="token punctuation">(</span><span class="token string">"consumerSecret"</span><span class="token punctuation">,</span> <span class="token string">"???"</span><span class="token punctuation">)</span><span class="token punctuation">;</span> <span class="token comment">// OAuth1 Consumer Secret</span>
properties<span class="token punctuation">.</span><span class="token function">setProperty</span><span class="token punctuation">(</span><span class="token string">"token"</span><span class="token punctuation">,</span> <span class="token string">"???"</span><span class="token punctuation">)</span><span class="token punctuation">;</span> <span class="token comment">// OAuth1 Token</span>
properties<span class="token punctuation">.</span><span class="token function">setProperty</span><span class="token punctuation">(</span><span class="token string">"tokenSecret"</span><span class="token punctuation">,</span> <span class="token string">"???"</span><span class="token punctuation">)</span><span class="token punctuation">;</span> <span class="token comment">// OAuth1 Token Secret</span>
<span class="token class-name">List</span><span class="token generics"><span class="token punctuation">&lt;</span><span class="token class-name">String</span><span class="token punctuation">></span></span> terms <span class="token operator">=</span> <span class="token class-name">Arrays</span><span class="token punctuation">.</span><span class="token function">asList</span><span class="token punctuation">(</span><span class="token string">"term1"</span><span class="token punctuation">,</span> <span class="token string">"term2"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token class-name">StreamSource</span><span class="token generics"><span class="token punctuation">&lt;</span><span class="token class-name">String</span><span class="token punctuation">></span></span> streamSource <span class="token operator">=</span>
             <span class="token class-name">TwitterSources</span><span class="token punctuation">.</span><span class="token function">stream</span><span class="token punctuation">(</span>credentials<span class="token punctuation">,</span>
                     <span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">-></span> <span class="token keyword">new</span> <span class="token class-name">StatusesFilterEndpoint</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">trackTerms</span><span class="token punctuation">(</span>terms<span class="token punctuation">)</span>
             <span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token class-name">Pipeline</span> p <span class="token operator">=</span> <span class="token class-name">Pipeline</span><span class="token punctuation">.</span><span class="token function">create</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
p<span class="token punctuation">.</span><span class="token function">readFrom</span><span class="token punctuation">(</span>streamSource<span class="token punctuation">)</span>
 <span class="token punctuation">.</span><span class="token function">withoutTimestamps</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
 <span class="token punctuation">.</span><span class="token function">writeTo</span><span class="token punctuation">(</span><span class="token class-name">Sinks</span><span class="token punctuation">.</span><span class="token function">logger</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
</code></pre>
<p>These connectors are currently under incubation, and will be part of a
main release in the future.</p>
<h2><a class="anchor" aria-hidden="true" id="improved-jet-installation"></a><a href="#improved-jet-installation" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Improved Jet Installation</h2>
<p>We've also made many improvements to the Jet installation package. It
has been cleaned up to reduce the size, and now supports the following:</p>
<ul>
<li>Default config format is now YAML and many of the common options are
in the default configuration.</li>
<li>A rolling file logger which writes to the log folder is now the
default logger</li>
<li>Support for daemon mode through <code>jet-start -d</code> switch.</li>
<li>Improved readme and a new &quot;hello world&quot; application which can be
submitted right after installation.</li>
<li>Improved JDK9+ support, to avoid illegal import warnings.</li>
</ul>
<h2><a class="anchor" aria-hidden="true" id="hazelcast-40"></a><a href="#hazelcast-40" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Hazelcast 4.0</h2>
<p>Another major change that's worth noting is that Jet is now based on
Hazelcast 4.0 - which in itself was a major release and brought many new
features and technical improvements such as improved performance and
Intel Optane DC Support and encryption at rest.</p>
<h2><a class="anchor" aria-hidden="true" id="breaking-changes-and-migration-guide"></a><a href="#breaking-changes-and-migration-guide" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Breaking Changes and Migration Guide</h2>
<p>As part of 4.0, we've also done some house cleaning and as a result some
things have been moved around. All the changes are listed as part of the
<a href="/blog/2020/04/01/upgrading-to-jet-40">migration guide blog post</a>.</p>
<p>We are committed to backwards compatibility going forward and any
interfaces or classes which are subject to change will be marked as
<code>@Beta</code> or <code>@EvolvingApi</code> going forwards.</p>
<h2><a class="anchor" aria-hidden="true" id="wrapping-up"></a><a href="#wrapping-up" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Wrapping Up</h2>
<p>This is a big release for Hazelcast Jet, and we have many more exciting
features in the pipeline (pun intended), including SQL support, extended
support for 2PC, improved Serialization support, even more connectors,
Kubernetes Operators and many more. We will also be aiming to make
shorter, more frequent releases to bring new features to users quicker.</p>
</span></div></article></div><div class="post"><header class="postHeader"><h1 class="postHeaderTitle"><a href="/blog/2020/02/20/transactional-processors">Transactional connectors in Hazelcast Jet</a></h1><p class="post-meta">February 20, 2020</p><div class="authorBlock"><p class="post-authorName"><a target="_blank" rel="noreferrer noopener">Viliam Ďurina</a></p><div class="authorPhoto"><a target="_blank" rel="noreferrer noopener"><img src="https://en.gravatar.com/userimage/154381144/a68feb9e86a976869d646e7cf7669510.jpg" alt="Viliam Ďurina"/></a></div></div></header><article class="post-content"><div><span><p><img src="/blog/assets/2020-02-20-transactional-processors-featured-img.png" alt="Transaction Processors Featured
Image"></p>
<p>Hazelcast Jet is a distributed stream processing engine which supports
exactly-once semantics even in the presence of cluster member failures.
This is achieved by snapshotting the internal state of the processors at
regular intervals into a reliable storage and then, in case of a
failure, using the latest snapshot to restore the state and continue.</p>
<p>However, the exactly-once guarantee didn't work with most of the
connectors. Only <a href="/docs/architecture/fault-tolerance">replayable
sources</a>,
such as Apache Kafka or IMap Journal were supported. And no sink
supported this level of guarantee. Why was that?</p>
<p>The original snapshot API had only one phase. A processor was asked to
save its state at regular intervals and that was it. But a sink writes
items to some external resource and must commit if the snapshot was
successful; and it must not commit if it wasn't. It also needs to ensure
that if some processor committed, all will commit, even in the presence
of failures. This is where distributed transactions come to the rescue.</p>
<h2><a class="anchor" aria-hidden="true" id="distributed-transactions"></a><a href="#distributed-transactions" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Distributed transactions</h2>
<p>Jet uses the two-phase commit algorithm to coordinate individual
transactions. The basic algorithm is simple:</p>
<ol>
<li><p>The coordinator asks all participants to prepare for commit</p></li>
<li><p>If all participants were successful, the coordinator asks them to
commit. Otherwise it asks all of them to roll back</p></li>
</ol>
<p>For correct functionality it is required that if a participant reported
success in the first phase, it must be able to commit when requested.</p>
<p>Jet acts as a transaction coordinator. Individual processors (that is
the parallel workers doing the writes) are adapters to actual
transactional resources, that is to databases, message queues etc. So
even if you have just one transactional connector in your pipeline, you
have multiple participants of a distributed transaction, one on each
cluster member.</p>
<h2><a class="anchor" aria-hidden="true" id="two-phase-snapshot-procedure"></a><a href="#two-phase-snapshot-procedure" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Two-phase snapshot procedure</h2>
<p>The commit procedure in Jet is tied to the life cycle of the snapshot.
When a snapshot is taken, the previous transaction is committed and a
new one is started. The snapshot also serves as the durable storage for
the coordinator.</p>
<p>Since Jet 4.0, the snapshot has two phases. In the first phase the
participants prepare, in the second phase they commit. Important thing
is that the snapshot is successful and can be used to restore the state
of a job after the 1st phase is successful. If the job fails before
executing the 2nd phase, that is without executing the commits, the
processors must be able to commit the transactions after the job
restart. To do so, they store transaction IDs to the snapshot. This is
the basic process:</p>
<ol>
<li><p>When a processor starts, it opens transaction <code>T0</code>. It writes
incoming items, but doesn't commit.</p></li>
<li><p>Later the processor is asked to do the 1st phase of the snapshot (the
<code>snapshotCommitPrepare()</code> method). The processor prepares <code>T0</code>,
stores its ID to the snapshot and starts <code>T1</code>.</p></li>
<li><p>Items that arrive until the 2nd phase occurs are handled using <code>T1</code>.</p></li>
<li><p>When a coordinator member receives responses from all processors that
they successfully did 1st phase, it marks the snapshot as successful
and initiates the phase-2.</p></li>
<li><p>Some time later the processor is asked to do the 2nd phase (the
<code>snapshotCommitFinish()</code> method). The processor now commits <code>T0</code> and
continues to use <code>T1</code> until the next snapshot.</p></li>
<li><p>The process repeats with incremented transaction ID.</p></li>
</ol>
<p>Keep in mind that a failure can occur at or between any of the above
steps and exactly-once guarantee must be preserved. If it occurs before
step 2, the transaction is just rolled back by the remote system when
the client disconnects.</p>
<p>If it occurs between steps 2-4, items in <code>T1</code> are are rolled back by the
remote system because the transaction wasn't prepared (the XA API
requires this). But there's also <code>T0</code> that is prepared, but not
committed. After the job restarts, it will restore from a previous
snapshot (step 4 wasn't yet executed), and since <code>T0</code> isn't found in the
restored state, it will be rolled back.</p>
<p>If the failure occurs after step 4, then after the job restarts, it will
try to commit all transaction IDs found in the restored state. So it
will try to commit <code>T0</code>. The commit must be idempotent: if that
transaction was already committed, it should do nothing, because we
don't know if the step 5 was executed or not.</p>
<h2><a class="anchor" aria-hidden="true" id="consistency-with-internal-state"></a><a href="#consistency-with-internal-state" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Consistency with internal state</h2>
<p>The 1st phase is common for transactional processors and for processors
that only save internal state. It is coordinated using the snapshot
barrier, based on the <a href="/docs/architecture/fault-tolerance#distributed-snapshot">Chandy-Lamport
algorithm</a>.
The consequence is that the moment at which internal processors save
their state and external processors prepare and switch their
transactions is the same. Therefore you can combine exactly-once stages
of any type in the pipeline and it will work seamlessly.</p>
<h2><a class="anchor" aria-hidden="true" id="transactions-are-needed-for-sources-too"></a><a href="#transactions-are-needed-for-sources-too" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Transactions are needed for sources too</h2>
<p>It might seem that since sources are designed to be read, we don’t need
anything to store. But, for example, some message systems use
acknowledgements, which are in fact writes: they change the state of the
message to consumed or they delete the message.</p>
<p>Jet supports JMS as a source. We’ve initially implemented the JMS source
using XA transactions, but it turned out that major brokers don’t
support it or the support is buggy. For example, ActiveMQ only delivers
a handful of messages to consumers and then stops
(<a href="https://issues.apache.org/jira/projects/AMQ/issues/AMQ-7369">issue</a>).
Artemis sometimes loses messages
(<a href="https://issues.apache.org/jira/projects/ARTEMIS/issues/ARTEMIS-2546">issue</a>).
RabbitMQ doesn't support two-phase transactions at all.</p>
<p>Therefore for JMS source we implemented a different strategy. We
acknowledge consumption in the 2nd phase of the snapshot. But if the job
fails after the snapshot is successful but before we manage to
acknowledge, already processed messages could be redelivered, so we
store the IDs of seen messages in the snapshot and then use that to
deduplicate. If you’re interested in details, check the <a href="https://github.com/hazelcast/hazelcast-jet/blob/master/hazelcast-jet-core/src/main/java/com/hazelcast/jet/impl/connector/StreamJmsP.java">source
code</a>.</p>
<h2><a class="anchor" aria-hidden="true" id="real-life-issues"></a><a href="#real-life-issues" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Real-life issues</h2>
<p>As mentioned above, some brokers have incorrect or buggy XA
implementation. In other cases, prepared transactions are rolled back
when the client disconnects (for example in
<a href="https://jira.mariadb.org/browse/MDEV-742">MariaDB</a> or <a href="https://github.com/h2database/h2database/issues/2347">H2
Database</a>) - these
systems are not usable at all. On the contrary, other implementations
keep even non-prepared transactions, such as Artemis
(<a href="https://issues.apache.org/jira/browse/ARTEMIS-2559">issue</a>, fixed
recently). Artemis doesn't even return these transactions when calling
<code>recover()</code>, the XA API method to list prepared transactions, but those
transactions still exist and hold locks. Transaction interleaving is
mostly also not supported, this prevents us from doing any work while
waiting for the 2nd phase.</p>
<p>Apache Kafka, while having all the building blocks needed to implement
XA standard, has its own API. It also lacks a method to commit a
transaction after reconnection, but we’ve been able to do it by calling
a few <a href="https://github.com/hazelcast/hazelcast-jet/blob/master/extensions/kafka/src/main/java/com/hazelcast/jet/kafka/impl/ResumeTransactionUtil.java#L43-L64">private
methods</a>.
Also it binds transaction ID to the connection which forces us to have
multiple open connections.</p>
<h2><a class="anchor" aria-hidden="true" id="transaction-id-pool"></a><a href="#transaction-id-pool" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Transaction ID pool</h2>
<p>Due to the above real-life limitations in most connectors we use two
transaction IDs interchangeably per processor. This avoids the need for
the <code>recover()</code> method to list prepared transactions, which is
unreliable or missing. Instead, we just probe known transaction IDs for
existence.</p>
<p>This tactic also avoids the problem with Apache Kafka that it binds the
transaction ID to a connection: we keep a pool of 2 connections in each
processor instead and we don't have to open a new connection after each
snapshot.</p>
<p>All connectors except for the file sink use this approach, including the
JMS and JDBC sinks
<a href="https://github.com/hazelcast/hazelcast-jet/pull/1813">planned</a> for 4.1.</p>
<h2><a class="anchor" aria-hidden="true" id="conclusion"></a><a href="#conclusion" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Conclusion</h2>
<p>The new feature allowed us to implement exactly-once guarantee for
sources and sinks where it previously wasn't possible. Even though these
kinds of connectors are not ideal for a distributed system because they
generally are not distributed, they still are very useful for
integration with existing systems. JMS source, Kafka sink and file sink
are available out-of-the-box in Jet 4.0.</p>
<p>If you consider writing your own exactly-once connector, currently you
have to implement the Core API <code>Processor</code> class. We consider
introducing some higher-level API in the future.</p>
</span></div></article></div><div class="docs-prevnext"><a class="docs-prev" href="/blog/">← Prev</a><a class="docs-next" href="/blog/page3/">Next →</a></div></div></div></div></div><footer class="nav-footer" id="footer"><section class="sitemap"><div style="text-align:left"><a href="/" class="nav-home"><img src="/img/logo-light.svg" alt="Hazelcast Jet" width="200" height="40"/></a><div style="margin-left:12px"><a class="github-button" href="https://github.com/hazelcast/hazelcast-jet" data-icon="octicon-star" data-count-href="/facebook/docusaurus/stargazers" data-show-count="true" data-count-aria-label="# stargazers on GitHub" aria-label="Star this project on GitHub">Star On GitHub</a></div></div><div><h5>Docs</h5><a href="/docs/get-started/intro">Get Started</a><a href="/docs/concepts/dag">Concepts</a><a href="/docs/tutorials/kafka">Tutorials</a><a href="/docs/architecture/distributed-computing">Architecture</a><a href="/docs/operations/installation">Operations Guide</a><a href="/docs/enterprise">Enterprise Edition</a></div><div><h5>Community</h5><a href="https://groups.google.com/forum/#!forum/hazelcast-jet" target="_blank" rel="noreferrer noopener">Google Groups</a><a href="http://stackoverflow.com/questions/tagged/hazelcast-jet" target="_blank" rel="noreferrer noopener">Stack Overflow</a><a href="https://slack.hazelcast.com">Slack</a></div><div><h5>Latest From the Blog</h5><a href="/blog/2023/06/14/jet-engine-in-hazelcast">Jet engine lives on in Hazelcast 5.x</a><a href="/blog/2021/04/21/jet-45-is-released">Jet 4.5 Released</a><a href="/blog/2021/03/17/billion-events-per-second">Billion Events Per Second with Millisecond Latency: Streaming Analytics at Giga-Scale</a><a href="/blog/2021/02/03/jet-44-is-released">Jet 4.4 Released</a><a href="/blog/2020/10/23/jet-43-is-released">Jet 4.3 Released</a></div><div><h5>More</h5><a href="https://github.com/hazelcast/hazelcast-jet">GitHub Project</a><a href="http://hazelcast.com/company/careers/">Work at Hazelcast</a><a href="/license">License</a></div></section><section class="copyright">Copyright © 2023 Hazelcast Inc.</section></footer></div><script type="text/javascript" src="https://cdn.jsdelivr.net/docsearch.js/1/docsearch.min.js"></script><script>
                document.addEventListener('keyup', function(e) {
                  if (e.target !== document.body) {
                    return;
                  }
                  // keyCode for '/' (slash)
                  if (e.keyCode === 191) {
                    const search = document.getElementById('search_input_react');
                    search && search.focus();
                  }
                });
              </script><script>
              var search = docsearch({
                
                apiKey: '79d1e4941621b9fd761d279d4d19ed69',
                indexName: 'hazelcast-jet',
                inputSelector: '#search_input_react',
                algoliaOptions: {"facetFilters":["language:en","version:4.5.4"]}
              });
            </script></body></html>