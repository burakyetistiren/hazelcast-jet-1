<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta http-equiv="X-UA-Compatible" content="IE=edge"/><title>Processing 10M queries / second on a single node using Jet and gRPC · Hazelcast Jet</title><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta name="generator" content="Docusaurus"/><meta name="description" content="Implementing data processing pipelines occasionally requires calling an"/><meta name="docsearch:language" content="en"/><meta property="og:title" content="Processing 10M queries / second on a single node using Jet and gRPC · Hazelcast Jet"/><meta property="og:type" content="website"/><meta property="og:url" content="https://jet-start.sh/blog/2020/05/25/grcp"/><meta property="og:description" content="Implementing data processing pipelines occasionally requires calling an"/><meta property="og:image" content="https://jet-start.sh/img/Hazelcast-Jet-Logo-Blue_Dark.jpg"/><meta name="twitter:card" content="summary"/><meta name="twitter:image" content="https://jet-start.sh/img/Hazelcast-Jet-Logo-Blue_Dark.jpg"/><link rel="shortcut icon" href="/img/favicon.png"/><link rel="stylesheet" href="https://cdn.jsdelivr.net/docsearch.js/1/docsearch.min.css"/><link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css"/><link rel="alternate" type="application/atom+xml" href="https://jet-start.sh/blog/atom.xml" title="Hazelcast Jet Blog ATOM Feed"/><link rel="alternate" type="application/rss+xml" href="https://jet-start.sh/blog/feed.xml" title="Hazelcast Jet Blog RSS Feed"/><script>
              (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
              (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
              m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
              })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

              ga('create', 'UA-158279495-1', 'auto');
              ga('send', 'pageview');
            </script><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Ubuntu:300,400,500,600"/><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:300,400,500,600,700,800"/><script type="text/javascript" src="https://buttons.github.io/buttons.js"></script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.0/clipboard.min.js"></script><script type="text/javascript" src="/js/code-block-buttons.js"></script><script type="text/javascript" src="https://plausible.io/js/plausible.js" async="" defer="" data-domain="jet-start.sh"></script><script src="/js/scrollSpy.js"></script><link rel="stylesheet" href="/css/prism.css"/><link rel="stylesheet" href="/css/main.css"/><script src="/js/codetabs.js"></script></head><body class="sideNavVisible separateOnPageNav"><div class="fixedHeaderContainer"><div class="headerWrapper wrapper"><header><a href="/"><img class="logo" src="/img/logo-dark.svg" alt="Hazelcast Jet"/></a><a href="/versions"><h3>4.5.4</h3></a><div class="navigationWrapper navigationSlider"><nav class="slidingNav"><ul class="nav-site nav-site-internal"><li class=""><a href="/docs/get-started/intro" target="_self">Docs</a></li><li class=""><a href="/download" target="_self">Download</a></li><li class=""><a href="/demos" target="_self">Demos</a></li><li class=""><a href="https://github.com/hazelcast/hazelcast-jet" target="_self">GitHub</a></li><li class=""><a href="https://slack.hazelcast.com/" target="_self">Community</a></li><li class="siteNavGroupActive"><a href="/blog/" target="_self">Blog</a></li><li class="navSearchWrapper reactNavSearchWrapper"><input type="text" id="search_input_react" placeholder="Search" title="Search"/></li></ul></nav></div></header></div></div><div class="navPusher"><div class="docMainWrapper wrapper"><div class="docsNavContainer" id="docsNav"><nav class="toc"><div class="toggleNav"><section class="navWrapper wrapper"><div class="navBreadcrumb wrapper"><div class="navToggle" id="navToggler"><div class="hamburger-menu"><div class="line1"></div><div class="line2"></div><div class="line3"></div></div></div><h2><i>›</i><span>All posts</span></h2><div class="tocToggler" id="tocToggler"><i class="icon-toc"></i></div></div><div class="navGroups"><div class="navGroup"><h3 class="navGroupCategoryTitle">All posts</h3><ul class=""><li class="navListItem"><a class="navItem" href="/blog/2023/06/14/jet-engine-in-hazelcast">Jet engine lives on in Hazelcast 5.x</a></li><li class="navListItem"><a class="navItem" href="/blog/2021/04/21/jet-45-is-released">Jet 4.5 Released</a></li><li class="navListItem"><a class="navItem" href="/blog/2021/03/17/billion-events-per-second">Billion Events Per Second with Millisecond Latency: Streaming Analytics at Giga-Scale</a></li><li class="navListItem"><a class="navItem" href="/blog/2021/02/03/jet-44-is-released">Jet 4.4 Released</a></li><li class="navListItem"><a class="navItem" href="/blog/2020/10/23/jet-43-is-released">Jet 4.3 Released</a></li><li class="navListItem"><a class="navItem" href="/blog/2020/10/06/enabling-full-text-search">Enabling Full-text Search with Change Data Capture in a Legacy Application</a></li><li class="navListItem"><a class="navItem" href="/blog/2020/09/18/cdc-meets-stream-processing">Change Data Capture meets Stream Processing</a></li><li class="navListItem"><a class="navItem" href="/blog/2020/08/05/gc-tuning-for-jet">Sub-10 ms Latency in Java: Concurrent GC with Green Threads</a></li><li class="navListItem"><a class="navItem" href="/blog/2020/07/16/designing-evergreen-cache-cdc">Designing an Evergreen Cache with Change Data Capture</a></li><li class="navListItem"><a class="navItem" href="/blog/2020/07/14/jet-42-is-released">Jet 4.2 is Released</a></li><li class="navListItem"><a class="navItem" href="/blog/2020/06/23/jdk-gc-benchmarks-rematch">Performance of Modern Java on Data-Heavy Workloads: The Low-Latency Rematch</a></li><li class="navListItem"><a class="navItem" href="/blog/2020/06/09/jdk-gc-benchmarks-part2">Performance of Modern Java on Data-Heavy Workloads: Batch Processing</a></li><li class="navListItem"><a class="navItem" href="/blog/2020/06/09/jdk-gc-benchmarks-part1">Performance of Modern Java on Data-Heavy Workloads: Real-Time Streaming</a></li><li class="navListItem navListItemActive"><a class="navItem" href="/blog/2020/05/25/grcp">Processing 10M queries / second on a single node using Jet and gRPC</a></li><li class="navListItem"><a class="navItem" href="/blog/2020/05/18/spark-jet">How Hazelcast Jet Compares to Apache Spark</a></li><li class="navListItem"><a class="navItem" href="/blog/2020/04/29/jet-41-is-released">Jet 4.1 is Released</a></li><li class="navListItem"><a class="navItem" href="/blog/2020/04/01/upgrading-to-jet-40">Upgrading to Jet 4.0</a></li><li class="navListItem"><a class="navItem" href="/blog/2020/03/30/ml-inference">Machine Learning Inference at Scale</a></li><li class="navListItem"><a class="navItem" href="/blog/2020/03/02/jet-40-is-released">Jet 4.0 is Released</a></li><li class="navListItem"><a class="navItem" href="/blog/2020/02/20/transactional-processors">Transactional connectors in Hazelcast Jet</a></li><li class="navListItem"><a class="navItem" href="/blog/2020/01/28/new-website">Announcing New Documentation Website</a></li><li class="navListItem"><a class="navItem" href="/blog/2019/11/12/stream-deduplication">Stream Deduplication with Hazelcast Jet</a></li></ul></div></div></section></div><script>
            var coll = document.getElementsByClassName('collapsible');
            var checkActiveCategory = true;
            for (var i = 0; i < coll.length; i++) {
              var links = coll[i].nextElementSibling.getElementsByTagName('*');
              if (checkActiveCategory){
                for (var j = 0; j < links.length; j++) {
                  if (links[j].classList.contains('navListItemActive')){
                    coll[i].nextElementSibling.classList.toggle('hide');
                    coll[i].childNodes[1].classList.toggle('rotate');
                    checkActiveCategory = false;
                    break;
                  }
                }
              }

              coll[i].addEventListener('click', function() {
                var arrow = this.childNodes[1];
                arrow.classList.toggle('rotate');
                var content = this.nextElementSibling;
                content.classList.toggle('hide');
              });
            }

            document.addEventListener('DOMContentLoaded', function() {
              createToggler('#navToggler', '#docsNav', 'docsSliderActive');
              createToggler('#tocToggler', 'body', 'tocActive');

              var headings = document.querySelector('.toc-headings');
              headings && headings.addEventListener('click', function(event) {
                var el = event.target;
                while(el !== headings){
                  if (el.tagName === 'A') {
                    document.body.classList.remove('tocActive');
                    break;
                  } else{
                    el = el.parentNode;
                  }
                }
              }, false);

              function createToggler(togglerSelector, targetSelector, className) {
                var toggler = document.querySelector(togglerSelector);
                var target = document.querySelector(targetSelector);

                if (!toggler) {
                  return;
                }

                toggler.onclick = function(event) {
                  event.preventDefault();

                  target.classList.toggle(className);
                };
              }
            });
        </script></nav></div><div class="container mainContainer postContainer blogContainer"><div class="wrapper"><div class="lonePost"><div class="post"><header class="postHeader"><h1 class="postHeaderTitle"><a href="/blog/2020/05/25/grcp">Processing 10M queries / second on a single node using Jet and gRPC</a></h1><p class="post-meta">May 25, 2020</p><div class="authorBlock"><p class="post-authorName"><a href="https://www.linkedin.com/in/frantisek-hartman/" target="_blank" rel="noreferrer noopener">František Hartman, Marko Topolnik</a></p><div class="authorPhoto"><a href="https://www.linkedin.com/in/frantisek-hartman/" target="_blank" rel="noreferrer noopener"><img src="https://i.stack.imgur.com/3X7wE.png" alt="František Hartman, Marko Topolnik"/></a></div></div></header><div><span><p>Implementing data processing pipelines occasionally requires calling an
external service, for example: predicting/classifying based on a ML
model, looking up records from a database or a full-text search engine,
and using a dedicated platform that computes financial risk exposure.</p>
<p>These are the typical reasons why you'd want to have some processing
done outside of the Hazelcast Jet infrastructure:</p>
<ul>
<li>The service already exists</li>
<li>The service is implemented by another team or in a different language</li>
<li>The deployment of the service needs to be independent of the
deployment of the pipeline (e.g. you need to update ML model without
modifying the pipeline)</li>
<li>Scaling the service independently of the processing pipeline</li>
</ul>
<h2><a class="anchor" aria-hidden="true" id="grpc"></a><a href="#grpc" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>gRPC</h2>
<p><a href="https://grpc.io/">gRPC</a> is an RPC system that trades a bit of
convenience for much better performance, and also includes first-class
support for several critical concerns. Quoting from their website:</p>
<blockquote>
<p>gRPC is a modern open source high performance RPC framework that can run
in any environment. It can efficiently connect services in and across
data centers with pluggable support for load balancing, tracing, health
checking and authentication.</p>
</blockquote>
<p>With gRPC you define the service endpoint and the messages using
Protocol Buffers (Protobuf) and its interface description language
(IDL). The tooling then generates code for the server, serialization and
deserialization of the messages, and the client. Hazelcast Jet's gRPC
module makes it convenient to use the generated code to call the
endpoint from a Jet pipeline.</p>
<p>The gRPC framework provides several RPC types, but most commonly used
are unary RPC and bidirectional streaming RPC. Unary RPC is what's
usually called just &quot;RPC&quot;: the client sends a request and receives a
response. With bidirectional streaming RPC, the client starts a single
request, writes any number of messages to the request stream. Then the
client receives any number of messages in the response stream, which
makes it more similar to a messaging system with 2 topics than RPC.</p>
<h2><a class="anchor" aria-hidden="true" id="using-grpc-from-a-jet-pipeline"></a><a href="#using-grpc-from-a-jet-pipeline" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Using gRPC from a Jet Pipeline</h2>
<p>With Hazelcast Jet 4.1 we have released first-class support for
accessing gRPC endpoints from Jet pipelines. In this post we investigate
the performance of the <a href="https://github.com/grpc/grpc-java">gRPC-Java</a>
framework and the effects of various settings on maximum throughput both
in vanilla gRPC and in a Jet environment.</p>
<p>Let’s start with an example of calling a gRPC endpoint from Jet.  For
example, given this protobuf definition:</p>
<pre><code class="hljs css language-proto"><span class="hljs-class"><span class="hljs-keyword">service</span> <span class="hljs-title">Greeter</span> </span>{
  <span class="hljs-comment">// Sends a greeting rpc SayHello</span>
  (HelloRequest) returns (HelloReply) {}
}
</code></pre>
<p>We can create the following service factory:</p>
<pre><code class="hljs css language-java"><span class="token keyword">var</span> greeterService <span class="token operator">=</span> <span class="token function">unaryService</span><span class="token punctuation">(</span> <span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">-></span>
  <span class="token class-name">ManagedChannelBuilder</span><span class="token punctuation">.</span><span class="token function">forAddress</span><span class="token punctuation">(</span><span class="token string">"localhost"</span><span class="token punctuation">,</span> <span class="token number">5000</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">usePlaintext</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
  channel <span class="token operator">-></span> <span class="token class-name">GreeterGrpc</span><span class="token punctuation">.</span><span class="token function">newStub</span><span class="token punctuation">(</span>channel<span class="token punctuation">)</span><span class="token operator">::</span><span class="token function">sayHello</span>
<span class="token punctuation">)</span><span class="token punctuation">;</span>
</code></pre>
<p>The first lambda returns a client channel builder. Jet uses the builder
you provide to create gRPC channels, which correspond to network
connections. You can set any desired configuration options on the
builder, such as compression and encryption. These will turn out to be
relevant to our investigation.</p>
<p>The second lambda takes a gRPC client channel obtained from the builder
and returns a function which Jet will call for each item of the
pipeline. The class <code>GreeterGrpc</code> is auto-generated by gRPC and its
<code>newStub</code> method creates the client stub. You can change the default
settings before returning it from the lambda. We won’t modify this
further in our investigation.</p>
<p>Once you have constructed a <code>ServiceFactory</code>, you can pass it to a Jet
pipeline stage transform such as <code>mapUsingServiceAsync</code>. Jet uses the
factory to create instances of the service, and passes these to the
lambda you provide. Here's an example:</p>
<pre><code class="hljs css language-java"><span class="token class-name">BatchStage</span><span class="token generics"><span class="token punctuation">&lt;</span><span class="token class-name">Integer</span><span class="token punctuation">></span></span> stage <span class="token operator">=</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>
stage<span class="token punctuation">.</span><span class="token function">mapUsingServiceAsync</span><span class="token punctuation">(</span>greeterServiceFactory<span class="token punctuation">,</span> <span class="token punctuation">(</span>service<span class="token punctuation">,</span> item<span class="token punctuation">)</span> <span class="token operator">-></span>
    service<span class="token punctuation">.</span><span class="token function">call</span><span class="token punctuation">(</span><span class="token class-name">HelloRequest</span><span class="token punctuation">.</span><span class="token function">newBuilder</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">setValue</span><span class="token punctuation">(</span>item<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">build</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre>
<p>The word <code>Async</code> in the stage transform name means that the lambda
function must return a <code>CompletableFuture&lt;T&gt;</code>, and in our case
<code>service.call()</code> returns just that.</p>
<h2><a class="anchor" aria-hidden="true" id="the-environment"></a><a href="#the-environment" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>The environment</h2>
<p>We ran the benchmarks in AWS on
2 instances of type c5.9xlarge. This instance type has:</p>
<ul>
<li>36 vCPUs</li>
<li>76 GiB of RAM</li>
<li>10 Gigabit network.</li>
</ul>
<p>One instance ran a single Hazelcast Jet
member, the other one ran the gRPC server.</p>
<h2><a class="anchor" aria-hidden="true" id="the-grpc-benchmarks"></a><a href="#the-grpc-benchmarks" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>The gRPC Benchmarks</h2>
<p>The gRPC-Java framework provides its own set of
<a href="https://github.com/grpc/grpc-java/tree/master/benchmarks">benchmarks</a>.
We will use these to establish a baseline to compare with the
performance of our Jet pipeline. The benchmark consists of running a
gRPC server and a client.</p>
<p>We issued this command to start the server:</p>
<pre><code class="hljs css language-bash">./grpc-benchmarks/bin/qps_server --address=<span class="hljs-variable">$SERVER</span> --transport=netty_epoll
</code></pre>
<p>And to run the client:</p>
<pre><code class="hljs css language-bash">grpc-benchmarks/bin/qps_client --address=<span class="hljs-variable">$SERVER</span> --transport=netty_epoll \
  --channels=36 --outstanding_rpcs=64 --client_payload=8 --server_payload=8
</code></pre>
<p>The parameters relevant to our investigation are:</p>
<ul>
<li>--transport - possible values are netty_epoll and netty_nio,
netty_epoll should be most performant, but it is only available on
Linux</li>
<li>--channels - number of channels (i.e., network connections) to create
on the client side</li>
<li>--outstanding_rpcs - how many requests per channel to submit without
waiting for a response</li>
<li>--client_payload and --server_payload - size of the payload sent in a
request/response</li>
<li>--streaming_rpcs - when present, uses a bidirectional streaming
endpoint, otherwise a unary endpoint</li>
</ul>
<p>Here's an example of of the output:</p>
<pre><code class="hljs css language-text">Channels:                          36
Outstanding RPCs per Channel:      64
Server Payload Size:                8
Client Payload Size:                8
50%ile Latency (in micros):      1951
90%ile Latency (in micros):      3695
95%ile Latency (in micros):      5183
99%ile Latency (in micros):     10559
99.9%ile Latency (in micros):   15807
Maximum Latency (in micros):    37375
QPS:                           986128
</code></pre>
<p>Our focus for this benchmark was throughput, so we focused on the QPS
metric - Queries Per Second. The results are quite interesting: we could
clearly identify the number of channels as the most important factor.
Using 72-108 channels increased the throughput by 20x-30x, compared to
the baseline of one channel. Since we're using very small messages,
per-message overhead inside the gRPC layer dominates over the network
limits. This is why you can achieve more throughput by adding more
channels.</p>
<p><img src="/blog/assets/2020-05-25-grpc-channels.png" alt="Througput for channels, at 128 outstanding RPCs"></p>
<p>The number of outstanding RPCs seems to have a sweet spot with a value
of 128 or 256 for this particular configuration of instance type,
network and benchmark parameters. With very low message processing time,
the optimal number of outstanding RPCs is mostly dictated by the number
of in-flight messages in the network layer. To a first approximation,
this number equals network throughput in terms of messages per second,
multiplied by the roundtrip latency of one message. Our messages are
very small, so the optimal number of outstanding RPCs is quite high.</p>
<p>As for the transport epoll and NIO transport types - it seems that for
unary calls they both reach similar maximum performance, but overall,
NIO is better.  For streaming calls, epoll wins in maximum performance
with similar results using 108 channels and 128 outstanding RPCs or 72
channels and 256 outstanding RPCs. With other settings there doesn’t
seem to be a clear winner.</p>
<p><img src="/blog/assets/2020-05-25-grpc-epoll_nio.png" alt="netty_epoll vs netty_nio"></p>
<p>All charts present best results for various configurations of number of
channels, outstanding RPC etc. The full data is available in this
<a href="https://docs.google.com/spreadsheets/d/1psjHF5ZRlxYAwxn4LA_XhvYKB0KuLXMHW8iEjrUAteE/edit#gid=63601685">spreadsheet</a>
.</p>
<h2><a class="anchor" aria-hidden="true" id="jet-benchmarks"></a><a href="#jet-benchmarks" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Jet Benchmarks</h2>
<p>We created 2 types of workloads on the gRPC server side:</p>
<ul>
<li>Very fast computation (integer multiplication)</li>
<li>CPU-bound task taking 10 ms</li>
</ul>
<p>We ran each workload as:</p>
<ul>
<li>Unary RPC</li>
<li>Bidirectional streaming RPC</li>
</ul>
<p>In the Jet pipeline we don’t have the exact same parameters as in the
gRPC benchmark, but there are similar ones:</p>
<ul>
<li>Local parallelism of the <code>mapUsingServiceAsync</code> step dictates the
number of mapping processors, each processor has its own channel
instance so this is roughly equivalent to the number of channels.</li>
<li>The parameter <code>maxConcurrentOps</code> specifies how many concurrent
asynchronous mapping operations each Jet processor can issue without
waiting for a response. This gives you control over the same aspect as
the number of outstanding RPCs, but in a less direct way.</li>
</ul>
<p>This is the pipeline for a unary service, all the other benchmark
pipelines follow the same pattern:</p>
<pre><code class="hljs css language-java"><span class="token class-name">Pipeline</span> p <span class="token operator">=</span> <span class="token class-name">Pipeline</span><span class="token punctuation">.</span><span class="token function">create</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token class-name">BatchStage</span><span class="token generics"><span class="token punctuation">&lt;</span><span class="token class-name">Integer</span><span class="token punctuation">></span></span> stage <span class="token operator">=</span> p<span class="token punctuation">.</span><span class="token function">readFrom</span><span class="token punctuation">(</span><span class="token function">intSource</span><span class="token punctuation">(</span>jobBatchSize<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
stage<span class="token punctuation">.</span><span class="token function">mapUsingServiceAsync</span><span class="token punctuation">(</span>unaryService<span class="token punctuation">,</span>
       maxConcurrentOps<span class="token punctuation">,</span> <span class="token boolean">true</span><span class="token punctuation">,</span>
       <span class="token punctuation">(</span>service<span class="token punctuation">,</span> item<span class="token punctuation">)</span> <span class="token operator">-></span> service<span class="token punctuation">.</span><span class="token function">call</span><span class="token punctuation">(</span><span class="token class-name">HelloRequest</span><span class="token punctuation">.</span><span class="token function">newBuilder</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">setValue</span><span class="token punctuation">(</span>item<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">build</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token punctuation">.</span><span class="token function">setLocalParallelism</span><span class="token punctuation">(</span>localParallelism<span class="token punctuation">)</span>
    <span class="token punctuation">.</span><span class="token function">aggregate</span><span class="token punctuation">(</span><span class="token class-name">AggregateOperations</span><span class="token punctuation">.</span><span class="token function">counting</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token punctuation">.</span><span class="token function">writeTo</span><span class="token punctuation">(</span><span class="token class-name">Sinks</span><span class="token punctuation">.</span><span class="token function">observable</span><span class="token punctuation">(</span>runId<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
</code></pre>
<h2><a class="anchor" aria-hidden="true" id="fast-computation-benchmark"></a><a href="#fast-computation-benchmark" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Fast computation benchmark</h2>
<p>This benchmark is very similar to the benchmark from gRPC repo. It is
doing minimal work on the gRPC server side and measures overhead of the
Jet pipeline and network.</p>
<p>First let’s compare this with the gRPC benchmark. We achieve similar
(slightly lower) results for unary RPC, and about half for bidirectional
streaming. It’s good that we don’t get e.g. 10x less, but the drop for
bidirectional streaming is rather surprising.</p>
<p>Best throughput results achieved in gRPC benchmark and Jet benchmark:</p>
<p><img src="/blog/assets/2020-05-18-grpc-vs-jet.png" alt="gRPC benchmark and Jet pipeline"></p>
<p>Similar to the gRPC benchmark, we can observe that to achieve maximum
throughput we need to increase the number of channels and the
maxConcurrentOps parameter.</p>
<p><img src="/blog/assets/2020-05-25-fast_unary_bidi.png" alt="unary vs bidirectional"></p>
<h2><a class="anchor" aria-hidden="true" id="batch-endpoint"></a><a href="#batch-endpoint" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Batch endpoint</h2>
<p>We also tried each RPC with a modified message type containing a batch
of messages in combination with <code>mapUsingServiceAsynchBatched</code>. This is
not always possible to do - you need to be able to change the interface
of the server, but if you can, it provides a huge boost to the
throughput.</p>
<p>Jet uses what has been variously called natural or <a href="https://mechanical-sympathy.blogspot.com/2011/10/smart-batching.html">smart
batching</a>.
Typically, a batching algorithm creates batches of fixed size or waits
for a fixed time to create a batch. This increases latency in
low-throughput scenarios. Smart batching instead creates a batch from
whatever items came in while the previous batch was being processed.
When the traffic is low, this results in small batches (including single
items), preserving the best possible latency, and as the traffic grows,
larger batches automatically form, up to the limit set by the
maxBatchSize parameter.</p>
<p>After modifying the endpoint to work with batches, our results improved
dramatically:</p>
<ol>
<li>Smart batching increased the throughput by roughly 7x.</li>
<li>We needed less channels (i.e., less system resources) to reach the
maximum throughput.</li>
<li>Even though bidirectional streaming remained a winner, its advantage
over unary became very slim.</li>
</ol>
<p>Note that for this benchmark there is no maxConcurrentOps setting,
because <code>mapUsingServiceAsyncBatched</code> doesn't have it as a configurable
option.</p>
<p><img src="/blog/assets/2020-05-25-fast_unary_bidirectional_batching.png" alt="unary vs bidirectional with batching"></p>
<h2><a class="anchor" aria-hidden="true" id="cpu-bound-task-of-10-ms"></a><a href="#cpu-bound-task-of-10-ms" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>CPU bound task of 10 ms</h2>
<p>This benchmark simulates a task that takes 10 ms to complete and is
CPU-bound.  Because our testing machine has 36 vCPUs, the theoretical
maximum throughput is 3600 requests/s.</p>
<p>Interestingly, we can see that unary RPC reaches the maximum possible
throughput for all settings, however bidirectional only for 36 channels.
The results don’t change significantly for different maxConcurrentOps
settings.</p>
<p><img src="/blog/assets/2020-05-25-10ms_unary_bidirectional.png" alt="10ms task unary vs bidirectional"></p>
<p>In this scenario, smart batching no longer has a significant edge. Unary
only gets close to a maximum with 72 channels. Bidirectional achieves
maximum throughput with both 36 and 72 channels.</p>
<p><img src="/blog/assets/2020-05-25-10ms_unary_bidirectional_batching.png" alt="10ms task unary vs bidirectional"></p>
<h2><a class="anchor" aria-hidden="true" id="conclusion"></a><a href="#conclusion" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Conclusion</h2>
<p>We can draw several conclusions from our investigation:</p>
<ul>
<li>There is no silver bullet, the results vary significantly with
workload type, network speed and latency, resources available (e.g. on
a local development machine the benchmark yields different results).</li>
<li>Bidirectional streaming is faster than unary, roughly 2x in our case.</li>
<li>For both unary and bidirectional streaming endpoints increasing the
number of channels provides better throughput, number of CPUs on the
gRPC server side might be a good starting point.</li>
<li>Batched endpoints make a big difference, allowing much higher
throughput with less channels.</li>
<li>The execution duration of the task also makes a significant
difference, especially when taking longer than the network roundtrip
(our 10 ms CPU bound task).</li>
</ul>
<p>So in any case you should test in your environment and your own workload
to find the most advantageous setup.</p>
<h2><a class="anchor" aria-hidden="true" id="links"></a><a href="#links" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Links</h2>
<p>For a full guide on how to use Jet's gRPC functionality and API details,
please see the <a href="/docs/how-tos/grpc">manual</a>.</p>
<p>The benchmark code is published on
<a href="https://github.com/frant-hartm/hazelcast-jet-grpc-benchmark">github.com</a></p>
<p>The spreadsheet with all the results can be seen
<a href="https://docs.google.com/spreadsheets/d/1psjHF5ZRlxYAwxn4LA_XhvYKB0KuLXMHW8iEjrUAteE/edit#gid">here</a>.</p>
</span></div></div><div class="blogSocialSection"></div></div><div class="blog-recent"><a class="button" href="/blog/">Recent posts</a></div></div></div><nav class="onPageNav"><ul class="toc-headings"><li><a href="#grpc">gRPC</a></li><li><a href="#using-grpc-from-a-jet-pipeline">Using gRPC from a Jet Pipeline</a></li><li><a href="#the-environment">The environment</a></li><li><a href="#the-grpc-benchmarks">The gRPC Benchmarks</a></li><li><a href="#jet-benchmarks">Jet Benchmarks</a></li><li><a href="#fast-computation-benchmark">Fast computation benchmark</a></li><li><a href="#batch-endpoint">Batch endpoint</a></li><li><a href="#cpu-bound-task-of-10-ms">CPU bound task of 10 ms</a></li><li><a href="#conclusion">Conclusion</a></li><li><a href="#links">Links</a></li></ul></nav></div><footer class="nav-footer" id="footer"><section class="sitemap"><div style="text-align:left"><a href="/" class="nav-home"><img src="/img/logo-light.svg" alt="Hazelcast Jet" width="200" height="40"/></a><div style="margin-left:12px"><a class="github-button" href="https://github.com/hazelcast/hazelcast-jet" data-icon="octicon-star" data-count-href="/facebook/docusaurus/stargazers" data-show-count="true" data-count-aria-label="# stargazers on GitHub" aria-label="Star this project on GitHub">Star On GitHub</a></div></div><div><h5>Docs</h5><a href="/docs/get-started/intro">Get Started</a><a href="/docs/concepts/dag">Concepts</a><a href="/docs/tutorials/kafka">Tutorials</a><a href="/docs/architecture/distributed-computing">Architecture</a><a href="/docs/operations/installation">Operations Guide</a><a href="/docs/enterprise">Enterprise Edition</a></div><div><h5>Community</h5><a href="https://groups.google.com/forum/#!forum/hazelcast-jet" target="_blank" rel="noreferrer noopener">Google Groups</a><a href="http://stackoverflow.com/questions/tagged/hazelcast-jet" target="_blank" rel="noreferrer noopener">Stack Overflow</a><a href="https://slack.hazelcast.com">Slack</a></div><div><h5>Latest From the Blog</h5><a href="/blog/2023/06/14/jet-engine-in-hazelcast">Jet engine lives on in Hazelcast 5.x</a><a href="/blog/2021/04/21/jet-45-is-released">Jet 4.5 Released</a><a href="/blog/2021/03/17/billion-events-per-second">Billion Events Per Second with Millisecond Latency: Streaming Analytics at Giga-Scale</a><a href="/blog/2021/02/03/jet-44-is-released">Jet 4.4 Released</a><a href="/blog/2020/10/23/jet-43-is-released">Jet 4.3 Released</a></div><div><h5>More</h5><a href="https://github.com/hazelcast/hazelcast-jet">GitHub Project</a><a href="http://hazelcast.com/company/careers/">Work at Hazelcast</a><a href="/license">License</a></div></section><section class="copyright">Copyright © 2023 Hazelcast Inc.</section></footer></div><script type="text/javascript" src="https://cdn.jsdelivr.net/docsearch.js/1/docsearch.min.js"></script><script>
                document.addEventListener('keyup', function(e) {
                  if (e.target !== document.body) {
                    return;
                  }
                  // keyCode for '/' (slash)
                  if (e.keyCode === 191) {
                    const search = document.getElementById('search_input_react');
                    search && search.focus();
                  }
                });
              </script><script>
              var search = docsearch({
                
                apiKey: '79d1e4941621b9fd761d279d4d19ed69',
                indexName: 'hazelcast-jet',
                inputSelector: '#search_input_react',
                algoliaOptions: {"facetFilters":["language:en","version:4.5.4"]}
              });
            </script></body></html>