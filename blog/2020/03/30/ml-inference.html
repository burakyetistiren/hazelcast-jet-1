<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta http-equiv="X-UA-Compatible" content="IE=edge"/><title>Machine Learning Inference at Scale · Hazelcast Jet</title><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta name="generator" content="Docusaurus"/><meta name="description" content="Machine learning projects can be split into two phases:"/><meta name="docsearch:language" content="en"/><meta property="og:title" content="Machine Learning Inference at Scale · Hazelcast Jet"/><meta property="og:type" content="website"/><meta property="og:url" content="https://jet-start.sh/blog/2020/03/30/ml-inference"/><meta property="og:description" content="Machine learning projects can be split into two phases:"/><meta property="og:image" content="https://jet-start.sh/img/Hazelcast-Jet-Logo-Blue_Dark.jpg"/><meta name="twitter:card" content="summary"/><meta name="twitter:image" content="https://jet-start.sh/img/Hazelcast-Jet-Logo-Blue_Dark.jpg"/><link rel="shortcut icon" href="/img/favicon.png"/><link rel="stylesheet" href="https://cdn.jsdelivr.net/docsearch.js/1/docsearch.min.css"/><link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css"/><link rel="alternate" type="application/atom+xml" href="https://jet-start.sh/blog/atom.xml" title="Hazelcast Jet Blog ATOM Feed"/><link rel="alternate" type="application/rss+xml" href="https://jet-start.sh/blog/feed.xml" title="Hazelcast Jet Blog RSS Feed"/><script>
              (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
              (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
              m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
              })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

              ga('create', 'UA-158279495-1', 'auto');
              ga('send', 'pageview');
            </script><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Ubuntu:300,400,500,600"/><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:300,400,500,600,700,800"/><script type="text/javascript" src="https://buttons.github.io/buttons.js"></script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.0/clipboard.min.js"></script><script type="text/javascript" src="/js/code-block-buttons.js"></script><script type="text/javascript" src="https://plausible.io/js/plausible.js" async="" defer="" data-domain="jet-start.sh"></script><script src="/js/scrollSpy.js"></script><link rel="stylesheet" href="/css/prism.css"/><link rel="stylesheet" href="/css/main.css"/><script src="/js/codetabs.js"></script></head><body class="sideNavVisible separateOnPageNav"><div class="fixedHeaderContainer"><div class="headerWrapper wrapper"><header><a href="/"><img class="logo" src="/img/logo-dark.svg" alt="Hazelcast Jet"/></a><a href="/versions"><h3>4.5.4</h3></a><div class="navigationWrapper navigationSlider"><nav class="slidingNav"><ul class="nav-site nav-site-internal"><li class=""><a href="/docs/get-started/intro" target="_self">Docs</a></li><li class=""><a href="/download" target="_self">Download</a></li><li class=""><a href="/demos" target="_self">Demos</a></li><li class=""><a href="https://github.com/hazelcast/hazelcast-jet" target="_self">GitHub</a></li><li class=""><a href="https://slack.hazelcast.com/" target="_self">Community</a></li><li class="siteNavGroupActive"><a href="/blog/" target="_self">Blog</a></li><li class="navSearchWrapper reactNavSearchWrapper"><input type="text" id="search_input_react" placeholder="Search" title="Search"/></li></ul></nav></div></header></div></div><div class="navPusher"><div class="docMainWrapper wrapper"><div class="docsNavContainer" id="docsNav"><nav class="toc"><div class="toggleNav"><section class="navWrapper wrapper"><div class="navBreadcrumb wrapper"><div class="navToggle" id="navToggler"><div class="hamburger-menu"><div class="line1"></div><div class="line2"></div><div class="line3"></div></div></div><h2><i>›</i><span>All posts</span></h2><div class="tocToggler" id="tocToggler"><i class="icon-toc"></i></div></div><div class="navGroups"><div class="navGroup"><h3 class="navGroupCategoryTitle">All posts</h3><ul class=""><li class="navListItem"><a class="navItem" href="/blog/2023/06/14/jet-engine-in-hazelcast">Jet engine lives on in Hazelcast 5.x</a></li><li class="navListItem"><a class="navItem" href="/blog/2021/04/21/jet-45-is-released">Jet 4.5 Released</a></li><li class="navListItem"><a class="navItem" href="/blog/2021/03/17/billion-events-per-second">Billion Events Per Second with Millisecond Latency: Streaming Analytics at Giga-Scale</a></li><li class="navListItem"><a class="navItem" href="/blog/2021/02/03/jet-44-is-released">Jet 4.4 Released</a></li><li class="navListItem"><a class="navItem" href="/blog/2020/10/23/jet-43-is-released">Jet 4.3 Released</a></li><li class="navListItem"><a class="navItem" href="/blog/2020/10/06/enabling-full-text-search">Enabling Full-text Search with Change Data Capture in a Legacy Application</a></li><li class="navListItem"><a class="navItem" href="/blog/2020/09/18/cdc-meets-stream-processing">Change Data Capture meets Stream Processing</a></li><li class="navListItem"><a class="navItem" href="/blog/2020/08/05/gc-tuning-for-jet">Sub-10 ms Latency in Java: Concurrent GC with Green Threads</a></li><li class="navListItem"><a class="navItem" href="/blog/2020/07/16/designing-evergreen-cache-cdc">Designing an Evergreen Cache with Change Data Capture</a></li><li class="navListItem"><a class="navItem" href="/blog/2020/07/14/jet-42-is-released">Jet 4.2 is Released</a></li><li class="navListItem"><a class="navItem" href="/blog/2020/06/23/jdk-gc-benchmarks-rematch">Performance of Modern Java on Data-Heavy Workloads: The Low-Latency Rematch</a></li><li class="navListItem"><a class="navItem" href="/blog/2020/06/09/jdk-gc-benchmarks-part2">Performance of Modern Java on Data-Heavy Workloads: Batch Processing</a></li><li class="navListItem"><a class="navItem" href="/blog/2020/06/09/jdk-gc-benchmarks-part1">Performance of Modern Java on Data-Heavy Workloads: Real-Time Streaming</a></li><li class="navListItem"><a class="navItem" href="/blog/2020/05/25/grcp">Processing 10M queries / second on a single node using Jet and gRPC</a></li><li class="navListItem"><a class="navItem" href="/blog/2020/05/18/spark-jet">How Hazelcast Jet Compares to Apache Spark</a></li><li class="navListItem"><a class="navItem" href="/blog/2020/04/29/jet-41-is-released">Jet 4.1 is Released</a></li><li class="navListItem"><a class="navItem" href="/blog/2020/04/01/upgrading-to-jet-40">Upgrading to Jet 4.0</a></li><li class="navListItem navListItemActive"><a class="navItem" href="/blog/2020/03/30/ml-inference">Machine Learning Inference at Scale</a></li><li class="navListItem"><a class="navItem" href="/blog/2020/03/02/jet-40-is-released">Jet 4.0 is Released</a></li><li class="navListItem"><a class="navItem" href="/blog/2020/02/20/transactional-processors">Transactional connectors in Hazelcast Jet</a></li><li class="navListItem"><a class="navItem" href="/blog/2020/01/28/new-website">Announcing New Documentation Website</a></li><li class="navListItem"><a class="navItem" href="/blog/2019/11/12/stream-deduplication">Stream Deduplication with Hazelcast Jet</a></li></ul></div></div></section></div><script>
            var coll = document.getElementsByClassName('collapsible');
            var checkActiveCategory = true;
            for (var i = 0; i < coll.length; i++) {
              var links = coll[i].nextElementSibling.getElementsByTagName('*');
              if (checkActiveCategory){
                for (var j = 0; j < links.length; j++) {
                  if (links[j].classList.contains('navListItemActive')){
                    coll[i].nextElementSibling.classList.toggle('hide');
                    coll[i].childNodes[1].classList.toggle('rotate');
                    checkActiveCategory = false;
                    break;
                  }
                }
              }

              coll[i].addEventListener('click', function() {
                var arrow = this.childNodes[1];
                arrow.classList.toggle('rotate');
                var content = this.nextElementSibling;
                content.classList.toggle('hide');
              });
            }

            document.addEventListener('DOMContentLoaded', function() {
              createToggler('#navToggler', '#docsNav', 'docsSliderActive');
              createToggler('#tocToggler', 'body', 'tocActive');

              var headings = document.querySelector('.toc-headings');
              headings && headings.addEventListener('click', function(event) {
                var el = event.target;
                while(el !== headings){
                  if (el.tagName === 'A') {
                    document.body.classList.remove('tocActive');
                    break;
                  } else{
                    el = el.parentNode;
                  }
                }
              }, false);

              function createToggler(togglerSelector, targetSelector, className) {
                var toggler = document.querySelector(togglerSelector);
                var target = document.querySelector(targetSelector);

                if (!toggler) {
                  return;
                }

                toggler.onclick = function(event) {
                  event.preventDefault();

                  target.classList.toggle(className);
                };
              }
            });
        </script></nav></div><div class="container mainContainer postContainer blogContainer"><div class="wrapper"><div class="lonePost"><div class="post"><header class="postHeader"><h1 class="postHeaderTitle"><a href="/blog/2020/03/30/ml-inference">Machine Learning Inference at Scale</a></h1><p class="post-meta">March 30, 2020</p><div class="authorBlock"><p class="post-authorName"><a href="https://twitter.com/voloda" target="_blank" rel="noreferrer noopener">Vladimir Schreiner</a></p><div class="authorPhoto"><a href="https://twitter.com/voloda" target="_blank" rel="noreferrer noopener"><img src="https://3l0wd94f0qdd10om8642z9se-wpengine.netdna-ssl.com/wp-content/uploads/2018/10/speaker-vladimir-schreiner-e1551380845855-170x170.jpg" alt="Vladimir Schreiner"/></a></div></div></header><div><span><p>Machine learning projects can be split into two phases:</p>
<ul>
<li>Training</li>
<li>Inference</li>
</ul>
<p>During the training phase, data science teams have to obtain, analyze
and understand available data and generalize it into a mathematical
model. The model uses the features of the sample data to reason about
data it has never seen. Although it can be completely custom code, it is
usually based on proven machine learning algorithms, such as Naïve
Bayes, K Means, Linear Regression, Deep Learning, Random Forests or
Decision Trees. The act of building the model from the sample (training)
data is referred to as training.</p>
<p>The inference phase refers to using the model to predict an unknown
property of the input data. This requires deploying the model into a
production environment and operating it.</p>
<h2><a class="anchor" aria-hidden="true" id="operating-machine-learning"></a><a href="#operating-machine-learning" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Operating Machine Learning</h2>
<p>The most straightforward way to deploy the model is to wrap it in a REST
web service and let other applications remotely invoke the inference
service. Many machine learning frameworks provide such a service
out-of-the-box to support simple deployments that don’t deal with much
data.</p>
<p>What Hazelcast Jet adds to this story is a simple way to deploy the
model so that it is automatically parallelized and scaled out across a
cluster of machines.</p>
<p><img src="/blog/assets/2020-03-30-parallel-inference.png" alt="Parallel ML inference"></p>
<p>Jet uses its parallel, distributed and resilient execution engine to
turn the model into a high-performance inference service. To use all
available CPU cores, Jet spins up multiple parallel instances of the
model and spreads the inference requests among them. The Jet cluster is
elastic; to scale with the workload, add or remove cluster members on
the fly with no downtime.</p>
<p>Another trick is using a <em>pipelined</em> design instead of a request-reply
pattern. It allows Jet to batch inference requests together and reduce
fixed overheads of serving each request individually. This improves the
overall throughput of the model significantly! The pipelined design
requires a change in the client’s workflow. Instead of calling the
inference service directly, it sends its inference request to an inbox.
It may be implemented using a message broker such as JMS topic, Kafka or
distributed topic of Hazelcast. Jet watches the inbox and groups
multiple requests together to use the model service efficiently. It uses
<a href="https://mechanical-sympathy.blogspot.com/2011/10/smart-batching.html">smart
batching</a>
where the batch size changes with the data volume to keep the latency
always low. The inference results are published to an outbox for callers
to pick it up.</p>
<h2><a class="anchor" aria-hidden="true" id="models-supported"></a><a href="#models-supported" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Models Supported</h2>
<h3><a class="anchor" aria-hidden="true" id="python-models"></a><a href="#python-models" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Python models</h3>
<p>Python is the lingua franca of the data science world. There is a wide
ecosystem of libraries and tools to build and train models in Python:
<a href="https://www.tensorflow.org/">TensorFlow</a>, <a href="https://keras.io/">Keras</a>,
<a href="http://deeplearning.net/software/theano/">Theano</a>,
<a href="https://scikit-learn.org/stable/">Scikit-learn</a> or
<a href="https://pytorch.org/">PyTorch</a> to name a few. Jet can host any Python
model.</p>
<p>Upon model deployment, Jet’s JVM runtime launches Python processes and
establishes bi-directional gRPC communication channels to stream
inference requests through it. So, the model runs natively in a Python
process that is completely managed by Jet. It can be tuned to spin
multiple Python processes on each machine to make use of multicore
processors.</p>
<p>Jet makes sure that the Python code is distributed to all machines that
participate in the cluster. If you add another machine to a Jet cluster,
it creates a directory on it and deploys the Python code there.
Moreover, Jet can install all required Python libraries to prepare the
runtime for your Python model.</p>
<p>Documentation:
<a href="https://docs.hazelcast.org/docs/jet/latest/manual/#map-using-python">https://docs.hazelcast.org/docs/jet/latest/manual/#map-using-python</a></p>
<p>Code sample:
<a href="https://github.com/hazelcast/hazelcast-jet/tree/master/examples/python">https://github.com/hazelcast/hazelcast-jet/tree/master/examples/python</a></p>
<p><img src="/blog/assets/2020-03-30-python-vms.svg" alt="Python integration architecture"></p>
<h3><a class="anchor" aria-hidden="true" id="java-models"></a><a href="#java-models" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Java models</h3>
<p>Java models are used for high-performance inference execution. Favourite
Java model libraries include
<a href="https://github.com/jpmml/jpmml-evaluator">JPMML</a>, <a href="https://www.tensorflow.org/install/lang_java">TensorFlow for
Java</a>,
<a href="https://mxnet.apache.org/api/java">MXNet</a>, <a href="https://xgboost.readthedocs.io/en/latest/jvm/index.html">XGBoost JVM
Package</a> and
<a href="https://www.h2o.ai/">H20</a>.</p>
<p>Similarly to Python, the model is packaged as a Jet Job resource. The
Job usually includes model inference code (the ML library) and a
serialized model. Jet runs the Java models in-process with the cluster
members so there is no need to start extra processes and there is no
communication overhead (serialization, deserialization, networking).
This makes Java model the best performing option. The inference job can
be configured to use one model instance per JVM or multiple model
instances.</p>
<p>Documentation:
<a href="https://docs.hazelcast.org/docs/jet/latest/manual/#machine-learning-model-prediction">https://docs.hazelcast.org/docs/jet/latest/manual/#machine-learning-model-prediction</a></p>
<p>Code samples:</p>
<ul>
<li><a href="https://github.com/hazelcast/hazelcast-jet-demos/tree/master/h2o-breast-cancer-classification">H2O Model</a></li>
<li><a href="https://github.com/hazelcast/hazelcast-jet-demos/tree/master/tensorflow">TensorFlow Model</a></li>
<li><a href="https://github.com/hazelcast/hazelcast-jet-demos/tree/master/realtime-image-recognition">Custom Java Model</a></li>
</ul>
<h3><a class="anchor" aria-hidden="true" id="remote-services"></a><a href="#remote-services" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Remote services</h3>
<p>We started this article by saying that using a model as an RPC service
is simple but requires extra effort when scaling. Jet supports this
pattern, too. The Jet Job can invoke a remote inference service. The
model isn’t managed by Jet in this case, so the operational and
performance advantages are gone. Jet still provides the convenience of
smart batching, inbox/outbox <a href="/docs/api/sources-sinks">connectors</a> and
many <a href="/docs/api/pipeline#types-of-transforms">pipeline operators</a>. Smart
batching works only if the RPC service can operate on batches of input
items.</p>
<p>Benefits of this setup</p>
<ul>
<li>Isolating the model service and the data pipeline</li>
<li>Sharing the model among many Jet pipelines</li>
</ul>
<p>Code samples:</p>
<ul>
<li><a href="https://github.com/hazelcast/hazelcast-jet/tree/master/examples/grpc">Invoking remote gRPC service</a></li>
<li><a href="https://github.com/hazelcast/hazelcast-jet-demos/tree/master/tensorflow">Remote TensorFlow</a></li>
</ul>
<h3><a class="anchor" aria-hidden="true" id="execution-mode-overview"></a><a href="#execution-mode-overview" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Execution Mode Overview</h3>
<table>
<thead>
<tr><th>Execution Mode</th><th>Java Model</th><th>Python Model</th><th>Remote Model</th></tr>
</thead>
<tbody>
<tr><td>Model managed by Jet</td><td>✅</td><td>✅</td><td>✅</td></tr>
<tr><td>Model shared between Jobs</td><td>❌</td><td>❌</td><td>✅</td></tr>
<tr><td>Jet ↔ Model Communication</td><td>Shared memory</td><td>gRPC<br>(processes collocated)</td><td>RPC<br>(processes usually on different machines)</td></tr>
<tr><td>Throughput (single node)</td><td>1M / sec</td><td>50k / sec</td><td>Depends on underlying architecture</td></tr>
<tr><td>Prerequisites</td><td>Model runs in JVM</td><td>Python runtime installed on all cluster machines</td><td>Model available as a RPC service</td></tr>
</tbody>
</table>
<h3><a class="anchor" aria-hidden="true" id="framework-integration-overview"></a><a href="#framework-integration-overview" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Framework Integration Overview</h3>
<table>
<thead>
<tr><th>Framework</th><th>Execution Mode</th><th>Code Sample</th></tr>
</thead>
<tbody>
<tr><td>H2O</td><td>Java</td><td><a href="https://github.com/hazelcast/hazelcast-jet-demos/tree/master/h2o-breast-cancer-classification">Code Sample</a></td></tr>
<tr><td>TensorFlow for Java</td><td>Java</td><td><a href="https://github.com/hazelcast/hazelcast-jet-demos/blob/master/tensorflow/src/main/java/InProcessClassification.java">Code Sample</a></td></tr>
<tr><td>Custom Java Model</td><td>Java</td><td><a href="https://github.com/hazelcast/hazelcast-jet-demos/tree/master/realtime-image-recognition">Code Sample</a></td></tr>
<tr><td>PMML</td><td>Java</td><td>N/A, use <a href="https://github.com/jpmml/jpmml-evaluator">JPMML Evaluator</a> as a Custom Java Model</td></tr>
<tr><td>MXNet</td><td>Java</td><td>N/A, use <a href="https://mxnet.apache.org/api/java.html">MXNet Java Inference API</a> as a Custom Java Model</td></tr>
<tr><td>XGBoost</td><td>Java</td><td>N/A, use <a href="https://xgboost.readthedocs.io/en/latest/jvm/index.html">XGBoost JVM Package</a> as a Custom Java Model</td></tr>
<tr><td><a href="https://keras.io/">Keras</a>, <a href="http://deeplearning.net/software/theano/">Theano</a>, <a href="https://scikit-learn.org/stable/">Scikit-learn</a> or <a href="https://pytorch.org/">PyTorch</a></td><td>Python</td><td>N/A, use the <a href="https://github.com/hazelcast/hazelcast-jet/tree/master/examples/python">Custom Python Model</a></td></tr>
<tr><td>Custom Python Model</td><td>Python</td><td><a href="https://github.com/hazelcast/hazelcast-jet/tree/master/examples/python">Code Sample</a></td></tr>
<tr><td>Remote gRPC service</td><td>Remote</td><td><a href="https://github.com/hazelcast/hazelcast-jet/tree/master/examples/grpc">Code Sample</a></td></tr>
<tr><td>Remote TensorFlow service</td><td>Remote</td><td><a href="https://github.com/hazelcast/hazelcast-jet-demos/blob/master/tensorflow/src/main/java/ModelServerClassification.java">Code Sample</a></td></tr>
</tbody>
</table>
</span></div></div><div class="blogSocialSection"></div></div><div class="blog-recent"><a class="button" href="/blog/">Recent posts</a></div></div></div><nav class="onPageNav"><ul class="toc-headings"><li><a href="#operating-machine-learning">Operating Machine Learning</a></li><li><a href="#models-supported">Models Supported</a><ul class="toc-headings"><li><a href="#python-models">Python models</a></li><li><a href="#java-models">Java models</a></li><li><a href="#remote-services">Remote services</a></li><li><a href="#execution-mode-overview">Execution Mode Overview</a></li><li><a href="#framework-integration-overview">Framework Integration Overview</a></li></ul></li></ul></nav></div><footer class="nav-footer" id="footer"><section class="sitemap"><div style="text-align:left"><a href="/" class="nav-home"><img src="/img/logo-light.svg" alt="Hazelcast Jet" width="200" height="40"/></a><div style="margin-left:12px"><a class="github-button" href="https://github.com/hazelcast/hazelcast-jet" data-icon="octicon-star" data-count-href="/facebook/docusaurus/stargazers" data-show-count="true" data-count-aria-label="# stargazers on GitHub" aria-label="Star this project on GitHub">Star On GitHub</a></div></div><div><h5>Docs</h5><a href="/docs/get-started/intro">Get Started</a><a href="/docs/concepts/dag">Concepts</a><a href="/docs/tutorials/kafka">Tutorials</a><a href="/docs/architecture/distributed-computing">Architecture</a><a href="/docs/operations/installation">Operations Guide</a><a href="/docs/enterprise">Enterprise Edition</a></div><div><h5>Community</h5><a href="https://groups.google.com/forum/#!forum/hazelcast-jet" target="_blank" rel="noreferrer noopener">Google Groups</a><a href="http://stackoverflow.com/questions/tagged/hazelcast-jet" target="_blank" rel="noreferrer noopener">Stack Overflow</a><a href="https://slack.hazelcast.com">Slack</a></div><div><h5>Latest From the Blog</h5><a href="/blog/2023/06/14/jet-engine-in-hazelcast">Jet engine lives on in Hazelcast 5.x</a><a href="/blog/2021/04/21/jet-45-is-released">Jet 4.5 Released</a><a href="/blog/2021/03/17/billion-events-per-second">Billion Events Per Second with Millisecond Latency: Streaming Analytics at Giga-Scale</a><a href="/blog/2021/02/03/jet-44-is-released">Jet 4.4 Released</a><a href="/blog/2020/10/23/jet-43-is-released">Jet 4.3 Released</a></div><div><h5>More</h5><a href="https://github.com/hazelcast/hazelcast-jet">GitHub Project</a><a href="http://hazelcast.com/company/careers/">Work at Hazelcast</a><a href="/license">License</a></div></section><section class="copyright">Copyright © 2023 Hazelcast Inc.</section></footer></div><script type="text/javascript" src="https://cdn.jsdelivr.net/docsearch.js/1/docsearch.min.js"></script><script>
                document.addEventListener('keyup', function(e) {
                  if (e.target !== document.body) {
                    return;
                  }
                  // keyCode for '/' (slash)
                  if (e.keyCode === 191) {
                    const search = document.getElementById('search_input_react');
                    search && search.focus();
                  }
                });
              </script><script>
              var search = docsearch({
                
                apiKey: '79d1e4941621b9fd761d279d4d19ed69',
                indexName: 'hazelcast-jet',
                inputSelector: '#search_input_react',
                algoliaOptions: {"facetFilters":["language:en","version:4.5.4"]}
              });
            </script></body></html>