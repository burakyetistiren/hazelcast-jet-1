<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta http-equiv="X-UA-Compatible" content="IE=edge"/><title>Performance of Modern Java on Data-Heavy Workloads: Real-Time Streaming · Hazelcast Jet</title><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta name="generator" content="Docusaurus"/><meta name="description" content="This post is a part of a series:"/><meta name="docsearch:language" content="en"/><meta property="og:title" content="Performance of Modern Java on Data-Heavy Workloads: Real-Time Streaming · Hazelcast Jet"/><meta property="og:type" content="website"/><meta property="og:url" content="https://jet-start.sh/blog/2020/06/09/jdk-gc-benchmarks-part1"/><meta property="og:description" content="This post is a part of a series:"/><meta property="og:image" content="https://jet-start.sh/img/Hazelcast-Jet-Logo-Blue_Dark.jpg"/><meta name="twitter:card" content="summary"/><meta name="twitter:image" content="https://jet-start.sh/img/Hazelcast-Jet-Logo-Blue_Dark.jpg"/><link rel="shortcut icon" href="/img/favicon.png"/><link rel="stylesheet" href="https://cdn.jsdelivr.net/docsearch.js/1/docsearch.min.css"/><link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css"/><link rel="alternate" type="application/atom+xml" href="https://jet-start.sh/blog/atom.xml" title="Hazelcast Jet Blog ATOM Feed"/><link rel="alternate" type="application/rss+xml" href="https://jet-start.sh/blog/feed.xml" title="Hazelcast Jet Blog RSS Feed"/><script>
              (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
              (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
              m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
              })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

              ga('create', 'UA-158279495-1', 'auto');
              ga('send', 'pageview');
            </script><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Ubuntu:300,400,500,600"/><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:300,400,500,600,700,800"/><script type="text/javascript" src="https://buttons.github.io/buttons.js"></script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.0/clipboard.min.js"></script><script type="text/javascript" src="/js/code-block-buttons.js"></script><script type="text/javascript" src="https://plausible.io/js/plausible.js" async="" defer="" data-domain="jet-start.sh"></script><script src="/js/scrollSpy.js"></script><link rel="stylesheet" href="/css/prism.css"/><link rel="stylesheet" href="/css/main.css"/><script src="/js/codetabs.js"></script></head><body class="sideNavVisible separateOnPageNav"><div class="fixedHeaderContainer"><div class="headerWrapper wrapper"><header><a href="/"><img class="logo" src="/img/logo-dark.svg" alt="Hazelcast Jet"/></a><a href="/versions"><h3>4.5.4</h3></a><div class="navigationWrapper navigationSlider"><nav class="slidingNav"><ul class="nav-site nav-site-internal"><li class=""><a href="/docs/get-started/intro" target="_self">Docs</a></li><li class=""><a href="/download" target="_self">Download</a></li><li class=""><a href="/demos" target="_self">Demos</a></li><li class=""><a href="https://github.com/hazelcast/hazelcast-jet" target="_self">GitHub</a></li><li class=""><a href="https://slack.hazelcast.com/" target="_self">Community</a></li><li class="siteNavGroupActive"><a href="/blog/" target="_self">Blog</a></li><li class="navSearchWrapper reactNavSearchWrapper"><input type="text" id="search_input_react" placeholder="Search" title="Search"/></li></ul></nav></div></header></div></div><div class="navPusher"><div class="docMainWrapper wrapper"><div class="docsNavContainer" id="docsNav"><nav class="toc"><div class="toggleNav"><section class="navWrapper wrapper"><div class="navBreadcrumb wrapper"><div class="navToggle" id="navToggler"><div class="hamburger-menu"><div class="line1"></div><div class="line2"></div><div class="line3"></div></div></div><h2><i>›</i><span>All posts</span></h2><div class="tocToggler" id="tocToggler"><i class="icon-toc"></i></div></div><div class="navGroups"><div class="navGroup"><h3 class="navGroupCategoryTitle">All posts</h3><ul class=""><li class="navListItem"><a class="navItem" href="/blog/2023/06/14/jet-engine-in-hazelcast">Jet engine lives on in Hazelcast 5.x</a></li><li class="navListItem"><a class="navItem" href="/blog/2021/04/21/jet-45-is-released">Jet 4.5 Released</a></li><li class="navListItem"><a class="navItem" href="/blog/2021/03/17/billion-events-per-second">Billion Events Per Second with Millisecond Latency: Streaming Analytics at Giga-Scale</a></li><li class="navListItem"><a class="navItem" href="/blog/2021/02/03/jet-44-is-released">Jet 4.4 Released</a></li><li class="navListItem"><a class="navItem" href="/blog/2020/10/23/jet-43-is-released">Jet 4.3 Released</a></li><li class="navListItem"><a class="navItem" href="/blog/2020/10/06/enabling-full-text-search">Enabling Full-text Search with Change Data Capture in a Legacy Application</a></li><li class="navListItem"><a class="navItem" href="/blog/2020/09/18/cdc-meets-stream-processing">Change Data Capture meets Stream Processing</a></li><li class="navListItem"><a class="navItem" href="/blog/2020/08/05/gc-tuning-for-jet">Sub-10 ms Latency in Java: Concurrent GC with Green Threads</a></li><li class="navListItem"><a class="navItem" href="/blog/2020/07/16/designing-evergreen-cache-cdc">Designing an Evergreen Cache with Change Data Capture</a></li><li class="navListItem"><a class="navItem" href="/blog/2020/07/14/jet-42-is-released">Jet 4.2 is Released</a></li><li class="navListItem"><a class="navItem" href="/blog/2020/06/23/jdk-gc-benchmarks-rematch">Performance of Modern Java on Data-Heavy Workloads: The Low-Latency Rematch</a></li><li class="navListItem"><a class="navItem" href="/blog/2020/06/09/jdk-gc-benchmarks-part2">Performance of Modern Java on Data-Heavy Workloads: Batch Processing</a></li><li class="navListItem navListItemActive"><a class="navItem" href="/blog/2020/06/09/jdk-gc-benchmarks-part1">Performance of Modern Java on Data-Heavy Workloads: Real-Time Streaming</a></li><li class="navListItem"><a class="navItem" href="/blog/2020/05/25/grcp">Processing 10M queries / second on a single node using Jet and gRPC</a></li><li class="navListItem"><a class="navItem" href="/blog/2020/05/18/spark-jet">How Hazelcast Jet Compares to Apache Spark</a></li><li class="navListItem"><a class="navItem" href="/blog/2020/04/29/jet-41-is-released">Jet 4.1 is Released</a></li><li class="navListItem"><a class="navItem" href="/blog/2020/04/01/upgrading-to-jet-40">Upgrading to Jet 4.0</a></li><li class="navListItem"><a class="navItem" href="/blog/2020/03/30/ml-inference">Machine Learning Inference at Scale</a></li><li class="navListItem"><a class="navItem" href="/blog/2020/03/02/jet-40-is-released">Jet 4.0 is Released</a></li><li class="navListItem"><a class="navItem" href="/blog/2020/02/20/transactional-processors">Transactional connectors in Hazelcast Jet</a></li><li class="navListItem"><a class="navItem" href="/blog/2020/01/28/new-website">Announcing New Documentation Website</a></li><li class="navListItem"><a class="navItem" href="/blog/2019/11/12/stream-deduplication">Stream Deduplication with Hazelcast Jet</a></li></ul></div></div></section></div><script>
            var coll = document.getElementsByClassName('collapsible');
            var checkActiveCategory = true;
            for (var i = 0; i < coll.length; i++) {
              var links = coll[i].nextElementSibling.getElementsByTagName('*');
              if (checkActiveCategory){
                for (var j = 0; j < links.length; j++) {
                  if (links[j].classList.contains('navListItemActive')){
                    coll[i].nextElementSibling.classList.toggle('hide');
                    coll[i].childNodes[1].classList.toggle('rotate');
                    checkActiveCategory = false;
                    break;
                  }
                }
              }

              coll[i].addEventListener('click', function() {
                var arrow = this.childNodes[1];
                arrow.classList.toggle('rotate');
                var content = this.nextElementSibling;
                content.classList.toggle('hide');
              });
            }

            document.addEventListener('DOMContentLoaded', function() {
              createToggler('#navToggler', '#docsNav', 'docsSliderActive');
              createToggler('#tocToggler', 'body', 'tocActive');

              var headings = document.querySelector('.toc-headings');
              headings && headings.addEventListener('click', function(event) {
                var el = event.target;
                while(el !== headings){
                  if (el.tagName === 'A') {
                    document.body.classList.remove('tocActive');
                    break;
                  } else{
                    el = el.parentNode;
                  }
                }
              }, false);

              function createToggler(togglerSelector, targetSelector, className) {
                var toggler = document.querySelector(togglerSelector);
                var target = document.querySelector(targetSelector);

                if (!toggler) {
                  return;
                }

                toggler.onclick = function(event) {
                  event.preventDefault();

                  target.classList.toggle(className);
                };
              }
            });
        </script></nav></div><div class="container mainContainer postContainer blogContainer"><div class="wrapper"><div class="lonePost"><div class="post"><header class="postHeader"><h1 class="postHeaderTitle"><a href="/blog/2020/06/09/jdk-gc-benchmarks-part1">Performance of Modern Java on Data-Heavy Workloads: Real-Time Streaming</a></h1><p class="post-meta">June 9, 2020</p><div class="authorBlock"><p class="post-authorName"><a href="https://twitter.com/mtopolnik" target="_blank" rel="noreferrer noopener">Marko Topolnik</a></p><div class="authorPhoto"><a href="https://twitter.com/mtopolnik" target="_blank" rel="noreferrer noopener"><img src="https://i.imgur.com/xuavzce.jpg" alt="Marko Topolnik"/></a></div></div></header><div><span><p>This post is a part of a series:</p>
<ul>
<li>Part 1 (you are here)</li>
<li><a href="/blog/2020/06/09/jdk-gc-benchmarks-part2">Part 2 (batch workload benchmark)</a></li>
<li><a href="/blog/2020/06/23/jdk-gc-benchmarks-rematch">Part 3 (low-latency benchmark)</a></li>
<li><a href="/blog/2020/08/05/gc-tuning-for-jet">Part 4 (concurrent GC with green threads)</a></li>
<li><a href="/blog/2021/03/17/billion-events-per-second">Part 5 (billion events per second)</a></li>
</ul>
<p>The Java runtime has been evolving more rapidly in recent years and,
after 15 years, we finally got a new default garbage collector: the
G1. Two more GCs are on their way to production and are available as
experimental features: Oracle's ZGC and OpenJDK's Shenandoah. We at
Hazelcast thought it was time to put all these new options to the test
and find which choices work well with workloads typical for our
distributed stream processing engine, <a href="https://github.com/hazelcast/hazelcast-jet">Hazelcast Jet</a>.</p>
<p>Jet is being used for a broad spectrum of use cases, with different
latency and throughput requirements. Here are three important
categories:</p>
<ol>
<li>Low-latency unbounded stream processing, with moderate state.
Example: detecting trends in 100 Hz sensor data from 10,000 devices
and sending corrective feedback within 10-20 milliseconds.</li>
<li>High-throughput, large-state unbounded stream processing. Example:
tracking GPS locations of millions of users, inferring their velocity
vectors.</li>
<li>Old-school batch processing of big data volumes. The relevant measure
is time to complete, which implies a high throughput demand. Example:
analyzing a day's worth of stock trading data to update the risk
exposure of a given portfolio.</li>
</ol>
<p>At the outset, we can observe the following:</p>
<ul>
<li>in scenario 1 the latency requirements enter the danger zone of GC
pauses: 100 milliseconds, something traditionally considered an
excellent result for a worst-case GC pause, may be a showstopper for
many use cases</li>
<li>scenarios 2 and 3 are similar in terms of demands on the garbage
collector. Less strict latency, but large pressure on the tenured
generation</li>
<li>scenario 2 is tougher because latency, even if less so than in
scenario 1, is still relevant</li>
</ul>
<p>We tried the following combinations:</p>
<ol>
<li>JDK 8 with the default Parallel collector and the optional
ConcurrentMarkSweep and G1</li>
<li>JDK 11 with the default G1 collector and the optional Parallel</li>
<li>JDK 14 with the default G1 as well as the experimental ZGC and
Shenandoah</li>
</ol>
<p>And here are our overall conclusions:</p>
<ol>
<li>On modern JDK versions, the G1 is one monster of a collector. It
handles heaps of dozens of GB with ease (we tried 60 GB), keeping
maximum GC pauses within 200 ms. Under extreme pressure it doesn't
show brittleness with catastrophic failure modes. Instead the Full GC
pauses rise into the low seconds range. Its Achilles' heel is the
upper bound on the GC pause in favorable low-pressure conditions,
which we couldn't push lower than 20-25 ms.</li>
<li>JDK 8 is an antiquated runtime. The default Parallel collector enters
huge Full GC pauses and the G1, although having less frequent Full
GCs, is stuck in an old version that uses just one thread to perform
it, resulting in even longer pauses. Even on a moderate heap of 12
GB, the pauses were exceeding 20 seconds for Parallel and a full
minute for G1. The ConcurrentMarkSweep collector is strictly worse
than G1 in all scenarios, and its failure mode are multi-minute Full
GC pauses.</li>
<li>The ZGC, while allowing substantially less throughput than G1, was
very good in that one weak area of G1, occasionally increasing our
latency by up to 10 ms under light load.</li>
<li>Shenandoah was a disappointment with occasional, but nevertheless
regular, latency spikes up to 220 ms in the low-pressure regime.</li>
<li>Neither ZGC nor Shenandoah showed as smooth failure modes as G1. They
exhibited brittleness, with the low-latency regime suddenly giving
way to very long pauses and even OOMEs.</li>
</ol>
<p>This post is Part 1 of a two-part series and presents our findings for
the two streaming scenarios. In <a href="/blog/2020/06/09/jdk-gc-benchmarks-part2">Part
2</a> we'll present the results
for batch processing.</p>
<h2><a class="anchor" aria-hidden="true" id="streaming-pipeline-benchmark"></a><a href="#streaming-pipeline-benchmark" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Streaming Pipeline Benchmark</h2>
<p>For the streaming benchmarks, we used the code available
<a href="https://github.com/mtopolnik/jet-gc-benchmark/blob/master/src/main/java/org/example/StreamingBenchmark.java">here</a>,
with some minor variations between the tests. Here is the main part, the
Jet pipeline:</p>
<pre><code class="hljs css language-java"><span class="token class-name">StreamStage</span><span class="token generics"><span class="token punctuation">&lt;</span><span class="token class-name">Long</span><span class="token punctuation">></span></span> source <span class="token operator">=</span> p<span class="token punctuation">.</span><span class="token function">readFrom</span><span class="token punctuation">(</span><span class="token function">longSource</span><span class="token punctuation">(</span><span class="token constant">ITEMS_PER_SECOND</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
                            <span class="token punctuation">.</span><span class="token function">withNativeTimestamps</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>
                            <span class="token punctuation">.</span><span class="token function">rebalance</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span> <span class="token comment">// Introduced in Jet 4.2</span>
source<span class="token punctuation">.</span><span class="token function">groupingKey</span><span class="token punctuation">(</span>n <span class="token operator">-></span> n <span class="token operator">%</span> <span class="token constant">NUM_KEYS</span><span class="token punctuation">)</span>
      <span class="token punctuation">.</span><span class="token function">window</span><span class="token punctuation">(</span><span class="token function">sliding</span><span class="token punctuation">(</span><span class="token constant">SECONDS</span><span class="token punctuation">.</span><span class="token function">toMillis</span><span class="token punctuation">(</span><span class="token constant">WIN_SIZE_SECONDS</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token constant">SLIDING_STEP_MILLIS</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
      <span class="token punctuation">.</span><span class="token function">aggregate</span><span class="token punctuation">(</span><span class="token function">counting</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
      <span class="token punctuation">.</span><span class="token function">filter</span><span class="token punctuation">(</span>kwr <span class="token operator">-></span> kwr<span class="token punctuation">.</span><span class="token function">getKey</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">%</span> <span class="token constant">DIAGNOSTIC_KEYSET_DOWNSAMPLING_FACTOR</span> <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">)</span>
      <span class="token punctuation">.</span><span class="token function">window</span><span class="token punctuation">(</span><span class="token function">tumbling</span><span class="token punctuation">(</span><span class="token constant">SLIDING_STEP_MILLIS</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
      <span class="token punctuation">.</span><span class="token function">aggregate</span><span class="token punctuation">(</span><span class="token function">counting</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
      <span class="token punctuation">.</span><span class="token function">writeTo</span><span class="token punctuation">(</span><span class="token class-name">Sinks</span><span class="token punctuation">.</span><span class="token function">logger</span><span class="token punctuation">(</span>wr <span class="token operator">-></span> <span class="token class-name">String</span><span class="token punctuation">.</span><span class="token function">format</span><span class="token punctuation">(</span><span class="token string">"time %,d: latency %,d ms, cca. %,d keys"</span><span class="token punctuation">,</span>
              <span class="token function">simpleTime</span><span class="token punctuation">(</span>wr<span class="token punctuation">.</span><span class="token function">end</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
              <span class="token constant">NANOSECONDS</span><span class="token punctuation">.</span><span class="token function">toMillis</span><span class="token punctuation">(</span><span class="token class-name">System</span><span class="token punctuation">.</span><span class="token function">nanoTime</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">-</span> wr<span class="token punctuation">.</span><span class="token function">end</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
              wr<span class="token punctuation">.</span><span class="token function">result</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">*</span> <span class="token constant">DIAGNOSTIC_KEYSET_DOWNSAMPLING_FACTOR</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
</code></pre>
<p>This pipeline represents use cases with an unbounded event stream where
the engine is asked to perform sliding window aggregation. You need this
kind of aggregation, for example, to obtain the time derivative of a
changing quantity, remove high-frequency noise from the data (smoothing)
or measure the intensity of the occurrence of some event (events per
second). The engine can first split the stream by some category (for
example, each distinct IoT device or smartphone) into substreams and
then independently track the aggregated value in each of them. In
Hazelcast Jet the sliding window moves in fixed-size steps that you
configure. For example, with a sliding step of 1 second you get a
complete set of results every second, and if the window size is 1
minute, the results reflect the events that occurred within the last
minute.</p>
<p>Some notes:</p>
<p>The code is entirely self-contained with no outside data sources or
sinks. We use a mock data source that simulates an event stream with
exactly the chosen number of events per second. Consecutive event
timestamps are an equal amount of time apart. The source never emits an
event whose timestamp is still in the future, but otherwise emits them
as fast as possible.</p>
<p>If the pipeline falls behind, events will be &quot;buffered&quot; but without any
storage. After falling behind, the pipeline must catch up by ingesting
data as fast as it can. Since our source is non-parallel, the limit on
its throughput was about 2.2 million events per second. We used 1
million simulated events per second, leaving a catching-up headroom of
1.2 million per second.</p>
<p>The pipeline measures its own latency by comparing the timestamp of an
emitted sliding window result with the actual wall-clock time. In more
detail, there are two aggregation stages with filtering between them. A
single sliding window result consists of many items, each for one
substream, and we're interested in the latency of the last-emitted
item. For this reason we first filter out most of the output, keeping
every 10,000th entry, and then direct the thinned-out stream to the
second, non-keyed tumbling window stage that notes the result size and
measures the latency. Non-keyed aggregation is not parallelized, so we
get a single point of measurement. The filtering stage is parallel and
data-local so the impact of the additional aggregation step is very
small (well below 1 ms).</p>
<p>We used a trivial aggregate function: counting, in effect obtaining the
events/second metric of the stream. It has minimal state (a single
<code>long</code> number) and produces no garbage. For any given heap usage in
gigabytes, such a small state per key implies the worst case for the
garbage collector: a very large number of objects. GC overheads scale
much more with object count than heap size. We also tested a variant
that computes the same aggregate function, but with a different
implementation that produces garbage.</p>
<p>We performed most of the streaming benchmarks on a single node since our
focus was the effect of memory management on pipeline performance and
network latency just adds noise into the picture. We repeated some key
tests on a three-node Amazon EC2 cluster to validate our prediction that
cluster performance won't affect our conclusions. You can find a more
detailed justification for this towards the end of <a href="/blog/2020/06/09/jdk-gc-benchmarks-part2">Part
2</a>.</p>
<p>We excluded the Parallel collector from the results for streaming
workloads because the latency spikes it introduces would be unacceptable
in pretty much any real-life scenario.</p>
<h3><a class="anchor" aria-hidden="true" id="scenario-1-low-latency-moderate-state"></a><a href="#scenario-1-low-latency-moderate-state" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Scenario 1: Low Latency, Moderate State</h3>
<p>For the first scenario we used these parameters:</p>
<ul>
<li>OpenJDK 14</li>
<li>JVM heap size 4 gigabytes</li>
<li>for G1, -XX:MaxGCPauseMillis=5</li>
<li>1 million events per second</li>
<li>50,000 distinct keys</li>
<li>30-second window sliding by 0.1 second</li>
</ul>
<p>In this scenario there's less than 1 GB heap usage. The collector is not
under high pressure, it has enough time to perform concurrent GC in the
background. These are the maximum pipeline latencies we observed with
the three garbage collectors we tested:</p>
<p><img src="/blog/assets/2020-06-01-light-streaming-latency.png" alt="Pipeline Latency with Light Streaming"></p>
<p>Note that these numbers include a fixed time of about 3 milliseconds to
emit the window results. The chart is pretty self-explanatory: the
default collector, G1, is pretty good on its own, but if you need even
better latency, you can use the experimental ZGC collector. We couldn't
reduce the latency spikes below 10 milliseconds, however we did note
that, in the case of ZGC and Shenandoah, they weren't due to outright GC
pauses but rather short periods of increased background GC work.
Shenandoah's overheads occasionally raised latency above 200 ms.</p>
<h3><a class="anchor" aria-hidden="true" id="scenario-2-large-state-less-strict-latency"></a><a href="#scenario-2-large-state-less-strict-latency" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Scenario 2: Large State, Less Strict Latency</h3>
<p>In scenario 2 we assume that, for various reasons outside our control,
(e.g., mobile network), the latency can grow into low seconds, which
relaxes the requirements we must impose on our stream processing
pipeline. On the other hand, we may be dealing with much larger data
volumes, on the order of millions or dozens of millions of keys.</p>
<p>In this scenario we can provision our hardware so it's heavily utilized,
relying on the GC to manage a large heap instead of spreading out the
data over many cluster nodes.</p>
<p>We performed many tests with different combinations to find out how the
interplay between various factors causes the runtime to either keep up
or fall behind. In the end we found two parameters that determine this:</p>
<ol>
<li>number of entries stored in the aggregation state</li>
<li>demand on the catching-up throughput</li>
</ol>
<p>The first one corresponds to the number of objects in the tenured
generation. Sliding window aggregation retains objects for a significant
time (the length of the window) and then releases them. This goes
directly against the Generational Garbage Hypothesis, which states that
objects will either die young or live forever. This regime puts the
strongest pressure on the GC, and since the GC effort scales with the
number of live objects, performance is highly sensitive to this
parameter.</p>
<p>The second parameter relates to how much GC overhead the application can
tolerate. To explain it better, let's use some diagrams. A pipeline
performing windowed aggregation goes through three distinct steps:</p>
<ol>
<li>processing events in real time, as they arrive</li>
<li>emitting the sliding window results</li>
<li>catching up with the events received while in step 2</li>
</ol>
<p>The three phases can be visualized as follows:</p>
<p><img src="/blog/assets/2020-06-01-sliding-window-1.png" alt="Phases of the sliding window computation"></p>
<p>If emitting the window result takes longer, we get a situation like this:</p>
<p><img src="/blog/assets/2020-06-01-sliding-window-2.png" alt="Phases of the sliding window computation"></p>
<p>Now the headroom has shrunk to almost nothing, the pipeline is barely
keeping up, and any temporary hiccups like an occasional GC pause will
cause latency to grow and recover at a very slow pace.</p>
<p>If we change this picture and present just the average event ingestion
rate after window emission, we get this:</p>
<p><img src="/blog/assets/2020-06-01-sliding-window-3.png" alt="Phases of the sliding window computation"></p>
<p>We call the height of the yellow rectangle &quot;catchup demand&quot;: it is the
demand on the throughput of the source. If it exceeds the actual maximum
throughput, the pipeline fails.</p>
<p>This is how it would look if window emission took way too long:</p>
<p><img src="/blog/assets/2020-06-01-sliding-window-4.png" alt="Phases of the sliding window computation"></p>
<p>The area of the red and the yellow rectangles is fixed, it corresponds
to the amount of data that must flow through the pipeline. Basically,
the red rectangle &quot;squeezes out&quot; the yellow one. But the yellow
rectangle's height is actually limited, in our case to 2.2 million
events per second. So whenever it would be taller than the limit, we'd
have a failing pipeline whose latency grows without bounds.</p>
<p>We worked out the formulas that predict the sizes of the rectangles for
a given combination of event rate, window size, sliding step and keyset
size, so that we could determine the catchup demand for each case.</p>
<p>Now that we have two more-or-less independent parameters derived from
many more parameters describing each individual setup, we can create a
2D-chart where each benchmark run has a point on it. We assigned a color
to each point, telling us whether the given combination worked or
failed. For example, for JDK 14 with G1 on a developer's laptop, we got
this picture:</p>
<p><img src="/blog/assets/2020-06-01-viable-combinations-jdk14.png" alt="Viable combinations of catchup demand and storage, JDK 14 and G1"></p>
<p>We made the distinction between &quot;yes&quot;, &quot;no&quot; and &quot;gc&quot;, meaning the
pipeline keeps up, doesn't keep up due to lack of throughput, or doesn't
keep up due to frequent long GC pauses. Note that the lack of throughput
can also be caused by concurrent GC activity and frequent short GC
pauses. In the end, the distinction doesn't matter a lot.</p>
<p>You can make out a contour that separates the lower-left area where
things work out from the rest of the space, where they fail. We made
the same kind of chart for other combinations of JDK and GC, extracted
the contours, and came up with this summary chart:</p>
<p><img src="/blog/assets/2020-06-01-viable-combinations.png" alt="Viable combinations of catchup demand and storage, JDK 14 and G1"></p>
<p>For reference, the hardware we used is a MacBook Pro 2018 with a 6-core
Intel Core i7 and 16 GB DDR4 RAM, configuring <code>-Xmx10g</code> for the JVM.
However, we do expect the overall relationship among the combinations to
remain the same on a broad range of hardware parameters. The chart
visualizes the superiority of the G1 over others, the weakness of the G1
on JDK 8, and the weakness of the experimental low-latency collectors
for this kind of workload.</p>
<p>The base latency, the time it takes to emit the window results, was in
the ballpark of 500 milliseconds, but latency would often take hikes due
to occasional Major GC's (which are not unreasonably long with the G1),
up to 10 seconds in the borderline cases (where the pipeline barely
keeps up), and still recover back to a second or two. We also noticed
the effects of JIT compilation in the borderline cases: the pipeline
would start out with a constantly increasing latency, but then after
around two minutes, its performance would improve and the latency would
make a full recovery.</p>
<p>Go to <a href="/blog/2020/06/09/jdk-gc-benchmarks-part2">Part 2: the Batch Pipeline Benchmarks</a>.</p>
<p><em>If you enjoyed reading this post, check out Jet at
<a href="https://github.com/hazelcast/hazelcast-jet">GitHub</a> and give us a
star!</em></p>
</span></div></div><div class="blogSocialSection"></div></div><div class="blog-recent"><a class="button" href="/blog/">Recent posts</a></div></div></div><nav class="onPageNav"><ul class="toc-headings"><li><a href="#streaming-pipeline-benchmark">Streaming Pipeline Benchmark</a><ul class="toc-headings"><li><a href="#scenario-1-low-latency-moderate-state">Scenario 1: Low Latency, Moderate State</a></li><li><a href="#scenario-2-large-state-less-strict-latency">Scenario 2: Large State, Less Strict Latency</a></li></ul></li></ul></nav></div><footer class="nav-footer" id="footer"><section class="sitemap"><div style="text-align:left"><a href="/" class="nav-home"><img src="/img/logo-light.svg" alt="Hazelcast Jet" width="200" height="40"/></a><div style="margin-left:12px"><a class="github-button" href="https://github.com/hazelcast/hazelcast-jet" data-icon="octicon-star" data-count-href="/facebook/docusaurus/stargazers" data-show-count="true" data-count-aria-label="# stargazers on GitHub" aria-label="Star this project on GitHub">Star On GitHub</a></div></div><div><h5>Docs</h5><a href="/docs/get-started/intro">Get Started</a><a href="/docs/concepts/dag">Concepts</a><a href="/docs/tutorials/kafka">Tutorials</a><a href="/docs/architecture/distributed-computing">Architecture</a><a href="/docs/operations/installation">Operations Guide</a><a href="/docs/enterprise">Enterprise Edition</a></div><div><h5>Community</h5><a href="https://groups.google.com/forum/#!forum/hazelcast-jet" target="_blank" rel="noreferrer noopener">Google Groups</a><a href="http://stackoverflow.com/questions/tagged/hazelcast-jet" target="_blank" rel="noreferrer noopener">Stack Overflow</a><a href="https://slack.hazelcast.com">Slack</a></div><div><h5>Latest From the Blog</h5><a href="/blog/2023/06/14/jet-engine-in-hazelcast">Jet engine lives on in Hazelcast 5.x</a><a href="/blog/2021/04/21/jet-45-is-released">Jet 4.5 Released</a><a href="/blog/2021/03/17/billion-events-per-second">Billion Events Per Second with Millisecond Latency: Streaming Analytics at Giga-Scale</a><a href="/blog/2021/02/03/jet-44-is-released">Jet 4.4 Released</a><a href="/blog/2020/10/23/jet-43-is-released">Jet 4.3 Released</a></div><div><h5>More</h5><a href="https://github.com/hazelcast/hazelcast-jet">GitHub Project</a><a href="http://hazelcast.com/company/careers/">Work at Hazelcast</a><a href="/license">License</a></div></section><section class="copyright">Copyright © 2023 Hazelcast Inc.</section></footer></div><script type="text/javascript" src="https://cdn.jsdelivr.net/docsearch.js/1/docsearch.min.js"></script><script>
                document.addEventListener('keyup', function(e) {
                  if (e.target !== document.body) {
                    return;
                  }
                  // keyCode for '/' (slash)
                  if (e.keyCode === 191) {
                    const search = document.getElementById('search_input_react');
                    search && search.focus();
                  }
                });
              </script><script>
              var search = docsearch({
                
                apiKey: '79d1e4941621b9fd761d279d4d19ed69',
                indexName: 'hazelcast-jet',
                inputSelector: '#search_input_react',
                algoliaOptions: {"facetFilters":["language:en","version:4.5.4"]}
              });
            </script></body></html>